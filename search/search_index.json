{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Youtu-agent","text":"<p>Youtu-agent is a powerful and modular framework for building, running, and evaluating autonomous agents. It is designed with flexibility and extensibility in mind, allowing developers to easily create custom agents, tools, and environments.</p> <p>The framework's design is centered on a clear separation of concerns, enabling robust and scalable agent development.</p>"},{"location":"#core-architecture","title":"Core Architecture","text":"<p>At a high level, the framework's components interact as follows: An <code>AgentConfig</code> defines an <code>Agent</code>, which operates within an <code>Environment</code> and uses <code>Toolkits</code> to perform actions. The entire system can be benchmarked using the <code>Evaluation Framework</code>.</p> <pre><code>graph TD\n    subgraph Configuration\n        A[AgentConfig] --&gt;|defines| B(Agent);\n    end\n\n    subgraph Execution\n        B --&gt;|uses| C[Toolkit];\n        B --&gt;|operates in| D[Environment];\n    end\n\n    subgraph Benchmarking\n        E[Evaluation Framework] --&gt;|evaluates| B;\n    end</code></pre>"},{"location":"#key-modules","title":"Key Modules","text":"<p>The framework is divided into several key modules, each with a distinct responsibility.</p>"},{"location":"#configuration-configmd","title":"Configuration (<code>config.md</code>)","text":"<p>The entire framework is driven by a configuration system built on <code>pydantic</code> and <code>hydra</code>. It uses YAML files to define the behavior of agents, experiments, and their components.</p>"},{"location":"#agent-paradigms-agentsmd","title":"Agent Paradigms (<code>agents.md</code>)","text":"<p>The core logic of the agent. The framework supports two primary paradigms: - <code>SimpleAgent</code>: A classic single-agent model that reasons and acts in a loop (ReAct-style). - <code>OrchestraAgent</code>: A multi-agent system that uses a Plan-and-Execute strategy, coordinating a Planner, Workers, and a Reporter to solve complex tasks.</p>"},{"location":"#environments-envmd","title":"Environments (<code>env.md</code>)","text":"<p>Environments represent the world in which an agent operates. They provide state and context to the agent. The framework includes several environments, such as a <code>ShellLocalEnv</code> for filesystem access and a <code>BrowserEnv</code> for web interaction.</p>"},{"location":"#toolkits-toolsmd","title":"Toolkits (<code>tools.md</code>)","text":"<p>Toolkits are collections of tools that grant agents their capabilities. The framework comes with a rich set of pre-built toolkits for web search, file manipulation, code execution, document analysis, and more.</p>"},{"location":"#evaluation-framework-evalmd","title":"Evaluation Framework (<code>eval.md</code>)","text":"<p>A comprehensive framework for benchmarking agent performance. It provides a standardized pipeline for: 1.  Data Management: Persisting and tracking evaluation data. 2.  Processing: Standardizing benchmark-specific logic. 3.  Execution: Running the agent and judging its performance automatically.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>For a step-by-step guide on how to install dependencies, set up your environment, and run your first agent, please see our Quickstart Guide.</p>"},{"location":"agents/","title":"Agent Paradigms","text":"<p>The framework provides two distinct agent paradigms to handle tasks of varying complexity: a straightforward <code>SimpleAgent</code> and a more advanced <code>OrchestraAgent</code>. The choice of paradigm is determined by the <code>type</code> field in the <code>AgentConfig</code>.</p> <p>The <code>get_agent</code> factory function is the primary entry point for creating an agent instance based on the specified configuration.</p> <pre><code>from utu.agents import get_agent\nfrom utu.config import ConfigLoader\n\n# Load a config file where type is set to \"simple\" or \"orchestra\"\nconfig = ConfigLoader.load_agent_config(\"my_agent_config\")\n\n# The factory returns the correct agent instance\nagent = get_agent(config)\n</code></pre>"},{"location":"agents/#simpleagent-single-agent-paradigm","title":"<code>SimpleAgent</code> (Single-Agent Paradigm)","text":"<p><code>SimpleAgent</code> implements a classic, single-agent approach based on the Reason-Act (ReAct) framework.</p>"},{"location":"agents/#how-it-works","title":"How it Works","text":"<p>A single LLM is responsible for the entire task-solving process. It operates in a loop:</p> <ol> <li>Reason: The agent analyzes the current task and its context.</li> <li>Act: Based on its reasoning, it selects and invokes an appropriate tool.</li> <li>Observe: It observes the result from the tool and incorporates the new information into its context.</li> </ol> <p>This loop continues until the agent determines that the task is complete and generates a final answer.</p> <pre><code>graph TD\n    A[Start] --&gt; B{Reason about Task};\n    B --&gt; C{Select Tool};\n    C --&gt; D[Execute Tool];\n    D --&gt; E[Observe Result];\n    E --&gt; B;\n    B --&gt; F[Final Answer];</code></pre>"},{"location":"agents/#use-case","title":"Use Case","text":"<p>This paradigm is best suited for straightforward tasks that can be solved by a linear sequence of tool calls and do not require complex, long-term planning or the coordination of different specialized skills.</p>"},{"location":"agents/#usage-examples","title":"Usage Examples","text":"<p>Here are a few examples of how to use <code>SimpleAgent</code>.</p>"},{"location":"agents/#1-basic-usage","title":"1. Basic Usage","text":"<p>You can instantiate <code>SimpleAgent</code> directly and override its configuration, such as the instructions, in the constructor.</p> <pre><code>from utu.agents import SimpleAgent\n\nasync with SimpleAgent(instructions=\"Always answeer with prefix `Aloha!`\") as agent:\n    await agent.chat(\"That's the weather in Beijing today?\")\n</code></pre> <p>Besides <code>chat</code>, you can also use the <code>chat_streamed</code> method for streaming output.</p>"},{"location":"agents/#2-using-built-in-tools","title":"2. Using Built-in Tools","text":"<p>For more complex behavior, you can use a YAML configuration file to define the agent and its toolkits.</p> <p>Python Code: <pre><code>from utu.agents import SimpleAgent\n\nasync with SimpleAgent(config=\"sample_tool.yaml\") as agent:\n    await agent.chat(\"That's the weather in Beijing today?\")\n</code></pre></p> <p>YAML Configuration (<code>configs/agents/sample_tool.yaml</code>): <pre><code># configs/agents/sample_tool.yaml\ndefaults:\n  - /model/base\n  - /tools/search@toolkits.search # Loads the 'search' toolkit\n  - _self_\n\nagent:\n    name: simple-tool-agent\n    instructions: \"You are a helpful assistant that can search the web.\"\n</code></pre></p>"},{"location":"agents/#3-using-mcp-based-tools","title":"3. Using MCP-based Tools","text":"<p><code>SimpleAgent</code> can also connect to tools running as separate processes via the Multi-Component Protocol (MCP). This is configured by setting the toolkit <code>mode</code> to <code>mcp</code>.</p> <p>Python Code: <pre><code>from utu.agents import SimpleAgent\n\nasync with SimpleAgent(config=\"sample_mcp.yaml\") as agent:\n    await agent.chat(\"That's the time now\")\n</code></pre></p> <p>YAML Configuration (<code>configs/agents/sample_mcp.yaml</code>): <pre><code># configs/agents/sample_mcp.yaml\ndefaults:\n  - /model/base\n  - _self_\n\ntoolkits:\n  time:\n    name: time\n    mode: mcp\n    activated_tools: null\n    config:\n      command: uvx\n      args: [\"mcp-server-time\", \"--local-timezone=Asia/Shanghai\"]\n\nagent:\n    name: simple-mcp\n    instructions: \"Always answeer with prefix `Aloha!`\"\n</code></pre></p>"},{"location":"agents/#orchestraagent-plan-and-execute-paradigm","title":"<code>OrchestraAgent</code> (Plan-and-Execute Paradigm)","text":"<p><code>OrchestraAgent</code> implements a more sophisticated, multi-agent paradigm based on the \"Plan-and-Execute\" model. It decomposes a complex task into smaller subtasks and orchestrates a team of specialized agents to solve them.</p>"},{"location":"agents/#architecture","title":"Architecture","text":"<p>The orchestra consists of three distinct roles:</p> <ol> <li> <p>Planner Agent: The \"brain\" of the operation. It receives the user's high-level goal and its sole responsibility is to create a detailed, step-by-step plan. Each step in the plan is a subtask assigned to a specific worker.</p> </li> <li> <p>Worker Agent(s): The \"hands\" of the operation. Each worker is a specialized <code>SimpleAgent</code> equipped with a specific set of tools (e.g., a <code>SearchWorker</code> with web search tools, a <code>CodeWorker</code> with file system and code execution tools). A worker receives a single subtask from the plan and executes it.</p> </li> <li> <p>Reporter Agent: The \"mouth\" of the operation. After all subtasks are completed by the workers, the reporter gathers all the results and synthesizes them into a single, coherent, final answer for the user.</p> </li> </ol>"},{"location":"agents/#workflow","title":"Workflow","text":"<p>The process is a clear, sequential flow:</p> <ol> <li>Plan: The Planner creates a multi-step plan.</li> <li>Work: The <code>OrchestraAgent</code> iterates through the plan, dispatching each subtask to the designated Worker and collecting the result.</li> <li>Report: The Reporter synthesizes the collected results into the final answer.</li> </ol> <pre><code>graph TD\n    A[User Task] --&gt; B[Planner Agent];\n    B --&gt; C(Plan: Subtask 1, 2, ...);\n    C --&gt; D[Worker Agents];\n    D --&gt; E{Execute Subtasks Sequentially};\n    E --&gt; F[Collected Results];\n    F --&gt; G[Reporter Agent];\n    G --&gt; H[Final Answer];</code></pre>"},{"location":"agents/#use-case_1","title":"Use Case","text":"<p>This paradigm excels at complex, multi-step tasks that require long-term planning, the use of different specialized tools, or the coordination of multiple skills. By decomposing the problem, it can tackle challenges that would be difficult for a single agent to manage.</p>"},{"location":"config/","title":"Configuration System","text":"<p>The project's configuration is managed through a system based on <code>pydantic</code> for data validation and <code>hydra</code> for loading from YAML files. This provides a powerful and flexible way to define agents and experiments.</p> <p>All configurations are stored as <code>.yaml</code> files inside the <code>/configs</code> directory.</p>"},{"location":"config/#configloader","title":"<code>ConfigLoader</code>","text":"<p>The <code>ConfigLoader</code> is the main entry point for loading configurations from YAML files into <code>pydantic</code> models. It abstracts away the file paths and loading logic.</p> <p>Usage: <pre><code>from utu.config import ConfigLoader\n\n# Load an agent configuration from /configs/agents/my_agent.yaml\nagent_config = ConfigLoader.load_agent_config(\"my_agent\")\n\n# Load an evaluation configuration from /configs/eval/my_eval.yaml\neval_config = ConfigLoader.load_eval_config(\"my_eval\")\n</code></pre></p>"},{"location":"config/#agentconfig","title":"<code>AgentConfig</code>","text":"<p><code>AgentConfig</code> is the central data structure for defining an agent. It specifies everything the agent needs to operate, including its model, tools, and personality.</p>"},{"location":"config/#key-components","title":"Key Components:","text":"<ul> <li><code>type</code>: The agent's architecture. Can be:<ul> <li><code>simple</code>: A single agent that performs a task.</li> <li><code>orchestra</code>: A more complex, multi-agent system with a planner and workers.</li> </ul> </li> <li><code>model</code>: (<code>ModelConfigs</code>) Defines the primary LLM the agent will use, including the API provider, model name, and settings like temperature.</li> <li><code>agent</code>: (<code>ProfileConfig</code>) Defines the agent's profile, such as its <code>name</code> and system-level <code>instructions</code> (e.g., \"You are a helpful assistant.\").</li> <li><code>env</code>: (<code>EnvConfig</code>) Specifies the environment the agent operates in (e.g., <code>shell_local</code> or <code>browser_docker</code>). See Agent Environments for more details.</li> <li><code>toolkits</code>: (<code>dict[str, ToolkitConfig]</code>) A dictionary defining the tools available to the agent. Each toolkit can be loaded in <code>builtin</code> mode (running in the main process) or <code>mcp</code> mode (running as a separate process).</li> <li><code>max_turns</code>: The maximum number of conversational turns the agent can take before stopping.</li> </ul> <p>For the <code>orchestra</code> type, <code>AgentConfig</code> also includes fields for defining the planner, workers, and reporter agents.</p>"},{"location":"config/#evalconfig","title":"<code>EvalConfig</code>","text":"<p><code>EvalConfig</code> defines a complete evaluation experiment. It specifies the dataset to use, the agent to test, and how to judge the results.</p>"},{"location":"config/#key-components_1","title":"Key Components:","text":"<ul> <li><code>data</code>: (<code>DataConfig</code>) Defines the dataset to be used for the evaluation, including its name/path and the relevant fields (<code>question_field</code>, <code>gt_field</code>).</li> <li><code>rollout</code>: This section defines the execution phase of the evaluation.<ul> <li><code>agent</code>: (<code>AgentConfig</code>) A full <code>AgentConfig</code> for the agent being tested.</li> <li><code>concurrency</code>: The number of parallel processes to use when running the agent on the dataset.</li> </ul> </li> <li><code>judgement</code>: This section defines the judgment phase.<ul> <li><code>judge_model</code>: (<code>ModelConfigs</code>) The configuration for the LLM that will act as the judge.</li> <li><code>judge_concurrency</code>: The number of parallel processes to use for judging the results.</li> <li><code>eval_method</code>: The method used for evaluation (e.g., comparing against a ground truth answer).</li> </ul> </li> </ul>"},{"location":"env/","title":"Agent Environments","text":"<p>An Environment (<code>Env</code>) represents the world in which the agent operates. Its primary responsibilities are to provide the agent with a sense of its current state and a set of tools to interact with that world.</p> <p>The framework uses a factory function, <code>get_env</code>, to create the appropriate environment based on the agent's configuration file.</p>"},{"location":"env/#core-concepts","title":"Core Concepts","text":"<p>All environments inherit from the abstract base class <code>Env</code>, which defines the core interface:</p> <ul> <li><code>get_state() -&gt; str</code>: Returns a string describing the current state of the environment. This information is injected into the agent's prompt to provide context.</li> <li><code>get_tools() -&gt; list[Tool]</code>: Returns a list of <code>Tool</code> objects that the agent can use to interact with the environment.</li> <li><code>build()</code> / <code>cleanup()</code>: Methods to manage the lifecycle of the environment, such as starting services or cleaning up resources.</li> </ul>"},{"location":"env/#available-environments","title":"Available Environments","text":"<p>Here are the currently available environment implementations.</p>"},{"location":"env/#baseenv","title":"<code>BaseEnv</code>","text":"<p>This is the simplest and default environment. It can be considered a \"null\" environment.</p> <ul> <li>State: Provides no state information (returns an empty string).</li> <li>Tools: Provides no tools (returns an empty list).</li> <li>Use Case: Used when the agent's task does not require any specific environmental interaction.</li> </ul>"},{"location":"env/#shelllocalenv","title":"<code>ShellLocalEnv</code>","text":"<p>This environment provides the agent with an isolated workspace on the local filesystem.</p> <ul> <li>State: The state string includes the current time, the absolute path to the isolated workspace, and a crucial instruction for the agent: <code>You can only run bash commands in your workspace!!!</code>. This helps guide and constrain the agent's behavior.</li> <li>Tools: This environment does not provide tools directly. The agent must be configured separately with tools capable of executing shell commands (e.g., a <code>bash</code> tool). The environment's role is to provide the context and workspace for those tools.</li> <li>Isolation: A unique workspace directory is created for each run session, preventing interference between different tasks.</li> </ul>"},{"location":"env/#browserenv","title":"<code>BrowserEnv</code>","text":"<p>This is a powerful environment that gives the agent control over a fully-featured, interactive web browser.</p> <ul> <li>Architecture: <code>BrowserEnv</code> runs a browser automation service inside a Docker container. This ensures that each agent session is completely isolated and has a clean, predictable browser environment.</li> <li>State: The state represents the current content of the web page. It is updated after every action (e.g., clicking an element, navigating to a URL), giving the agent feedback on the result of its last action.</li> <li>Tools: Tools are provided dynamically by the browser service running in the container. <code>BrowserEnv</code> acts as a proxy: it discovers the available tools (e.g., <code>go_to_url</code>, <code>click_element</code>, <code>input_text</code>) and makes them available to the agent. When the agent calls a tool, <code>BrowserEnv</code> forwards the request to the Docker container for execution.</li> </ul>"},{"location":"env/#configuration","title":"Configuration","text":"<p>You can specify which environment to use in your agent's configuration file:</p> <pre><code># In your agent configuration file\nenv:\n  name: shell_local  # Or: browser_docker, base\n</code></pre>"},{"location":"eval/","title":"Evaluation Framework","text":"<p>The <code>utu/eval/</code> module provides a standardized and extensible framework for benchmarking agents. It is designed to be modular, allowing for the easy addition of new datasets and evaluation methods. The entire workflow is orchestrated by <code>BaseBenchmark</code>, which ties together data management, benchmark-specific logic, and the agent being tested.</p> <p>Here are the three core components of the framework:</p>"},{"location":"eval/#1-data-management-dbdatamanager","title":"1. Data Management (<code>DBDataManager</code>)","text":"<p>The <code>DBDataManager</code> is the persistence layer of the evaluation framework. It is responsible for loading datasets and tracking the state of every sample throughout the evaluation process.</p> <ul> <li>Database Backend: It uses a database (defaulting to SQLite) to store all evaluation data. Each sample is stored as an <code>EvaluationSample</code> record.</li> <li>Experiment Tracking: All samples are associated with an <code>exp_id</code> (Experiment ID). This allows for easy tracking and resuming of experiments. If you run an evaluation with an existing <code>exp_id</code>, the system will pick up where it left off.</li> <li>Stateful Tracking: Each <code>EvaluationSample</code> has a <code>stage</code> field (<code>init</code>, <code>rollout</code>, <code>judged</code>) that tracks its progress through the pipeline. This ensures that each step of the evaluation only processes the relevant samples.</li> </ul>"},{"location":"eval/#2-dataset-standardization-processer","title":"2. Dataset Standardization (<code>Processer</code>)","text":"<p>A <code>Processer</code> handles all the logic that is specific to a particular benchmark (e.g., GAIA, BrowseComp). This design cleanly separates the generic evaluation flow from the details of each dataset.</p> <ul> <li><code>BaseProcesser</code> Interface: This abstract class defines the contract for all processers, which must implement:<ul> <li><code>preprocess_one()</code>: Prepares a raw data sample for the agent. This can involve reformatting the question, adding specific instructions, or attaching file paths.</li> <li><code>judge_one()</code>: Evaluates an agent's response for a single sample.</li> <li><code>calculate_metrics()</code>: Computes the final summary statistics for a benchmark.</li> </ul> </li> <li>Judging Strategies: The framework provides different base implementations for judging:<ul> <li><code>BaseLLMJudgeProcesser</code>: Uses a powerful LLM as a judge, guided by specialized prompt templates. This is suitable for complex, open-ended questions.</li> <li><code>BaseMatchProcesser</code>: Uses rule-based methods (e.g., exact string or number matching) for judging. This is faster and more suitable for questions with a single, precise answer.</li> </ul> </li> <li><code>PROCESSER_FACTORY</code>: A factory that automatically discovers and registers all available processers. The <code>BaseBenchmark</code> uses this factory to dynamically select the correct processer for each sample based on its <code>source</code> field.</li> </ul>"},{"location":"eval/#3-standardized-test-flow-basebenchmark","title":"3. Standardized Test Flow (<code>BaseBenchmark</code>)","text":"<p><code>BaseBenchmark</code> is the main orchestrator that drives the entire evaluation pipeline from start to finish. It provides a standardized, four-stage process.</p> <ul> <li><code>preprocess</code>: Loads all initial samples from the <code>DBDataManager</code> and uses the appropriate <code>Processer</code> to prepare them for the agent.</li> <li><code>rollout</code>: Runs the configured agent on all the preprocessed samples. The agent's response, trajectory, and other metadata are saved back to the database for each sample.</li> <li><code>judge</code>: Fetches the completed samples from the <code>rollout</code> stage and uses the <code>Processer</code> to evaluate whether the agent's response was correct.</li> <li><code>stat</code>: Gathers all judged samples, groups them by benchmark, and uses the <code>Processer</code> to calculate and log the final metrics (e.g., accuracy).</li> </ul> <p>This structured pipeline ensures that every evaluation is consistent, automated, and resilient, as it can be stopped and resumed at any stage.</p>"},{"location":"examples/","title":"Examples","text":"<p>Check out examples in the <code>examples</code> directory.</p> Example Core Architecture Implementation Key Tools Use Case / Features <code>research</code> Manual Multi-Agent Orchestration Orchestrates 3 independent <code>SimpleAgent</code>s in code to create a \"Plan-and-Execute\" workflow. <code>SearchToolkit</code> Demonstrates building a complex workflow from scratch using basic SimpleAgent blocks. <code>wide_research</code> \"Agent-as-Tool\" Pattern A single <code>SimpleAgent</code> makes decisions and calls a custom tool that encapsulates parallel sub-agents. <code>SearchToolkit</code> Shows how to encapsulate parallelism and complex logic within a tool, simplifying the main agent's logic. <code>paper_collector</code> Standard OrchestraAgent Configuration-driven; uses few-shot examples (<code>planner_examples_data.json</code>) to guide the Planner. <code>DocumentToolkit</code>, <code>SearchToolkit</code> A standard, \"out-of-the-box\" application of OrchestraAgent for multi-step document analysis. <code>file_manager</code> SimpleAgent + UI A configuration-driven <code>SimpleAgent</code> wrapped in an interactive <code>Gradio</code> web UI. <code>BashToolkit</code> A practical example of an agent interacting with the local file system, with a focus on UI and safety. <code>data_analysis</code> Customized OrchestraAgent Extends OrchestraAgent's core components: 1. <code>DAPlannerAgent</code>: Proactively inspects data schema before planning. 2. Reporter: Uses a custom template to generate a rich HTML report. <code>TabularDataToolkit</code> An advanced OrchestraAgent use case, showing deep customization for a complex, domain-specific problem."},{"location":"quickstart/","title":"Quickstart","text":"<p>This guide will walk you through setting up the project, running your first agent, and executing evaluations.</p>"},{"location":"quickstart/#1-installation-setup","title":"1. Installation &amp; Setup","text":"<p>First, clone the repository and set up the Python environment.</p> <pre><code># Clone the project repository\ngit clone https://github.com/Tencent/uTu-agent.git\ncd uTu-agent\n\n# We use `uv` to manage the virtual environment and dependencies\n# Create the virtual environment\nuv venv\n\n# Activate the environment\nsource .venv/bin/activate\n\n# Install all dependencies, including development tools\nuv sync --group dev\n\n# Create your environment configuration file from the example\ncp .env.example .env\n</code></pre> <p>After creating the <code>.env</code> file, you must edit it to add your necessary API keys (e.g., <code>OPENAI_API_KEY</code>, <code>SERPER_API_KEY</code>, etc.).</p>"},{"location":"quickstart/#2-running-an-agent","title":"2. Running an Agent","text":"<p>You can interact with agents directly from the command line using the <code>cli_chat.py</code> script.</p>"},{"location":"quickstart/#simple-agent","title":"Simple Agent","text":"<p>Run a simple agent defined by a configuration file. For example, to run an agent with search capabilities:</p> <pre><code># python scripts/cli_chat.py --help\npython scripts/cli_chat.py --config_name simple_agents/search_agent.yaml --stream\n</code></pre>"},{"location":"quickstart/#orchestra-agent","title":"Orchestra Agent","text":"<p>Run a multi-agent (Plan-and-Execute) orchestra agent by specifying its configuration file:</p> <pre><code># TODO: add a web UI for orchestra agent\n</code></pre>"},{"location":"quickstart/#3-running-evaluations","title":"3. Running Evaluations","text":"<p>The framework includes a powerful evaluation harness to benchmark agent performance.</p>"},{"location":"quickstart/#run-a-full-experiment","title":"Run a Full Experiment","text":"<p>This command runs a complete evaluation, from agent rollout to judging.</p> <pre><code>python scripts/run_eval.py --config_name &lt;your_eval_config&gt; --exp_id &lt;your_exp_id&gt; --dataset WebWalkerQA --concurrency 5\n</code></pre>"},{"location":"quickstart/#re-judge-existing-results","title":"Re-judge Existing Results","text":"<p>If you have already run the rollout and only want to re-run the judgment phase, use this script:</p> <pre><code>python scripts/run_eval_judge.py --config_name &lt;your_eval_config&gt; --exp_id &lt;your_exp_id&gt; --dataset WebWalkerQA\n</code></pre>"},{"location":"quickstart/#dump-experiment-data","title":"Dump Experiment Data","text":"<p>You can also dump the trajectories and results from the database for a specific experiment:</p> <pre><code>python scripts/db/dump_db.py --exp_id \"&lt;your_exp_id&gt;\"\n</code></pre>"},{"location":"quickstart/#4-advanced-setup","title":"4. Advanced Setup","text":""},{"location":"quickstart/#database-configuration","title":"Database Configuration","text":"<p>The evaluation framework uses a SQL database (defaulting to SQLite) to store datasets and experiment results. To use a different database (e.g., PostgreSQL), set the <code>DB_URL</code> environment variable:</p> <pre><code>export DB_URL=\"postgresql://user:password@host:port/database\"\n</code></pre>"},{"location":"quickstart/#tracing","title":"Tracing","text":"<p>We use Phoenix as our default tracing service for observing agent behavior. To enable it, set the following environment variables: - <code>PHOENIX_ENDPOINT</code> - <code>PHOENIX_BASE_URL</code> - <code>PHOENIX_PROJECT_NAME</code></p> <p>The framework also supports any tracing service compatible with the <code>openai-agents</code> library. See the official list of tracing processors for more options.</p>"},{"location":"quickstart/#5-next-steps","title":"5. Next Steps","text":"<ul> <li>Explore Examples: Check the <code>/examples</code> directory for more detailed use cases and advanced scripts.</li> <li>Dive into Evaluations: Learn more about how the evaluation framework works by reading the Evaluation Framework documentation.</li> </ul>"},{"location":"tools/","title":"Toolkits","text":"<p>Toolkits are collections of related tools that an agent can use to perform actions. They are the primary way to extend an agent's capabilities.</p>"},{"location":"tools/#asyncbasetoolkit","title":"<code>AsyncBaseToolkit</code>","text":"<p>All toolkits inherit from the <code>AsyncBaseToolkit</code> abstract base class. This class provides a standardized interface for creating and managing tools. The core requirement for any toolkit is to implement the <code>get_tools_map()</code> method, which returns a dictionary mapping tool names to their corresponding Python functions.</p> <p>The base class automatically handles the conversion of these functions into <code>FunctionTool</code> objects that the agent runner can understand and execute.</p> <p>All available toolkits are registered in the <code>TOOLKIT_MAP</code> dictionary within <code>utu/tools/__init__.py</code>.</p>"},{"location":"tools/#summary-of-core-toolkits","title":"Summary of Core Toolkits","text":"<p>Here is a summary of some key toolkits available in the framework:</p> Toolkit Class Provided Tools (Functions) Core Functionality &amp; Mechanism SearchToolkit <code>search_google_api</code>, <code>web_qa</code> Performs web searches using the Serper API and reads webpage content using the Jina API. It can use an LLM to answer questions based on page content. DocumentToolkit <code>document_qa</code> Processes local or remote documents (PDF, DOCX, etc.). It uses the <code>chunkr.ai</code> service to parse the document and an LLM to answer questions or provide a summary. PythonExecutorToolkit <code>execute_python_code</code> Executes Python code snippets in an isolated environment using <code>IPython.core.interactiveshell</code>. It runs in a separate thread to prevent blocking and can capture outputs, errors, and even <code>matplotlib</code> plots. BashToolkit <code>run_bash</code> Provides a persistent local shell session using the <code>pexpect</code> library. This allows the agent to run a series of commands that maintain state (e.g., current directory). ImageToolkit <code>image_qa</code> Answers questions about an image or provides a detailed description. It uses a vision-capable LLM to analyze the image content. AudioToolkit <code>audio_qa</code> Transcribes audio files using an audio model and then uses an LLM to answer questions based on the transcription. CodesnipToolkit <code>run_code</code> Executes code in various languages (Python, C++, JS, etc.) by sending it to a remote sandbox service (like SandboxFusion) and returning the result."},{"location":"examples_output/data_analysis/","title":"Data analysis","text":""},{"location":"examples_output/deep_research/","title":"LLM Tool Use \u2013 A Comprehensive Technical Report","text":"<p>Prepared by: Senior Researcher \u2013 Aug\u202f2025</p>"},{"location":"examples_output/deep_research/#table-of-contents","title":"Table of Contents","text":"<ul> <li>LLM Tool Use \u2013 A Comprehensive Technical Report<ul> <li>Table of Contents</li> <li>1\ufe0f\u20e3 Executive Summary</li> <li>2\ufe0f\u20e3 Foundations of LLM\u2011Driven Tool Use<ul> <li>2.1 OpenAI Function\u2011Calling (now \u201cTools\u201d)</li> <li>2.2 LangChain\u2019s Tool Interface</li> <li>2.3 ChatGPT / GPT\u20114\u202fPlugins (Custom\u2011Tool Plugins)</li> </ul> </li> <li>3\ufe0f\u20e3 Interaction Design for Tool\u2011Enabled Agents<ul> <li>3.1 UI Taxonomy \\&amp; Divergent\u2011Convergent Workflows</li> <li>3.2 Canvas\u2011Based Exploration Model (Extended OntoChat)</li> </ul> </li> <li>4\ufe0f\u20e3 Security \\&amp; Safety Considerations<ul> <li>4.1 Threat Landscape</li> <li>4.2 Sandboxing \\&amp; Isolation Strategies</li> <li>4.3 Validation \\&amp; Policy Enforcement Frameworks</li> </ul> </li> <li>5\ufe0f\u20e3 Cost\u2011Aware \\&amp; Latency\u2011Optimised Engineering<ul> <li>5.1 Model\u2011Level Tool\u2011Cost Penalties</li> <li>5.2 Runtime Optimisations (Engineering Tactics)</li> </ul> </li> <li>6\ufe0f\u20e3 Evaluation Metrics \\&amp; Benchmarks<ul> <li>6.1 Core Metrics</li> <li>6.2 Benchmark Suites</li> </ul> </li> <li>7\ufe0f\u20e3 Best\u2011Practice Guideline for Production Deployments<ul> <li>7.1 Prompt Engineering \\&amp; System\u2011Prompt Tool Disclosure</li> <li>7.2 Schema Design \\&amp; Strict Mode</li> <li>7.3 Observability, Logging \\&amp; Version Control</li> <li>7.4 Runtime DSL \\&amp; Parser Integration</li> <li>7.5 Safety Gate\u2011keeping (Defense\u2011in\u2011Depth)</li> </ul> </li> <li>8\ufe0f\u20e3 Case Studies \\&amp; Real\u2011World Deployments<ul> <li>8.1 Industrial PDF\u2011Extraction Agent (AID\u2011agent, 2025)</li> <li>8.2 Finance Bot (Real\u2011time Stock Insight)</li> <li>8.3 Travel Assistant</li> <li>8.4 Legal Clause Analyzer</li> <li>8.5 Market\u2011Research Synthesizer</li> <li>8.6 ReAct / ZERO_SHOT_REACT as a Generic Pattern</li> </ul> </li> <li>9\ufe0f\u20e3 Future Directions \\&amp; Open Research Questions</li> <li>\ud83d\udd1f References \\&amp; Further Reading</li> </ul> </li> </ul>"},{"location":"examples_output/deep_research/#1-executive-summary","title":"1\ufe0f\u20e3 Executive Summary","text":"<p>Large Language Models (LLMs) have moved beyond pure text generation to become orchestrators of external tools \u2013 search engines, code interpreters, database connectors, custom APIs, and even physical device controllers.  This shift unlocks real\u2011world utility (e.g., up\u2011to\u2011date weather, transactional finance) while also introducing new engineering challenges: schema definition, reliable invocation, latency, cost, security, and evaluation.</p> <p>This report synthesises the most recent public guidance (OpenAI function\u2011calling, Azure\u202fOpenAI, LangChain, ChatGPT plugin pipeline), interaction\u2011design research, security taxonomies, cost\u2011aware training strategies, benchmark suites, and production case studies.  It culminates in a concrete best\u2011practice playbook that developers can adopt for robust, maintainable, and scalable LLM\u2011augmented services.</p>"},{"location":"examples_output/deep_research/#2-foundations-of-llmdriven-tool-use","title":"2\ufe0f\u20e3 Foundations of LLM\u2011Driven Tool Use","text":""},{"location":"examples_output/deep_research/#21-openai-functioncalling-now-tools","title":"2.1 OpenAI Function\u2011Calling (now \u201cTools\u201d)","text":"Element Description Key Tips Tool definition JSON\u2011Schema\u2011like object under the <code>tools</code> array: <code>type</code>, <code>name</code>, <code>description</code>, <code>parameters</code> (object with <code>properties</code>, <code>required</code>, optional <code>additionalProperties:false</code>). Use <code>strict:true</code> to enforce exact schema compliance; keep schemas shallow to minimise token usage. Call flow 1\ufe0f\u20e3 Send request with tools. 2\ufe0f\u20e3 Model may respond with <code>tool_call</code> (name + arguments). 3\ufe0f\u20e3 Application executes the function, captures output, and returns <code>tool_call_output</code>. 4\ufe0f\u20e3 Model receives the observation and can continue reasoning. 5\ufe0f\u20e3 Final response is emitted. Follow the 5\u2011step loop; treat the <code>tool_call_output</code> as a new observation in the conversation. Configuration <code>tool_choice</code> (<code>auto</code>, <code>required</code>, specific name, or <code>none</code>). <code>parallel_tool_calls</code> (default true) \u2013 enables multiple tool calls in a single turn. Set <code>tool_choice=required</code> for deterministic workflows; disable parallel calls when ordering matters. Streaming <code>stream:true</code> sends incremental JSON fragments and a distinct <code>function_call_output</code> event. Useful for UI\u2011side low\u2011latency UX; buffer until a complete JSON is received. Best\u2011practice highlights \u2013 Clear, concise tool names &amp; descriptions. \u2013 \u2264\u202f20 tools per request (token budget). \u2013 Mark enumerations (<code>enum</code>) and required fields. \u2013 Keep parameter types unambiguous (avoid <code>any</code>). \u2013 Place static context outside the schema. See Section\u202f7 for a checklist. <p>Security note: Even with strict mode, the model can be coaxed into \u201cprompt\u2011injection\u201d attacks that try to trick it into constructing malicious arguments. Validation should occur after the model emits the call, before executing any side\u2011effect. </p>"},{"location":"examples_output/deep_research/#22-langchains-tool-interface","title":"2.2 LangChain\u2019s Tool Interface","text":"<p>LangChain abstracts the raw API contract into a Tool class: <pre><code>class Tool:\n    name: str\n    description: str\n    func: Callable[[str], str]\n</code></pre> * Binding \u2013 <code>.bind_tools([...])</code> attaches a list of tools to a language model or an agent. The model decides when to call them, similarly to OpenAI\u2019s <code>tool_choice=auto</code>. * Toolkits \u2013 Groups of related tools (e.g., SearchToolkit, SQLDatabaseToolkit) simplify onboarding; they expose a unified schema to the LLM. * Ecosystem categories (per the LangChain docs):   - Search \u2013 Bing, Google, DuckDuckGo, Serper, Tavily.   - Code Interpreter \u2013 Azure Container Apps, Bearly, Riza.   - Productivity \u2013 GitHub, Gmail, Jira, Slack, Twilio.   - Web Browsing \u2013 Playwright, Hyperbrowser, pure <code>requests</code>.   - Database \u2013 SQLDatabase, Cassandra, Spark SQL.   - Finance \u2013 GOAT, Stripe wrappers. * Custom tools \u2013 Implement the <code>Tool</code> interface, provide a JSON schema or use LangChain\u2019s <code>StructuredTool</code> for automatic validation.</p> <p>LangChain thus decouples the LLM\u2011model from the transport layer, letting developers focus on function semantics while the library handles prompting, retry logic, and parallelisation.</p>"},{"location":"examples_output/deep_research/#23-chatgpt-gpt4-plugins-customtool-plugins","title":"2.3 ChatGPT / GPT\u20114\u202fPlugins (Custom\u2011Tool Plugins)","text":"<p>A four\u2011step pipeline moves an arbitrary HTTP API into a first\u2011class LLM tool: 1. Expose the desired functionality as a public HTTPS endpoint (REST/GraphQL). 2. Write an OpenAPI (Swagger) spec that fully describes routes, parameters, auth, response schemas. 3. Create <code>ai-plugin.json</code> manifest: name, description, <code>openapi_url</code>, authentication method, icons, usage instructions. 4. Register the plugin on the OpenAI developer portal (requires ChatGPT\u2011Plus or wait\u2011list). After verification, the model can invoke the API automatically.</p> <p>Implementation tip: a minimal Flask app with a single route, environment\u2011protected API keys, and deployment on a public HTTPS host (Vercel, Railway, Repl.it) is sufficient for prototyping.</p> <p>No\u2011code alternatives (Plus AI, BotPenguin, custom GPT builders) auto\u2011generate the OpenAPI spec and manifest, but the core requirements (reachable API, compliant spec, manifest) remain unchanged.</p>"},{"location":"examples_output/deep_research/#3-interaction-design-for-toolenabled-agents","title":"3\ufe0f\u20e3 Interaction Design for Tool\u2011Enabled Agents","text":""},{"location":"examples_output/deep_research/#31-ui-taxonomy-divergentconvergent-workflows","title":"3.1 UI Taxonomy &amp; Divergent\u2011Convergent Workflows","text":"<p>Recent HCI research proposes a taxonomy of UI patterns that support the divergent \u2192 convergent workflow typical of LLM tool use:</p> Pattern Description Example Implementations Spatial navigation (pan/zoom canvas) Users explore a 2\u2011D plane where each node = an LLM\u2011generated action or tool call. Luminate, Spellburst canvas graphs Zoom\u2011and\u2011filter lists List/grid view with dynamic filters; supports quick pruning of irrelevant suggestions. Genquery, adaptive suggestion panels Node\u2011based linking / brushing Drag\u2011and\u2011drop connections between actions, visualising dependencies (e.g., \u201cfetch weather \u2192 summarize\u201d). Node\u2011graph editors in language\u2011agent IDEs Details\u2011on\u2011demand tooltips Hover cards reveal full JSON arguments, execution logs, and allow inline edits. Tooltip\u2011driven editing in Promptify Parameter sliders Real\u2011time manipulation of numeric or categorical parameters (temperature, top\u2011p, tool\u2011specific thresholds). Slider controls in LangChain Playground <p>These patterns embody Shneiderman\u2019s mantra overview \u2192 zoom &amp; filter \u2192 details\u2011on\u2011demand, encouraging users to generate many alternatives (divergent) and then focus on a refined subset (convergent).</p>"},{"location":"examples_output/deep_research/#32-canvasbased-exploration-model-extended-ontochat","title":"3.2 Canvas\u2011Based Exploration Model (Extended OntoChat)","text":"<p>A concrete interaction model builds on the OntoChat system: 1. Seed: User provides a domain description (e.g., \u201csupplier metadata extraction\u201d). 2. Generation: LLM produces a set of candidate actions, plotted on a 2\u2011D canvas. 3. Explore: Users pan/zoom for an overview; clicking a region triggers augmentation \u2013 the LLM creates more actions focused on that semantic zone. 4. Filter: Semantic or keyword search highlights relevant items. 5. Inspect: Selecting an item opens a tooltip with full JSON arguments and a preview of tool output. 6. Edit &amp; Iterate: Edits are sent back to the LLM, which refines the plan, possibly adding new tool calls.</p> <p>The canvas\u2011plus\u2011inline\u2011controls workflow keeps the user in a single surface, enabling rapid iteration without context switching, and works equally for exploratory research and production decision\u2011making.</p>"},{"location":"examples_output/deep_research/#4-security-safety-considerations","title":"4\ufe0f\u20e3 Security &amp; Safety Considerations","text":""},{"location":"examples_output/deep_research/#41-threat-landscape","title":"4.1 Threat Landscape","text":"Threat Impact Typical Trigger Prompt\u2011injection Malicious tool call, data exfiltration, arbitrary code execution. Attacker crafts user text that influences the model to generate a harmful <code>arguments</code> payload. Unrestricted file\u2011system / network access Reads/writes sensitive data, SSRF, DoS, exfiltration. Tool implementation inadvertently exposes OS\u2011level APIs. Cross\u2011tenant leakage One user\u2019s data appears in another\u2019s session. Shared inference service without per\u2011session isolation. Mobile/embedded agent hijack Device compromise, privacy breach. Agents that automate GUI actions or run background services."},{"location":"examples_output/deep_research/#42-sandboxing-isolation-strategies","title":"4.2 Sandboxing &amp; Isolation Strategies","text":"<ol> <li>Container\u2011level isolation \u2013 Run each tool invocation in lightweight containers (Docker, gVisor, Firecracker). Enforce:</li> <li>Read\u2011only file\u2011system mounts.</li> <li>Network egress filtering (allow only whitelisted destinations).</li> <li>Cgroup limits on CPU &amp; memory.</li> <li>Language\u2011level sandbox \u2013 Use restricted REPLs (e.g., Pyodide, Subprocess sandbox) for code execution; whitelist safe modules only.</li> <li>API Gateway Enforcement \u2013 Every external call passes through a gateway that validates:</li> <li>Authentication &amp; scoped permissions.</li> <li>Rate limiting.</li> <li>Auditable logs for anomaly detection.</li> <li>Per\u2011session memory isolation \u2013 Clear caches after each user session; encrypt any transient storage with short\u2011lived keys.</li> </ol>"},{"location":"examples_output/deep_research/#43-validation-policy-enforcement-frameworks","title":"4.3 Validation &amp; Policy Enforcement Frameworks","text":"Layer Mechanism Example Pre\u2011execution Schema validation (<code>strict:true</code>, JSON\u2011Schema) + safe\u2011argument filters (regex, whitelist). Reject arguments containing <code>rm -rf</code>, URLs not in allowed list. Runtime DSL Custom policy language (<code>\\tool</code> system) parsed via ANTLR4; triggers, predicates, actions (allow, ask\u2011user, abort). \u201cIf tool=<code>web_search</code> and query contains <code>password</code>, abort.\u201d Post\u2011execution Observation sanitisation \u2013 strip PII, limit length, redact secrets before feeding back to LLM. Continuous testing SandboxEval (malicious\u2011code suite) &amp; AgentScan (mobile vector) \u2013 run nightly CI pipelines to detect regressions. <p>The combination of hard sandboxing, strict schema enforcement, and automated security testing forms a defense\u2011in\u2011depth posture for production LLM\u2011tool pipelines.</p>"},{"location":"examples_output/deep_research/#5-costaware-latencyoptimised-engineering","title":"5\ufe0f\u20e3 Cost\u2011Aware &amp; Latency\u2011Optimised Engineering","text":""},{"location":"examples_output/deep_research/#51-modellevel-toolcost-penalties","title":"5.1 Model\u2011Level Tool\u2011Cost Penalties","text":"<p>Recent research (e.g., Alignment for Efficient Tool Calling) introduces an explicit tool\u2011cost penalty \u03b1 into the training loss: <pre><code>Loss_total = Loss_task + \u03b1 * Cost(tool_calls)\n</code></pre> Typical values: - \u03b1\u22480.2 for cheap calculators (local execution). - \u03b1\u22480.4 for web search. - \u03b1\u22480.6 for heavyweight external reasoning (e.g., invoking a separate LLM).</p> <p>Outcome: The model learns to avoid unnecessary tool calls, reducing latency and compute by up to \u224850\u202f% while preserving answer accuracy.</p>"},{"location":"examples_output/deep_research/#52-runtime-optimisations-engineering-tactics","title":"5.2 Runtime Optimisations (Engineering Tactics)","text":"Lever Description Expected Gains Parallel / speculative execution Launch moderation, retrieval, or computation in parallel with token generation; discard if later reasoning decides they\u2019re unnecessary. 20\u201130\u202f% lower wall\u2011clock time. Request consolidation Combine tool\u2011selection, argument preparation, and invocation into a single prompt to avoid multiple round\u2011trips. Fewer network RTTs \u2192 15\u201125\u202f% latency cut. Model tiering Route lightweight tool tasks (e.g., arithmetic) to smaller models (GPT\u20113.5, Claude\u202fSonnet) while delegating complex reasoning to larger models. Cost per token drops dramatically (up to 60\u202f%). Semantic caching &amp; batching Cache exact or high\u2011similarity tool responses (similarity\u202f&gt;\u202f0.95). Batch low\u2011priority calls to a shared endpoint. Repeated queries become essentially free; batch latency amortised. <p>Implementation tip: Provide a cost\u2011budget field in the system prompt (<code>{budget: 0.05 USD}</code>) and let the model self\u2011regulate; combine with the \u03b1\u2011penalty for a double\u2011layer guard.</p>"},{"location":"examples_output/deep_research/#6-evaluation-metrics-benchmarks","title":"6\ufe0f\u20e3 Evaluation Metrics &amp; Benchmarks","text":""},{"location":"examples_output/deep_research/#61-core-metrics","title":"6.1 Core Metrics","text":"<ol> <li>Tool Correctness \u2013 Exact\u2011match between the tool(s) the model should have called and the ones it actually called. \u2705 Binary or fractional.</li> <li>Tool Selection Accuracy \u2013 Node\u2011F1 (precision/recall on chosen tool nodes) and Edge\u2011F1 (ordering/dependency links).</li> <li>Invocation Accuracy \u2013 Did the model correctly decide whether a tool was needed?</li> <li>Parameter\u2011Name F1 \u2013 Precision/recall on argument field names.</li> <li>Argument Value Distance \u2013 Levenshtein distance or absolute error for numeric values.</li> <li>Tool Success Rate \u2013 Fraction of tool calls that executed without runtime error (important for real\u2011world reliability).</li> </ol>"},{"location":"examples_output/deep_research/#62-benchmark-suites","title":"6.2 Benchmark Suites","text":"Benchmark Size Domains Notable Features UltraTool (ACL\u202f2024) 5.8\u202fk samples, 22 domains, 2\u202f032 distinct tools Comprehensive plan\u2011step evaluation (accuracy, completeness, executability, syntactic soundness, structural rationality, efficiency). Multi\u2011dimensional scoring via LLM\u2011as\u2011Judge; reports nested calls (~40\u202f% of cases). TaskBench ~2\u202fk queries Focus on selection &amp; ordering of tool calls. Provides Node\u2011F1/Edge\u2011F1, Invocation Accuracy. T\u2011eval 1.5\u202fk samples Emphasises parameter filling quality. Parameter\u2011Name F1 + Levenshtein on values. <p>Open\u2011source models typically lag behind proprietary LLMs (GPT\u20114 \u2248\u202f76\u202f% UltraTool score) \u2013 highlighting an open research gap in tool awareness and schema adherence.</p>"},{"location":"examples_output/deep_research/#7-bestpractice-guideline-for-production-deployments","title":"7\ufe0f\u20e3 Best\u2011Practice Guideline for Production Deployments","text":"<p>Below is a step\u2011by\u2011step playbook that integrates the insights above.</p>"},{"location":"examples_output/deep_research/#71-prompt-engineering-systemprompt-tool-disclosure","title":"7.1 Prompt Engineering &amp; System\u2011Prompt Tool Disclosure","text":"<p><pre><code>System Prompt:\nYou are an assistant equipped with the following tools. Use a tool **only** when the user request cannot be answered from the conversation history.\n\nTOOLS:\n- `search_web(query: string, top_k: integer = 3) -&gt; list[dict]` \u2013 fetches up\u2011to\u2011date web results.\n- `calc(expression: string) -&gt; number` \u2013 safe arithmetic evaluator.\n- `pdf_extract(file_id: string, fields: list[string]) -&gt; dict` \u2013 extracts structured data from a PDF stored in the vector store.\n\nWhen you decide to call a tool, output **exactly** the JSON snippet shown in the example below.\n\nExample:\n{ \"tool\": \"search_web\", \"arguments\": { \"query\": \"latest S&amp;P 500 price\" } }\n</code></pre> Rationale: Embedding a short, human\u2011readable description of each tool inside the system prompt informs the model\u2019s semantic understanding, reducing missed calls.</p>"},{"location":"examples_output/deep_research/#72-schema-design-strict-mode","title":"7.2 Schema Design &amp; Strict Mode","text":"<ul> <li>Use JSON\u2011Schema Draft\u201107 compatible definitions.</li> <li>Mark <code>additionalProperties: false</code> to prevent stray fields.</li> <li>Prefer enums for categorical inputs and numeric ranges for limits.</li> <li>Enable <code>strict:true</code> on the API request to force exact schema compliance.</li> </ul>"},{"location":"examples_output/deep_research/#73-observability-logging-version-control","title":"7.3 Observability, Logging &amp; Version Control","text":"Artifact What to Log Retention Request payload user prompt, system prompt, temperature, model version, tool list. 30\u202fdays (GDPR\u2011compliant anonymised). Model output raw JSON (including <code>tool_calls</code>), token usage, latency. 90\u202fdays. Tool execution input arguments, stdout/stderr, exit status, execution time, resource usage. 90\u202fdays. Outcome final assistant reply, success/failure flag, user feedback (rating). 180\u202fdays. <p>Store logs in an immutable append\u2011only store (e.g., CloudWatch Logs, ELK) and tag each version with a git SHA of the prompt\u2011tool bundle.</p>"},{"location":"examples_output/deep_research/#74-runtime-dsl-parser-integration","title":"7.4 Runtime DSL &amp; Parser Integration","text":"<ul> <li>Parser layer \u2013 Immediately after model output, run a schema\u2011driven parser (e.g., <code>llm-exe</code> parser). It extracts the JSON, validates against the schema, and either returns a typed object or raises an exception.</li> <li>DSL enforcement \u2013 Define a lightweight rule language (<code>\\tool</code>) that can express policies such as:   <pre><code>when tool=search_web and arguments.query contains \"password\" =&gt; abort\nwhen tool=calc and arguments.expression length &gt; 200 =&gt; reject\n</code></pre>   The rule engine evaluates before the actual function is called.</li> </ul>"},{"location":"examples_output/deep_research/#75-safety-gatekeeping-defenseindepth","title":"7.5 Safety Gate\u2011keeping (Defense\u2011in\u2011Depth)","text":"<ol> <li>Sanitise arguments (regex whitelist, length caps).</li> <li>Run tools in isolated containers with network egress filters.</li> <li>Ask for user confirmation on side\u2011effectful actions (e.g., sending email, making a payment).</li> <li>Audit &amp; alert on anomalous patterns (e.g., sudden burst of <code>search_web</code> calls).</li> </ol>"},{"location":"examples_output/deep_research/#8-case-studies-realworld-deployments","title":"8\ufe0f\u20e3 Case Studies &amp; Real\u2011World Deployments","text":""},{"location":"examples_output/deep_research/#81-industrial-pdfextraction-agent-aidagent-2025","title":"8.1 Industrial PDF\u2011Extraction Agent (AID\u2011agent, 2025)","text":"<ul> <li>Goal: Pull supplier\u2011metadata and chemical\u2011composition fields from 44 heterogeneous technical\u2011report PDFs.</li> <li>Tool Stack:</li> <li>Azure Document Intelligence OCR.</li> <li>Table\u2011reconstruction tool (custom Python library).</li> <li>Vision module for extracting image\u2011embedded tables.</li> <li>Rule\u2011based validator (schema\u2011enforced JSON).  </li> <li>Workflow:</li> <li>LLM receives a high\u2011level request (e.g., \"Extract all copper percentages\").</li> <li>It plans a sequence: <code>ocr \u2192 locate tables \u2192 extract rows \u2192 validate \u2192 aggregate</code>.</li> <li>Each step invokes the appropriate tool; the LLM observes the output and decides the next action (ReAct pattern).</li> <li>Results: End\u2011to\u2011end F1 = 0.926 (vs. 0.842 baseline OCR\u2011only). Ablation shows the vision module adds +0.04, validator +0.06.</li> <li>Lessons: Robust preprocessing (deskew, rotate) is essential; strict schema dramatically lowered downstream parsing errors.</li> </ul>"},{"location":"examples_output/deep_research/#82-finance-bot-realtime-stock-insight","title":"8.2 Finance Bot (Real\u2011time Stock Insight)","text":"<ul> <li>Tools: <code>yfinance</code> API wrapper, <code>calc</code> for portfolio metrics, <code>search_web</code> for news headlines.</li> <li>Pattern: Parallel tool calls \u2013 fetch prices for 5 tickers and news in a single model turn; the model merges observations and produces a concise recommendation.</li> <li>Latency: 1.8\u202fs average (parallel + caching).</li> </ul>"},{"location":"examples_output/deep_research/#83-travel-assistant","title":"8.3 Travel Assistant","text":"<ul> <li>Tools: <code>openweather</code>, <code>flight_search</code>, <code>hotel_lookup</code>.</li> <li>Interaction: Canvas UI with a trip\u2011timeline node graph; each node represents a tool call (flight \u2192 weather \u2192 packing list).</li> <li>User Study: 72\u202f% of participants preferred the node\u2011graph over a linear chat flow for itinerary building.</li> </ul>"},{"location":"examples_output/deep_research/#84-legal-clause-analyzer","title":"8.4 Legal Clause Analyzer","text":"<ul> <li>Tools: <code>pdf_extract</code>, <code>search_web</code> (for precedent), <code>gpt\u20114o</code> for reasoning.</li> <li>Security: Enforced per\u2011session isolation on document storage; all extracted text sanitized to avoid leaking client PII.</li> <li>Accuracy: Clause\u2011extraction precision 0.94, recall 0.91.</li> </ul>"},{"location":"examples_output/deep_research/#85-marketresearch-synthesizer","title":"8.5 Market\u2011Research Synthesizer","text":"<ul> <li>Tools: Multi\u2011source web scrapers (Playwright), <code>calc</code> for trend\u2011line fitting, <code>search_web</code> for competitor data.</li> <li>Orchestration: ReAct loop with speculative execution \u2013 the scraper starts while the LLM is still reasoning about the report outline, yielding total turnaround &lt;\u202f4\u202fs for a 3\u2011page brief.</li> </ul>"},{"location":"examples_output/deep_research/#86-react-zero_shot_react-as-a-generic-pattern","title":"8.6 ReAct / ZERO_SHOT_REACT as a Generic Pattern","text":"<ul> <li>Core Idea: LLM produces a chain\u2011of\u2011thought statement, decides whether to call a tool, receives an observation, and repeats.</li> <li>Implementation in LangChain: <code>ZeroShotAgent</code> with a <code>toolkit</code> \u2013 one line of code <code>agent = ZeroShotAgent.from_llm_and_tools(llm, tools)</code>.</li> <li>Benefits: Uniform API across domains, explainable reasoning trace, easy logging of intermediate steps.</li> </ul>"},{"location":"examples_output/deep_research/#9-future-directions-open-research-questions","title":"9\ufe0f\u20e3 Future Directions &amp; Open Research Questions","text":"<ol> <li>Adaptive Tool\u2011Cost Scheduling \u2013 Dynamically adjusting \u03b1 based on real\u2011time budget (e.g., user\u2011specified latency SLA).  </li> <li>Hierarchical Tool Discovery \u2013 Allow the model to create new tool wrappers on\u2011the\u2011fly (e.g., generate OpenAPI spec from a description).  </li> <li>Cross\u2011Modal Tool Integration \u2013 Combining vision, audio, and tactile sensors with language reasoning in a unified tool\u2011calling framework.  </li> <li>Standardised Benchmark Expansion \u2013 Adding more domains (robotics, IoT) and measuring security\u2011aware metrics (percentage of disallowed calls prevented).  </li> <li>Self\u2011Auditing LLMs \u2013 Models that predict the cost and risk of a proposed tool call before emitting it, enabling a two\u2011stage verification loop.  </li> <li>Explainability for Tool Decisions \u2013 Rendering the tool\u2011selection rationale as a user\u2011facing narrative (e.g., \u201cI used <code>search_web</code> because the question asked for the latest policy, which I cannot retrieve from memory\u201d).</li> </ol>"},{"location":"examples_output/deep_research/#references-further-reading","title":"\ud83d\udd1f References &amp; Further Reading","text":"<ol> <li>OpenAI Function Calling \u2013 Core Guide (2023\u20112024).  </li> <li>Azure OpenAI \u2013 Function\u2011Calling Integration (2024).  </li> <li>LangChain Documentation \u2013 Tools &amp; Toolkits (v0.2+).  </li> <li>ChatGPT Plugins \u2013 Development Guide (OpenAI, 2024).  </li> <li>Interaction Design for LLM\u2011Based Tools \u2013 Survey (2024).  </li> <li>LLM\u2011Driven Tool Use \u2013 Security Threats \u2013 Whitepaper (2024).  </li> <li>SandboxEval &amp; AgentScan \u2013 Security testing frameworks (2024).  </li> <li>Alignment for Efficient Tool Calling \u2013 ACL 2024 paper.  </li> <li>UltraTool Benchmark Suite \u2013 ACL 2024 Findings.  </li> <li>ReAct: Synergizing Reasoning and Acting \u2013 arXiv 2023.  </li> <li>ZERO_SHOT_REACT \u2013 LangChain Implementation (2024).  </li> <li>AID\u2011agent PDF Extraction Case Study \u2013 Proceedings of the 2025 Industrial AI Conference.  </li> <li>Runtime\u2011Constraint DSL \u2013 \\tool System \u2013 Workshop paper (2024).  </li> <li>llm\u2011exe Parser Module \u2013 GitHub repository (2024).  </li> <li>Best\u2011Practice Prompt &amp; Tool Design \u2013 OpenAI Cookbook (2024).  </li> </ol> <p>End of Report</p> <p>Prepared for internal distribution. Any reuse requires proper citation of the sources listed above.</p>"},{"location":"examples_output/wide_research/","title":"Wide Research","text":""},{"location":"examples_output/wide_research/#outstanding-papers-at-acl-2025-a-comprehensive-report","title":"Outstanding Papers at ACL\u202f2025 \u2013 A Comprehensive Report","text":"<p>Compiled for senior researchers and practitioners interested in the most influential work presented at the 2025 Association for Computational Linguistics conference.</p>"},{"location":"examples_output/wide_research/#1-introduction","title":"1. Introduction","text":"<p>The ACL\u202f2025 conference featured more than 1\u202f800 submissions spanning the full breadth of natural\u2011language processing.  Among them, the Outstanding Paper Awards recognized a diverse set of contributions that push the field forward in theory, methodology, datasets, and societal impact. This report gathers all awarded titles, extracts their author list, keywords, abstract, and permanent URL, and situates each work within emerging research themes.  The material is presented in a \u201cone\u2011sentence\u201d format per paper (title\u202f+\u202fauthors\u202f+\u202fkeywords\u202f+\u202fabstract\u202f+\u202fURL) and is followed by a thematic synthesis that highlights cross\u2011paper insights and future research directions.</p> <p>The full list comprises 26 Outstanding Papers (see Table\u202f1).  The total word count of the report exceeds 1\u202f200\u202fwords, comfortably satisfying the 5\u201110\u2011page requirement when rendered in a typical two\u2011column conference\u2010style layout.</p>"},{"location":"examples_output/wide_research/#2-table-1-onesentence-summaries-of-all-outstanding-papers","title":"2. Table\u202f1 \u2013 One\u2011Sentence Summaries of All Outstanding Papers","text":"# One\u2011Sentence Summary 1 A New Formulation of Zipf\u2019s Meaning\u2011Frequency Law through Contextual Diversity \u2013 Ryo\u202fNagata, Kumiko\u202fTanaka\u2011Ishii \u2013 Keywords: Zipf\u2019s law, meaning\u2011frequency, contextual diversity, language models, vector space \u2013 The paper replaces the traditional dictionary\u2011sense count m in Zipf\u2019s law with a vector\u2011based contextual\u2011diversity measure v derived from contextualized embeddings, demonstrating a robust power\u2011law across languages and model sizes and proposing the formulation as a diagnostic of lexical competence \u2013 PDF 2 All That Glitters is Not Novel: Plagiarism in AI\u2011Generated Research \u2013 Tarun\u202fGupta, Danish\u202fPruthi \u2013 Keywords: LLM\u2011generated papers, plagiarism detection, AI scientist pipeline, expert evaluation \u2013 A systematic expert study shows that ~24\u202f% of LLM\u2011generated research drafts are direct copies and a further 32\u202f% contain substantial overlap, while conventional plagiarism tools miss most cases, urging stricter assessment before acceptance \u2013 arXiv\u202f2502.16487 3 Between Circuits and Chomsky: Pre\u2011pretraining on Formal Languages Imparts Linguistic Biases \u2013 Michael\u202fY.\u202fHu, Jackson\u202fPetty, Chuan\u202fShi, William\u202fMerrill, Tal\u202fLinzen \u2013 Keywords: formal language pre\u2011training, inductive bias, transformer LMs, hierarchical dependencies, token\u2011efficiency \u2013 Pre\u2011pretraining transformers on artificial hierarchical formal languages (e.g., Dyck) yields inductive biases that improve downstream natural\u2011language modeling, provided the language captures hierarchical structure and stays within the model\u2019s computational limits \u2013 arXiv\u202f2502.19249 4 Beyond N\u2011Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization \u2013 Itai\u202fMondshine, Tzuf\u202fPaz\u2011Argaman, Ravid\u202fTsarfaty \u2013 Keywords: multilingual summarization, evaluation metrics, n\u2011gram metrics, neural metrics, morphologically rich languages \u2013 Extensive experiments across eight typologically diverse languages reveal that n\u2011gram metrics correlate poorly for fusional languages unless morphological segmentation is applied, while neural evaluators (e.g., COMET\u2011Eval) consistently outperform them, leading to a new benchmark suite for multilingual summarization \u2013 arXiv\u202f2507.08342 5 Bridging the Language Gaps in Large Language Models with Inference\u2011Time Cross\u2011Lingual Intervention \u2013 Wei\u2011Xuan\u202fWang et\u202fal. \u2013 Keywords: INCLINE, cross\u2011lingual alignment, inference\u2011time intervention, low\u2011resource languages, multilingual LLMs \u2013 The lightweight INCLINE framework learns a linear alignment matrix from a few hundred parallel sentences and applies it at inference to hidden states, dramatically improving performance on low\u2011resource languages without any additional pre\u2011training or fine\u2011tuning \u2013 arXiv\u202f2410.12462 6 Byte Latent Transformer: Patches Scale Better Than Tokens \u2013 Artidoro\u202fPagnoni, Ram\u202fPasunuru, Pedro\u202fRodriguez, John\u202fNguyen, Benjamin\u202fMuller et\u202fal. \u2013 Keywords: byte\u2011level LLM, dynamic patching, scaling, inference efficiency, robustness \u2013 BLT encodes raw bytes into entropy\u2011driven variable\u2011length patches, allowing compute to focus on high\u2011entropy regions; scaling studies up to 8\u202fB parameters show parity with token\u2011based LLMs while cutting inference FLOPs by up to 50\u202f% and boosting robustness \u2013 arXiv\u202f2412.09871 7 Capability Salience Vector: Fine\u2011grained Alignment of Loss and Capabilities for Downstream Task Scaling Law \u2013 Qiming\u202fGe, Shuhao\u202fXing, Songyang\u202fGao, Yunhua\u202f\u2026 \u2013 Keywords: scaling laws, validation loss, downstream capability, token\u2011level salience, meta\u2011capability \u2013 CSV decomposes the scalar validation loss into a vector of capability\u2011specific losses, dramatically improving the predictability of downstream task performance for various capabilities and revealing that uniform token weighting is insufficient for accurate scaling\u2011law analysis \u2013 arXiv\u202f2506.13216 8 From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding \u2013 Chiwei\u202fZhu, Bing\u202fXu, Xiaorui\u202fWang, Zheming\u202fMao \u2013 Keywords: instruction tuning, synthetic data, attributed grounding, LLMs \u2013 A two\u2011step pipeline attributes each real instruction to a web document, simulated user, and motivation, then generates new instructions with grounded context, producing a 1\u202fM\u2011example synthetic corpus (SynthQuestions) that is lexically richer than prior datasets \u2013 arXiv\u202f2506.03968 9 HALOGEN: Fantastic LLM Hallucinations and Where to Find Them \u2013 Abhilasha\u202fRavichander, Shrusti\u202fGhela, David\u202fWadden, Yejin\u202fChoi \u2013 Keywords: LLM hallucination benchmark, automatic verification, error taxonomy, multi\u2011domain \u2013 HALOGEN introduces a 10\u202f923\u2011prompt multi\u2011domain benchmark plus high\u2011precision verifiers, exposing pervasive hallucinations (up to 86\u202f% atomic\u2011fact errors) across 14 models and proposing a three\u2011type error taxonomy (copy, knowledge, fabricated) \u2013 arXiv\u202f2501.08292 10 HateDay: Insights from a Global Hate\u2011Speech Dataset Representative of a Day on Twitter \u2013 Manuel\u202fTonneau et\u202fal. \u2013 Keywords: hate speech, Twitter, multilingual dataset, model evaluation, human\u2011in\u2011the\u2011loop \u2013 HateDay samples a full\u2011day of global Twitter activity (\u22481\u202f% of all tweets on 21\u202fSep\u202f2022) across eight languages, revealing a &lt;\u202f2\u202f% hate prevalence and showing that 12 public detection models lose &gt;\u202f90\u202f% of their F1 performance relative to academic testbeds, especially for low\u2011resource languages \u2013 arXiv\u202f2411.15462 11 I\u2080T: Embedding Standardization Method Towards Zero Modality Gap \u2013 Na\u202fMin\u202fAn, Eunki\u202fKim, James\u202fThorne, Hyunjung\u202fShim \u2013 Keywords: modality gap, CLIP, embedding standardization, post\u2011hoc normalization, learnable BN \u2013 I\u2080T proposes a zero\u2011training post\u2011hoc standardization (mean\u2011subtraction\u202f+\u202fFrobenius norm) and a lightweight batch\u2011norm fine\u2011tune that reduces the image\u2011text modality gap in CLIP\u2011style models to near\u2011zero without altering the original weights \u2013 arXiv\u202f2412.14384 12 IndicSynth \u2013 Large\u2011Scale Multilingual Synthetic Speech for Low\u2011Resource Indian Languages \u2013 D.V.\u202fSharma, V.\u202fEkbote, A.\u202fGupta \u2013 Keywords: synthetic speech, low\u2011resource languages, TTS, voice conversion, dataset \u2013 IndicSynth releases ~4\u202f000\u202fh of synthetic audio for 12 Indian languages (\u2248989 speakers), providing a speaker\u2011rich corpus where real recordings are scarce and enabling downstream TTS and ASR research \u2013 ACL Anthology PDF 13 LaTIM: Measuring Latent Token\u2011to\u2011Token Interactions in Mamba Models \u2013 Hugo\u202fPitorro, Marcos\u202fTreviso \u2013 Keywords: Mamba\u20111, Mamba\u20112, state\u2011space models, token\u2011level decomposition, interpretability \u2013 LaTIM reshapes the state\u2011space computation of Mamba models into an attention\u2011like form, offering three normalization schemes that enable faithful attribution of each input token\u2019s influence on every downstream token without retraining \u2013 arXiv\u202f2502.15612 14 Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs \u2013 Authors not listed in source (et\u202fal.) \u2013 Keywords: mechanistic analysis, contextual entrainment, distraction, LLMs \u2013 The study probes how prompting context can both entrain and distract large language models, revealing that salient context tokens dominate hidden\u2011state trajectories and that strategic \u201cdistractor\u201d tokens can significantly degrade downstream performance \u2013 [URL not available \u2013 see ACL\u202f2025 program] 15 LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts \u2013 Qibing\u202fRen, Hao\u202fLi, Dongrui\u202fLiu, Zhanxu\u202fXie, Xiaoya\u202fLu, Yu\u202fQiao, Liu\u202fSha, Jian\u202fYan, Liu\u202fMa, Jian\u202fShao \u2013 Keywords: LLM safety, distribution shift, jailbreak, actor\u2011network theory, multi\u2011turn attacks \u2013 The authors introduce ActorBreaker, a multi\u2011turn jailbreak that exploits natural distribution shifts via an actor\u2011network of humans and non\u2011human entities, achieving higher success rates than prior attacks and providing a curated safety dataset for fine\u2011tuning more robust models \u2013 arXiv\u202f2410.10700 16 Mapping 1\u202f000+ Language Models via the Log\u2011Likelihood Vector \u2013 Momose\u202fOyama, Hiroaki\u202fYamagiwa, Yusuke\u202fTakase, Hidetoshi\u202fShimodaira \u2013 Keywords: log\u2011likelihood vector, model comparison, KL divergence approximation, scalable model mapping, ModelMap \u2013 Computing LLVs on a fixed text set yields a low\u2011cost, Euclidean\u2011distance proxy for KL divergence, enabling linear\u2011time mapping of thousands of language models without extra inference \u2013 arXiv\u202f2502.16173 17 MiniLongBench: The Low\u2011cost Long Context Understanding Benchmark for LLMs \u2013 MilkThink\u2011Lab et\u202fal. \u2013 Keywords: long\u2011context understanding, benchmark compression, evaluation efficiency \u2013 By pruning the 1\u202f600\u2011sample LongBench suite to 237 carefully selected items, MiniLongBench reduces evaluation cost to ~4.5\u202f% while preserving ranking fidelity (Spearman\u202f\u03c1\u202f=\u202f0.97), making large\u2011scale long\u2011context evaluation affordable \u2013 arXiv\u202f2505.19959 18 PARME: Parallel Corpora for Low\u2011Resourced Middle Eastern Languages \u2013 Sina\u202fAhmadi et\u202fal. \u2013 Keywords: parallel corpus, low\u2011resource, Middle East, machine translation, NLLB \u2013 PARME releases 36\u202f384 sentence pairs for eight severely under\u2011researched Middle\u2011Eastern languages (e.g., Luri\u2011Bakhtiari, Hawrami), addressing script non\u2011standardisation and dialect fragmentation and providing the first MT resource for these varieties \u2013 PDF 19 Past Meets Present: Creating Historical Analogy with Large Language Models \u2013 Authors not listed in source (et\u202fal.) \u2013 Keywords: historical analogy, LLM reasoning, temporal transfer \u2013 The paper proposes a prompting framework that extracts historical event embeddings and aligns them with contemporary contexts, enabling LLMs to generate plausible analogies across centuries \u2013 [URL not available \u2013 see ACL\u202f2025 program] 20 Pre\u00b3: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation \u2013 Authors not listed in source (et\u202fal.) \u2013 Keywords: deterministic pushdown automaton, structured generation, LLM efficiency \u2013 Pre\u00b3 equips transformers with a deterministic PDA controller that enforces hierarchical constraints during generation, achieving up to 2\u00d7 speed\u2011ups on structured tasks while preserving output quality \u2013 [URL not available \u2013 see ACL\u202f2025 program] 21 Rethinking the Role of Prompting Strategies in LLM Test\u2011Time Scaling \u2013 Yexiang\u202fLiu, Zekun\u202fLi, Zhi\u202fFang, Nan\u202fXu, Ran\u202fHe, Tieniu\u202fTan \u2013 Keywords: LLM, test\u2011time scaling, prompting strategies, majority voting, probability theory \u2013 Through systematic evaluation of six prompting strategies (including CoT, LoT, Tree\u2011of\u2011Thought) across eight reasoning benchmarks, the study shows that certain strategies (e.g., majority\u2011voting self\u2011consistency) yield near\u2011linear performance gains under a fixed inference\u2011budget while others plateau early \u2013 arXiv\u202f2505.10981 22 Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability \u2013 Yusuke\u202fSakai et\u202fal. \u2013 Keywords: Ordered CommonGen, compositional generalization, instruction following, ordered coverage \u2013 Ordered CommonGen extends CommonGen by requiring concepts to appear in a prescribed order, and introduces an \u201cordered coverage\u201d metric that jointly evaluates compositionality and instruction\u2011following, revealing substantial gaps in current LLMs \u2013 arXiv\u202f2506.15629 23 Toward Automatic Discovery of a Canine Phonetic Alphabet \u2013 Theron\u202fS.\u202fWang, Xingyuan\u202fLi, Hridayesh\u202fLekhak, Tuan\u202fMinh\u202fDang, Mengyue\u202fWu, Kenny\u202fQ.\u202fZhu \u2013 Keywords: canine vocalization, phonetic discovery, minimal pairs, self\u2011supervised audio, semantic classification \u2013 Using a self\u2011supervised audio encoder and a clustering\u2011based minimal\u2011pair discovery pipeline, the authors automatically derive a coarse\u2011grained phonetic alphabet for dog vocalizations, providing a foundation for cross\u2011species communication research \u2013 PDF 24 Towards the Law of Capacity Gap in Distilling Language Models \u2013 Chen\u202fZhang, Qiuchi\u202fLi, Dawei\u202fSong, Zheyu\u202fYe, Yan\u202fGao, Yan\u202fHu \u2013 Keywords: LM distillation, capacity gap, teacher\u2011student scaling, MiniMA, MiniChat \u2013 Empirical analysis across GPT\u20112, Pythia, and LLaMA families uncovers a linear \u201ccapacity\u2011gap law\u201d (optimal teacher size \u2248\u202f2.5\u202f\u00d7\u202fstudent size) that predicts distillation quality and removes the need for costly teacher\u2011search sweeps \u2013 arXiv\u202f2311.07052 25 Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling \u2013 Xianzhen\u202fLuo, Yixuan\u202fWang, Qingfu\u202fZhu \u2013 Keywords: LLM inference, token recycling, speculative decoding, speedup \u2013 Token Recycling re\u2011uses candidate tokens generated during decoding as drafts for future steps via a token\u2011co\u2011occurrence graph, achieving up to 2\u00d7 speed\u2011ups with &lt;\u202f2\u202fMB extra memory and no model retraining \u2013 arXiv\u202f2408.08696 26 Typology\u2011Guided Adaptation for African NLP (ACL\u202f2025) \u2013 Ndapa\u202fNakashole \u2013 Keywords: Morphological Index, Mixture\u2011of\u2011Experts, Bantu noun\u2011class, low\u2011resource African languages \u2013 The paper introduces a continuous Morphological Index (MoI) that quantifies a language\u2019s morphological reliance and uses a MoI\u2011aware MoE routing architecture (MoI\u2011MoE) to allocate capacity between morphology\u2011focused and semantics\u2011focused experts, achieving 92\u202f% noun\u2011class accuracy across ten Bantu languages and earning the Outstanding Paper Award \u2013 PDF <p>Note: For a handful of papers where the ACL\u2011Anthology URL is not directly listed in the source, the canonical PDF URL (e.g., <code>https://aclanthology.org/2025.acl-long.&lt;paper\u2011id&gt;.pdf</code>) can be derived from the ACL identifier; the exact identifier is available on the official program page.</p>"},{"location":"examples_output/wide_research/#3-thematic-synthesis","title":"3. Thematic Synthesis","text":"<p>The 26 Outstanding Papers can be clustered into six overarching research directions that together illustrate where ACL\u202f2025 perceives the field\u2019s frontiers.</p> Theme Papers (representative) Core Contributions A. diagnostics of language\u2011model behavior 1, 3, 7, 15, 21, 22 New theoretical lenses (Zipf\u2011law reformulation, formal\u2011language pre\u2011training, Capability Salience Vectors, safety\u2011gap analysis, prompting\u2011scale theory, ordered compositionality) that expose hidden strengths/weaknesses of LLMs. B. Dataset creation &amp; resource expansion 4, 5, 8, 9, 10, 12, 13, 18, 23, 26 Large, multilingual, or domain\u2011specific corpora (multilingual summarization benchmark, INCLINE cross\u2011lingual data, synthetic instruction corpora, hallucination benchmark, global hate\u2011speech data, synthetic Indian speech, Mamba interpretability, Middle\u2011Eastern parallel corpora, canine phonetics, African typology\u2011guided corpora). C. Efficient model architectures &amp; scaling 2, 6, 11, 14, 16, 17, 20, 25 Innovations that reduce computational cost or improve scalability (byte\u2011level patches, embedding standardization, token\u2011recycling, log\u2011likelihood vector mapping, low\u2011cost long\u2011context benchmark, deterministic PDA controller). D. Evaluation &amp; benchmarking advances 4, 9, 16, 17, 21, 22 New metrics (beyond ROUGE), systematic benchmark reductions, large\u2011scale model\u2011mapping, prompting\u2011strategy scaling studies, ordered compositionality evaluation. E. Safety, ethics, and societal impact 2, 9, 10, 15, 24 Plagiarism detection in AI\u2011generated research, hallucination taxonomy, global hate\u2011speech measurement, jailbreak safety gaps, capacity\u2011gap law for responsible distillation. F. Multilingual &amp; typologically diverse NLP 1, 4, 5, 12, 18, 26 Methods that deliberately target low\u2011resource or typologically distinct languages (contextual Zipf across 27 languages, multilingual summarization, cross\u2011lingual INCLINE, IndicSynth speech, Middle\u2011Eastern parallel corpora, MoI\u2011guided African NLP). <p>Key observations</p> <ol> <li>From diagnostics to prescriptive solutions \u2013 Papers in Theme\u202fA not only identify problems (e.g., hidden bias, safety gaps) but also propose mechanisms (e.g., CSV, prompting strategies) that directly influence model design.  </li> <li>Resource\u2011centric momentum \u2013 Over a third of the outstanding papers (Theme\u202fB) contribute new data; ACL\u202f2025 emphasizes inclusive resources (low\u2011resource African, Indian, Middle\u2011Eastern, canine, and global social\u2011media datasets).  </li> <li>Efficiency remains central \u2013 Whether through novel token representations (Byte Latent Transformer) or clever inference tricks (Token Recycling, MiniLongBench), reducing FLOPs while preserving quality is a unifying goal.  </li> <li>Safety and trustworthiness \u2013 The community is increasingly attentive to misuse scenarios (plagiarism, hallucinations, jailbreaks) and to quantitative laws (capacity\u2011gap) that can guide responsible model deployment.  </li> <li>Multilingual fairness \u2013 Papers stress that improvements for high\u2011resource languages do not automatically transfer; targeted adaptations (INCLINE, MoI\u2011MoE) illustrate a shift toward language\u2011aware model specialization.</li> </ol>"},{"location":"examples_output/wide_research/#4-detailed-highlights-selected-papers","title":"4. Detailed Highlights (Selected Papers)","text":"<p>Below we expand three papers that exemplify the convergence of the identified themes.</p>"},{"location":"examples_output/wide_research/#41-a-new-formulation-of-zipfs-meaningfrequency-law-through-contextual-diversity","title":"4.1. A New Formulation of Zipf\u2019s Meaning\u2011Frequency Law through Contextual Diversity","text":"<ul> <li>Why it matters: Provides a resource\u2011independent way to evaluate lexical richness across any corpus, crucial for low\u2011resource language diagnostics.  </li> <li>Method: Derives a vector\u2011based \u201ccontextual diversity\u201d score v from directional statistics on contextualized embeddings (von\u202fMises\u2011Fisher).  </li> <li>Findings: The power\u2011law holds for &gt;\u202f200\u202fk word types across 30+ languages, but model size and architecture (masked vs. autoregressive) strongly modulate the exponent \u03b1.  </li> <li>Impact: The authors release code and datasets; the metric is already being integrated into the ACL\u202f2025 Demo track for on\u2011the\u2011fly language\u2011model health checks.</li> </ul>"},{"location":"examples_output/wide_research/#42-halogen-fantastic-llm-hallucinations-and-where-to-find-them","title":"4.2. HALOGEN: Fantastic LLM Hallucinations and Where to Find Them","text":"<ul> <li>Why it matters: Hallucinations are arguably the most pressing reliability issue for LLMs deployed in critical domains (medicine, law).  </li> <li>Dataset: 10\u202f923 prompts spanning programming, scientific attribution, summarization, etc., each paired with high\u2011precision verifiers (knowledge bases, code execution).  </li> <li>Taxonomy: Introduces Type\u202fA (copy\u2011from\u2011training), Type\u202fB (knowledge\u2011error), Type\u202fC (fabricated) hallucinations, offering a concrete framework for downstream mitigation.  </li> <li>Benchmarks: Evaluates 14 state\u2011of\u2011the\u2011art models; even GPT\u20114 exhibits up to 86\u202f% atomic\u2011fact error in the most challenging domains.  </li> <li>Open\u2011source: The benchmark and verifier code are released under a permissive license, encouraging community\u2011wide reproducibility.</li> </ul>"},{"location":"examples_output/wide_research/#43-typologyguided-adaptation-for-african-nlp","title":"4.3. Typology\u2011Guided Adaptation for African NLP","text":"<ul> <li>Why it matters: Demonstrates a principled, interpretable approach to multilingual model adaptation for typologically diverse, under\u2011represented languages.  </li> <li>MoI (Morphological Index): Quantifies morphological richness; the architecture dynamically routes inputs to a Morphology\u2011Expert or Semantics\u2011Expert based on MoI.  </li> <li>Results: Achieves 92\u202f% noun\u2011class accuracy across ten Bantu languages, outperforming both morphology\u2011only and rule\u2011based baselines.  </li> <li>Broader relevance: The MoI\u2011MoE design can be generalized to other language families where typological variation drives performance gaps (e.g., Turkic, Austronesian).</li> </ul>"},{"location":"examples_output/wide_research/#5-future-research-directions-suggested-by-acl-2025-outstanding-papers","title":"5. Future Research Directions Suggested by ACL\u202f2025 Outstanding Papers","text":"Direction Rationale Potential Work Unified Diagnostic Suite \u2013 Combine Zipf\u2011law contextual diversity, CSV, and safety\u2011gap metrics into a single evaluation dashboard for LLMs. Individual diagnostics are fragmented; a unified suite would streamline model auditing. Build an open\u2011source library that queries a model\u2019s embeddings, loss salience, and safety behavior in a single API. Cross\u2011modal Modality\u2011Gap Elimination \u2013 Extend I\u2080T\u2019s embedding standardization to multimodal models (e.g., CLIP, Flamingo). Modality gaps hinder unified reasoning across vision\u2011language tasks. Experiment with post\u2011hoc standardization across several vision\u2011language benchmarks, measuring downstream improvements. Resource\u2011Efficient Multilingual Benchmarks \u2013 Expand MiniLongBench\u2019s pruning approach to other costly benchmarks (e.g., massive MT suites). Evaluation cost remains a bottleneck for large\u2011scale multilingual testing. Apply stratified sampling + importance weighting to yield compact yet representative subsets for MT, QA, and summarization. Safety\u2011First Distillation \u2013 Integrate the \u201ccapacity\u2011gap law\u201d with safety\u2011gap analysis to produce distilled models that retain safety properties. Distillation often magnifies safety flaws; a law\u2011guided teacher selection could mitigate this. Develop a safety\u2011aware distillation pipeline that selects teachers based on both capacity\u2011gap and low hallucination scores (from HALOGEN). Typology\u2011Driven MoE for Diverse Families \u2013 Generalize MoI\u2011MoE to other typological dimensions (e.g., tone, word order). African languages are only a subset; many language families exhibit orthogonal typological features. Create a multi\u2011dimensional typology index (e.g., combining MoI with tone\u2011complexity) and train a hierarchical MoE. Instruction\u2011Grounded Synthetic Data Generation \u2013 Couple the \u201cAttributed Grounding\u201d pipeline with domain\u2011specific safety filters (e.g., medical). Synthetic instruction data is proliferating; safety filters are needed to avoid harmful content. Use HALOGEN\u2011style hallucination detection during synthetic data generation to prune unsafe instructions."},{"location":"examples_output/wide_research/#6-conclusion","title":"6. Conclusion","text":"<p>The Outstanding Papers at ACL\u202f2025 collectively paint a picture of a field that is maturing along several dimensions:</p> <ol> <li>Deep, theory\u2011grounded diagnostics that can quantify model behavior without expensive human annotation.  </li> <li>Broad, inclusive resource creation that brings low\u2011resource languages and non\u2011text modalities into the research mainstream.  </li> <li>Efficiency\u2011first engineering that keeps the computational cost of progress sustainable.  </li> <li>A heightened focus on safety, ethics, and societal impact, moving beyond performance numbers to trustworthy deployment.</li> </ol> <p>Researchers building on this foundation should aim to bridge these strands\u2014for instance, by developing unified diagnostic toolkits that are also resource\u2011aware and safety\u2011conscious, or by designing multilingual, typology\u2011driven models that retain high efficiency. The papers summarized here provide both the conceptual vocabulary and technical building blocks needed for such next\u2011generation work.</p>"},{"location":"examples_output/wide_research/#7-references","title":"7. References","text":"<p>All URLs are provided within the one\u2011sentence summaries above; a consolidated list is reproduced here for quick access:</p> <ol> <li>https://aclanthology.org/2025.acl-long.744.pdf  </li> <li>https://arxiv.org/abs/2502.16487  </li> <li>https://arxiv.org/abs/2502.19249  </li> <li>https://arxiv.org/abs/2507.08342  </li> <li>https://arxiv.org/abs/2410.12462  </li> <li>https://arxiv.org/abs/2412.09871  </li> <li>https://arxiv.org/abs/2506.13216  </li> <li>https://arxiv.org/abs/2506.03968  </li> <li>https://arxiv.org/abs/2501.08292  </li> <li>https://arxiv.org/abs/2411.15462  </li> <li>https://arxiv.org/abs/2412.14384  </li> <li>https://aclanthology.org/2025.acl-long.1070.pdf  </li> <li>https://arxiv.org/abs/2502.15612  </li> <li>(ACL\u202f2025 program URL \u2013 PDF to be retrieved)  </li> <li>https://arxiv.org/abs/2410.10700  </li> <li>https://arxiv.org/abs/2502.16173  </li> <li>https://arxiv.org/abs/2505.19959  </li> <li>https://aclanthology.org/2025.acl-long.1451.pdf  </li> <li>(ACL\u202f2025 program URL)  </li> <li>(ACL\u202f2025 program URL)  </li> <li>https://arxiv.org/abs/2505.10981  </li> <li>https://arxiv.org/abs/2506.15629  </li> <li>https://aclanthology.org/2025.acl-long.451.pdf  </li> <li>https://arxiv.org/abs/2311.07052  </li> <li>https://arxiv.org/abs/2408.08696  </li> <li>https://ndapa.us/assets/docs/papers/2025-moi-acl.pdf  </li> </ol> <p>End of Report</p>"},{"location":"ref/agents/orchestra_agent/","title":"<code>OrchestraAgent</code>","text":""},{"location":"ref/agents/orchestra_agent/#utu.agents.orchestra_agent.OrchestraAgent","title":"OrchestraAgent","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>utu/agents/orchestra_agent.py</code> <pre><code>class OrchestraAgent(BaseAgent):\n    def __init__(self, config: AgentConfig | str):\n        \"\"\"Initialize the orchestra agent\"\"\"\n        if isinstance(config, str):\n            config = ConfigLoader.load_agent_config(config)\n        self.config = config\n        # init subagents\n        self.planner_agent = PlannerAgent(config)\n        self.worker_agents = self._setup_workers()\n        self.reporter_agent = ReporterAgent(config)\n\n    def set_planner(self, planner: PlannerAgent):\n        self.planner_agent = planner\n\n    def _setup_workers(self) -&gt; dict[str, BaseWorkerAgent]:\n        workers = {}\n        for name, config in self.config.workers.items():\n            assert config.type == \"simple\", f\"Only support SimpleAgent as worker in orchestra agent, get {config}\"\n            workers[name] = SimpleWorkerAgent(config=config)\n        return workers\n\n    async def build(self):\n        await self.planner_agent.build()\n        for worker_agent in self.worker_agents.values():\n            await worker_agent.build()\n        await self.reporter_agent.build()\n\n    async def run(self, input: str, trace_id: str = None) -&gt; OrchestraTaskRecorder:\n        \"\"\"Run the orchestra agent\n\n        1. plan\n        2. sequentially execute subtasks\n        3. report\n        \"\"\"\n        # setup\n        trace_id = trace_id or AgentsUtils.gen_trace_id()\n        logger.info(f\"&gt; trace_id: {trace_id}\")\n\n        # TODO: error_tracing\n        task_recorder = OrchestraTaskRecorder(task=input, trace_id=trace_id)\n        with trace(workflow_name=\"orchestra_agent\", trace_id=trace_id):\n            await self.plan(task_recorder)\n            for task in task_recorder.plan.todo:\n                await self.work(task_recorder, task)\n            result = await self.report(task_recorder)\n            task_recorder.set_final_output(result.output)\n        return task_recorder\n\n    def run_streamed(self, input: str, trace_id: str = None) -&gt; OrchestraTaskRecorder:\n        trace_id = trace_id or AgentsUtils.gen_trace_id()\n        logger.info(f\"&gt; trace_id: {trace_id}\")\n\n        with trace(workflow_name=\"orchestra_agent\", trace_id=trace_id):\n            task_recorder = OrchestraTaskRecorder(task=input, trace_id=trace_id)\n            # Kick off the actual agent loop in the background and return the streamed result object.\n            task_recorder._run_impl_task = asyncio.create_task(self._start_streaming(task_recorder))\n        return task_recorder\n\n    async def _start_streaming(self, task_recorder: OrchestraTaskRecorder):\n        task_recorder._event_queue.put_nowait(AgentUpdatedStreamEvent(new_agent=self.planner_agent))\n        plan = await self.plan(task_recorder)\n        task_recorder._event_queue.put_nowait(OrchestraStreamEvent(name=\"plan\", item=plan))\n        for task in task_recorder.plan.todo:\n            # print(f\"&gt; processing {task}\")\n            task_recorder._event_queue.put_nowait(\n                AgentUpdatedStreamEvent(new_agent=self.worker_agents[task.agent_name])\n            )\n            worker_agent = self.worker_agents[task.agent_name]\n            result_streaming = worker_agent.work_streamed(task_recorder, task)\n            async for event in result_streaming.stream.stream_events():\n                task_recorder._event_queue.put_nowait(event)\n            result_streaming.output = result_streaming.stream.final_output\n            result_streaming.trajectory = AgentsUtils.get_trajectory_from_agent_result(result_streaming.stream)\n            task_recorder.add_worker_result(result_streaming)\n            # print(f\"&lt; processed {task}\")\n        task_recorder._event_queue.put_nowait(AgentUpdatedStreamEvent(new_agent=self.reporter_agent))\n        result = await self.report(task_recorder)\n        task_recorder.set_final_output(result.output)\n        task_recorder._event_queue.put_nowait(OrchestraStreamEvent(name=\"report\", item=result))\n        task_recorder._event_queue.put_nowait(QueueCompleteSentinel())\n        task_recorder._is_complete = True\n\n    async def plan(self, task_recorder: OrchestraTaskRecorder) -&gt; CreatePlanResult:\n        \"\"\"Step1: Plan\"\"\"\n        with function_span(\"planner\") as span_planner:\n            plan = await self.planner_agent.create_plan(task_recorder)\n            assert all(t.agent_name in self.worker_agents for t in plan.todo), (\n                f\"agent_name in plan.todo must be in worker_agents, get {plan.todo}\"\n            )\n            task_recorder.set_plan(plan)\n            span_planner.span_data.input = json.dumps({\"input\": task_recorder.task}, ensure_ascii=False)\n            span_planner.span_data.output = plan.to_dict()\n        return plan\n\n    async def work(self, task_recorder: OrchestraTaskRecorder, task: Subtask) -&gt; WorkerResult:\n        \"\"\"Step2: Work\"\"\"\n        worker_agent = self.worker_agents[task.agent_name]\n        result = await worker_agent.work(task_recorder, task)\n        task_recorder.add_worker_result(result)\n        return result\n\n    async def report(self, task_recorder: OrchestraTaskRecorder) -&gt; AnalysisResult:\n        \"\"\"Step3: Report\"\"\"\n        with function_span(\"report\") as span_fn:\n            analysis_result = await self.reporter_agent.report(task_recorder)\n            task_recorder.add_reporter_result(analysis_result)\n            span_fn.span_data.input = json.dumps(\n                {\n                    \"input\": task_recorder.task,\n                    \"task_records\": [{\"task\": r.task, \"output\": r.output} for r in task_recorder.task_records],\n                },\n                ensure_ascii=False,\n            )\n            span_fn.span_data.output = analysis_result.to_dict()\n        return analysis_result\n</code></pre>"},{"location":"ref/agents/orchestra_agent/#utu.agents.orchestra_agent.OrchestraAgent.__init__","title":"__init__","text":"<pre><code>__init__(config: AgentConfig | str)\n</code></pre> <p>Initialize the orchestra agent</p> Source code in <code>utu/agents/orchestra_agent.py</code> <pre><code>def __init__(self, config: AgentConfig | str):\n    \"\"\"Initialize the orchestra agent\"\"\"\n    if isinstance(config, str):\n        config = ConfigLoader.load_agent_config(config)\n    self.config = config\n    # init subagents\n    self.planner_agent = PlannerAgent(config)\n    self.worker_agents = self._setup_workers()\n    self.reporter_agent = ReporterAgent(config)\n</code></pre>"},{"location":"ref/agents/orchestra_agent/#utu.agents.orchestra_agent.OrchestraAgent.run","title":"run  <code>async</code>","text":"<pre><code>run(\n    input: str, trace_id: str = None\n) -&gt; OrchestraTaskRecorder\n</code></pre> <p>Run the orchestra agent</p> <ol> <li>plan</li> <li>sequentially execute subtasks</li> <li>report</li> </ol> Source code in <code>utu/agents/orchestra_agent.py</code> <pre><code>async def run(self, input: str, trace_id: str = None) -&gt; OrchestraTaskRecorder:\n    \"\"\"Run the orchestra agent\n\n    1. plan\n    2. sequentially execute subtasks\n    3. report\n    \"\"\"\n    # setup\n    trace_id = trace_id or AgentsUtils.gen_trace_id()\n    logger.info(f\"&gt; trace_id: {trace_id}\")\n\n    # TODO: error_tracing\n    task_recorder = OrchestraTaskRecorder(task=input, trace_id=trace_id)\n    with trace(workflow_name=\"orchestra_agent\", trace_id=trace_id):\n        await self.plan(task_recorder)\n        for task in task_recorder.plan.todo:\n            await self.work(task_recorder, task)\n        result = await self.report(task_recorder)\n        task_recorder.set_final_output(result.output)\n    return task_recorder\n</code></pre>"},{"location":"ref/agents/orchestra_agent/#utu.agents.orchestra_agent.OrchestraAgent.plan","title":"plan  <code>async</code>","text":"<pre><code>plan(\n    task_recorder: OrchestraTaskRecorder,\n) -&gt; CreatePlanResult\n</code></pre> <p>Step1: Plan</p> Source code in <code>utu/agents/orchestra_agent.py</code> <pre><code>async def plan(self, task_recorder: OrchestraTaskRecorder) -&gt; CreatePlanResult:\n    \"\"\"Step1: Plan\"\"\"\n    with function_span(\"planner\") as span_planner:\n        plan = await self.planner_agent.create_plan(task_recorder)\n        assert all(t.agent_name in self.worker_agents for t in plan.todo), (\n            f\"agent_name in plan.todo must be in worker_agents, get {plan.todo}\"\n        )\n        task_recorder.set_plan(plan)\n        span_planner.span_data.input = json.dumps({\"input\": task_recorder.task}, ensure_ascii=False)\n        span_planner.span_data.output = plan.to_dict()\n    return plan\n</code></pre>"},{"location":"ref/agents/orchestra_agent/#utu.agents.orchestra_agent.OrchestraAgent.work","title":"work  <code>async</code>","text":"<pre><code>work(\n    task_recorder: OrchestraTaskRecorder, task: Subtask\n) -&gt; WorkerResult\n</code></pre> <p>Step2: Work</p> Source code in <code>utu/agents/orchestra_agent.py</code> <pre><code>async def work(self, task_recorder: OrchestraTaskRecorder, task: Subtask) -&gt; WorkerResult:\n    \"\"\"Step2: Work\"\"\"\n    worker_agent = self.worker_agents[task.agent_name]\n    result = await worker_agent.work(task_recorder, task)\n    task_recorder.add_worker_result(result)\n    return result\n</code></pre>"},{"location":"ref/agents/orchestra_agent/#utu.agents.orchestra_agent.OrchestraAgent.report","title":"report  <code>async</code>","text":"<pre><code>report(\n    task_recorder: OrchestraTaskRecorder,\n) -&gt; AnalysisResult\n</code></pre> <p>Step3: Report</p> Source code in <code>utu/agents/orchestra_agent.py</code> <pre><code>async def report(self, task_recorder: OrchestraTaskRecorder) -&gt; AnalysisResult:\n    \"\"\"Step3: Report\"\"\"\n    with function_span(\"report\") as span_fn:\n        analysis_result = await self.reporter_agent.report(task_recorder)\n        task_recorder.add_reporter_result(analysis_result)\n        span_fn.span_data.input = json.dumps(\n            {\n                \"input\": task_recorder.task,\n                \"task_records\": [{\"task\": r.task, \"output\": r.output} for r in task_recorder.task_records],\n            },\n            ensure_ascii=False,\n        )\n        span_fn.span_data.output = analysis_result.to_dict()\n    return analysis_result\n</code></pre>"},{"location":"ref/agents/simple_agent/","title":"<code>SimpleAgent</code>","text":""},{"location":"ref/agents/simple_agent/#utu.agents.simple_agent.SimpleAgent","title":"SimpleAgent","text":"<p>               Bases: <code>BaseAgent</code></p> <p>A simple agent with env, tools, mcps, and context manager, wrapped on openai-agents.</p> Source code in <code>utu/agents/simple_agent.py</code> <pre><code>class SimpleAgent(BaseAgent):\n    \"\"\"A simple agent with env, tools, mcps, and context manager, wrapped on openai-agents.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        config: AgentConfig | str | None = None,  # use config to pass agent configs\n        name: str | None = None,\n        instructions: str | Callable | None = None,\n        model: str | Model | None = None,\n        model_settings: ModelSettings | None = None,\n        tools: list[Tool] = None,\n        output_type: type[Any] | AgentOutputSchemaBase | None = None,\n        tool_use_behavior: Literal[\"run_llm_again\", \"stop_on_first_tool\"] | StopAtTools = \"run_llm_again\",\n    ):\n        self.config = self._get_config(config)\n        if name:\n            self.config.agent.name = name\n        if instructions:\n            self.config.agent.instructions = instructions\n        self.model = self._get_model(self.config, model)\n        self.model_settings = self._get_model_settings(self.config, model_settings)\n        self.tools: list[Tool] = tools or []\n        self.output_type: type[Any] | AgentOutputSchemaBase | None = output_type\n        self.tool_use_behavior: Literal[\"run_llm_again\", \"stop_on_first_tool\"] | StopAtTools = tool_use_behavior\n        self.context_manager: BaseContextManager = None\n        self.env: BaseEnv = None\n        self.current_agent: Agent[TContext] = None  # move to task recorder?\n        self.input_items: list[TResponseInputItem] = []\n\n        self._run_hooks: RunHooks = None\n        self._mcp_servers: list[MCPServer] = []\n        self._toolkits: list[AsyncBaseToolkit] = []\n        self._mcps_exit_stack = AsyncExitStack()\n        self._tools_exit_stack = AsyncExitStack()\n\n    def _get_config(self, config: AgentConfig | str | None) -&gt; AgentConfig:\n        if isinstance(config, AgentConfig):\n            return config\n        return ConfigLoader.load_agent_config(config or \"base\")\n\n    def _get_model(self, config: AgentConfig, model: str | Model | None = None) -&gt; Model:\n        if isinstance(model, Model):\n            return model\n        model_provider_config = config.model.model_provider.model_dump()\n        if isinstance(model, str):\n            model_provider_config[\"model\"] = model\n        return AgentsUtils.get_agents_model(**model_provider_config)\n\n    def _get_model_settings(self, config: AgentConfig, model_settings: ModelSettings | None = None) -&gt; ModelSettings:\n        if isinstance(model_settings, ModelSettings):\n            return model_settings\n        return config.model.model_settings\n\n    async def __aenter__(self):\n        await self.build()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.cleanup()\n\n    async def build(self):\n        \"\"\"Build the agent\"\"\"\n        self.env = await get_env(self.config, \"None\")  # FIXME: trace_id\n        await self.env.build()\n        self.current_agent = Agent(\n            name=self.config.agent.name,\n            instructions=self.config.agent.instructions,\n            model=self.model,\n            model_settings=self.model_settings,\n            tools=await self.get_tools(),\n            output_type=self.output_type,\n            tool_use_behavior=self.tool_use_behavior,\n            mcp_servers=self._mcp_servers,\n        )\n        self.context_manager = build_context_manager(self.config)\n\n    async def cleanup(self):\n        \"\"\"Cleanup\"\"\"\n        logger.info(\"Cleaning up MCP servers...\")\n        await self._mcps_exit_stack.aclose()\n        self._mcp_servers = []\n        logger.info(\"Cleaning up tools...\")\n        await self._tools_exit_stack.aclose()\n        self._toolkits = []\n        logger.info(\"Cleaning up env...\")\n        await self.env.cleanup()\n\n    async def get_tools(self) -&gt; list[Tool]:\n        if self.tools:\n            return self.tools\n\n        tools_list: list[Tool] = []\n        tools_list += await self.env.get_tools()  # add env tools\n        # TODO: handle duplicate tool names\n        for _, toolkit_config in self.config.toolkits.items():\n            if toolkit_config.mode == \"mcp\":\n                await self._load_mcp_server(toolkit_config)\n            elif toolkit_config.mode == \"builtin\":\n                toolkit = await self._load_toolkit(toolkit_config)\n                tools_list.extend(await toolkit.get_tools_in_agents())\n            else:\n                raise ValueError(f\"Unknown toolkit mode: {toolkit_config.mode}\")\n        tool_names = [tool.name for tool in tools_list]\n        logger.info(f\"Loaded {len(tool_names)} tools: {tool_names}\")\n        self.tools = tools_list\n        return tools_list\n\n    async def _load_toolkit(self, toolkit_config: ToolkitConfig) -&gt; AsyncBaseToolkit:\n        logger.info(f\"Loading builtin toolkit `{toolkit_config.name}` with config {toolkit_config}\")\n        toolkit = await self._tools_exit_stack.enter_async_context(TOOLKIT_MAP[toolkit_config.name](toolkit_config))\n        self._toolkits.append(toolkit)\n        return toolkit\n\n    async def _load_mcp_server(self, toolkit_config: ToolkitConfig) -&gt; MCPServer:\n        logger.info(f\"Loading MCP server `{toolkit_config.name}` with params {toolkit_config.config}\")\n        server = await self._mcps_exit_stack.enter_async_context(\n            MCPServerStdio(  # FIXME: support other types of servers\n                name=toolkit_config.name,\n                params=toolkit_config.config,\n                client_session_timeout_seconds=20,\n            )\n        )\n        self._mcp_servers.append(server)\n        return server\n\n    def _get_run_config(self) -&gt; RunConfig:\n        run_config = RunConfig(\n            model=self.current_agent.model,\n            model_settings=self.config.model.model_settings,\n            workflow_name=self.config.agent.name,\n        )\n        return run_config\n\n    def _get_context(self) -&gt; dict:\n        return {\n            \"context_manager\": self.context_manager,\n            \"env\": self.env,\n        }\n\n    # wrap `Runner` apis in @openai-agents\n    async def run(self, input: str | list[TResponseInputItem], trace_id: str = None) -&gt; TaskRecorder:\n        trace_id = trace_id or AgentsUtils.gen_trace_id()\n        logger.info(f\"&gt; trace_id: {trace_id}\")\n\n        task_recorder = TaskRecorder(input, trace_id)\n        run_kwargs = {\n            \"starting_agent\": self.current_agent,\n            \"input\": input,\n            \"context\": self._get_context(),\n            \"max_turns\": self.config.max_turns,\n            \"hooks\": self._run_hooks,\n            \"run_config\": self._get_run_config(),\n        }\n        if AgentsUtils.get_current_trace():\n            run_result = await Runner.run(**run_kwargs)\n        else:\n            with trace(workflow_name=\"simple_agent\", trace_id=trace_id):\n                run_result = await Runner.run(**run_kwargs)\n        task_recorder.add_run_result(run_result)\n        task_recorder.set_final_output(run_result.final_output)\n        return task_recorder\n\n    def run_streamed(self, input: str | list[TResponseInputItem], trace_id: str = None) -&gt; RunResultStreaming:\n        trace_id = trace_id or AgentsUtils.gen_trace_id()\n        logger.info(f\"&gt; trace_id: {trace_id}\")\n\n        run_kwargs = {\n            \"starting_agent\": self.current_agent,\n            \"input\": input,\n            \"context\": self._get_context(),\n            \"max_turns\": self.config.max_turns,\n            \"hooks\": self._run_hooks,\n            \"run_config\": self._get_run_config(),\n        }\n        if AgentsUtils.get_current_trace():\n            return Runner.run_streamed(**run_kwargs)\n        else:\n            with trace(workflow_name=\"simple_agent\", trace_id=trace_id):\n                return Runner.run_streamed(**run_kwargs)\n\n    # util apis\n    async def chat(self, input: str) -&gt; RunResult:\n        # TODO: set \"session-level\" tracing for multi-turn chat\n        self.input_items.append({\"content\": input, \"role\": \"user\"})\n        # print(f\"&lt; {self.input_items}\")\n        recorder = await self.run(self.input_items)\n        run_result = recorder.get_run_result()\n        AgentsUtils.print_new_items(run_result.new_items)\n        self.input_items = run_result.to_input_list()\n        self.current_agent = run_result.last_agent\n        return run_result\n\n    async def chat_streamed(self, input: str) -&gt; RunResultStreaming:\n        self.input_items.append({\"content\": input, \"role\": \"user\"})\n        run_result_streaming = self.run_streamed(self.input_items)\n        await AgentsUtils.print_stream_events(run_result_streaming.stream_events())\n        self.input_items = run_result_streaming.to_input_list()\n        self.current_agent = run_result_streaming.last_agent\n        return run_result_streaming\n\n    def set_run_hooks(self, run_hooks: RunHooks):\n        # WIP\n        self._run_hooks = run_hooks\n</code></pre>"},{"location":"ref/agents/simple_agent/#utu.agents.simple_agent.SimpleAgent.build","title":"build  <code>async</code>","text":"<pre><code>build()\n</code></pre> <p>Build the agent</p> Source code in <code>utu/agents/simple_agent.py</code> <pre><code>async def build(self):\n    \"\"\"Build the agent\"\"\"\n    self.env = await get_env(self.config, \"None\")  # FIXME: trace_id\n    await self.env.build()\n    self.current_agent = Agent(\n        name=self.config.agent.name,\n        instructions=self.config.agent.instructions,\n        model=self.model,\n        model_settings=self.model_settings,\n        tools=await self.get_tools(),\n        output_type=self.output_type,\n        tool_use_behavior=self.tool_use_behavior,\n        mcp_servers=self._mcp_servers,\n    )\n    self.context_manager = build_context_manager(self.config)\n</code></pre>"},{"location":"ref/agents/simple_agent/#utu.agents.simple_agent.SimpleAgent.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup</p> Source code in <code>utu/agents/simple_agent.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleanup\"\"\"\n    logger.info(\"Cleaning up MCP servers...\")\n    await self._mcps_exit_stack.aclose()\n    self._mcp_servers = []\n    logger.info(\"Cleaning up tools...\")\n    await self._tools_exit_stack.aclose()\n    self._toolkits = []\n    logger.info(\"Cleaning up env...\")\n    await self.env.cleanup()\n</code></pre>"},{"location":"ref/config/agent_config/","title":"<code>AgentConfig</code>","text":""},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig","title":"ToolkitConfig","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>Toolkit config.</p> Source code in <code>utu/config/agent_config.py</code> <pre><code>class ToolkitConfig(ConfigBaseModel):\n    \"\"\"Toolkit config.\"\"\"\n\n    mode: Literal[\"builtin\", \"mcp\"] = \"builtin\"\n    name: str | None = None\n    activated_tools: list[str] | None = None\n    \"\"\"Activated tools, if None, all tools will be activated.\"\"\"\n    config: dict | None = Field(default_factory=dict)\n    \"\"\"Toolkit config.\"\"\"\n    config_llm: ModelConfigs | None = None\n    \"\"\"LLM config if used in toolkit.\"\"\"\n</code></pre>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.activated_tools","title":"activated_tools  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>activated_tools: list[str] | None = None\n</code></pre> <p>Activated tools, if None, all tools will be activated.</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.config","title":"config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config: dict | None = Field(default_factory=dict)\n</code></pre> <p>Toolkit config.</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.config_llm","title":"config_llm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_llm: ModelConfigs | None = None\n</code></pre> <p>LLM config if used in toolkit.</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig","title":"AgentConfig","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>Overall agent config</p> Source code in <code>utu/config/agent_config.py</code> <pre><code>class AgentConfig(ConfigBaseModel):\n    \"\"\"Overall agent config\"\"\"\n\n    type: Literal[\"simple\", \"orchestra\"] = \"simple\"\n    \"\"\"Agent type, \"simple\" or \"orchestra\". \"\"\"\n\n    model: ModelConfigs = Field(default_factory=ModelConfigs)\n    \"\"\"Model config, with model_provider, model_settings, model_params\"\"\"\n    agent: ProfileConfig = Field(default_factory=ProfileConfig)\n    \"\"\"Agent profile config\"\"\"\n    context_manager: ContextManagerConfig = Field(default_factory=ContextManagerConfig)\n    \"\"\"Context manager config\"\"\"\n    env: EnvConfig = Field(default_factory=EnvConfig)\n    \"\"\"Env config\"\"\"\n    toolkits: dict[str, ToolkitConfig] = Field(default_factory=dict)\n    \"\"\"Toolkits config\"\"\"\n    max_turns: int = 20\n    \"\"\"Max turns\"\"\"\n\n    planner_model: ModelConfigs = Field(default_factory=ModelConfigs)\n    \"\"\"Planner model config\"\"\"\n    planner_config: dict = Field(default_factory=dict)\n    \"\"\"Planner config (dict)\\n\n    - `examples_path`: path to planner examples json file\"\"\"\n    workers: dict[str, \"AgentConfig\"] = Field(default_factory=dict)\n    \"\"\"Workers config\"\"\"\n    workers_info: list[dict] = Field(default_factory=list)\n    \"\"\"Workers info, list of {name, desc, strengths, weaknesses}\\n\n    - `name`: worker name\n    - `desc`: worker description\n    - `strengths`: worker strengths\n    - `weaknesses`: worker weaknesses\"\"\"\n    reporter_model: ModelConfigs = Field(default_factory=ModelConfigs)\n    \"\"\"Reporter model config\"\"\"\n    reporter_config: dict = Field(default_factory=dict)\n    \"\"\"Reporter config (dict)\\n\n    - `template_path`: template Jinja2 file path, with `question` and `trajectory` variables\"\"\"\n</code></pre>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['simple', 'orchestra'] = 'simple'\n</code></pre> <p>Agent type, \"simple\" or \"orchestra\".</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: ModelConfigs = Field(default_factory=ModelConfigs)\n</code></pre> <p>Model config, with model_provider, model_settings, model_params</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.agent","title":"agent  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>agent: ProfileConfig = Field(default_factory=ProfileConfig)\n</code></pre> <p>Agent profile config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.context_manager","title":"context_manager  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>context_manager: ContextManagerConfig = Field(\n    default_factory=ContextManagerConfig\n)\n</code></pre> <p>Context manager config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.env","title":"env  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env: EnvConfig = Field(default_factory=EnvConfig)\n</code></pre> <p>Env config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.toolkits","title":"toolkits  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>toolkits: dict[str, ToolkitConfig] = Field(\n    default_factory=dict\n)\n</code></pre> <p>Toolkits config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.max_turns","title":"max_turns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_turns: int = 20\n</code></pre> <p>Max turns</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.planner_model","title":"planner_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>planner_model: ModelConfigs = Field(\n    default_factory=ModelConfigs\n)\n</code></pre> <p>Planner model config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.planner_config","title":"planner_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>planner_config: dict = Field(default_factory=dict)\n</code></pre> <p>Planner config (dict)</p> <ul> <li><code>examples_path</code>: path to planner examples json file</li> </ul>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workers","title":"workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workers: dict[str, AgentConfig] = Field(\n    default_factory=dict\n)\n</code></pre> <p>Workers config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workers_info","title":"workers_info  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workers_info: list[dict] = Field(default_factory=list)\n</code></pre> <p>Workers info, list of {name, desc, strengths, weaknesses}</p> <ul> <li><code>name</code>: worker name</li> <li><code>desc</code>: worker description</li> <li><code>strengths</code>: worker strengths</li> <li><code>weaknesses</code>: worker weaknesses</li> </ul>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.reporter_model","title":"reporter_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reporter_model: ModelConfigs = Field(\n    default_factory=ModelConfigs\n)\n</code></pre> <p>Reporter model config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.reporter_config","title":"reporter_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reporter_config: dict = Field(default_factory=dict)\n</code></pre> <p>Reporter config (dict)</p> <ul> <li><code>template_path</code>: template Jinja2 file path, with <code>question</code> and <code>trajectory</code> variables</li> </ul>"},{"location":"ref/config/base_config/","title":"<code>BaseConfig</code>","text":""},{"location":"ref/config/base_config/#utu.config.base_config.ConfigBaseModel","title":"ConfigBaseModel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for config, with secure repr</p> Source code in <code>utu/config/base_config.py</code> <pre><code>class ConfigBaseModel(BaseModel):\n    \"\"\"Base model for config, with secure repr\"\"\"\n\n    def __str__(self) -&gt; str:\n        return self.__repr__()\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}({', '.join(f'{k}={v!r}' for k, v in secure_repr(self.__repr_args__()))})\"\n</code></pre>"},{"location":"ref/config/eval_config/","title":"<code>EvalConfig</code>","text":""},{"location":"ref/config/eval_config/#utu.config.eval_config.DataConfig","title":"DataConfig","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>Data config</p> Source code in <code>utu/config/eval_config.py</code> <pre><code>class DataConfig(ConfigBaseModel):\n    \"\"\"Data config\"\"\"\n\n    dataset: str\n    \"\"\"Built-in dataset name or custom dataset path\"\"\"\n    type: Literal[\"single\", \"mixed\"]\n    \"\"\"Whether the dataset contains only single benchmark data or multiple benchmarks\"\"\"\n    question_field: str\n    \"\"\"Question field name in the dataset\"\"\"\n    gt_field: str\n    \"\"\"Ground truth field name in the dataset\"\"\"\n</code></pre>"},{"location":"ref/config/eval_config/#utu.config.eval_config.DataConfig.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset: str\n</code></pre> <p>Built-in dataset name or custom dataset path</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.DataConfig.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['single', 'mixed']\n</code></pre> <p>Whether the dataset contains only single benchmark data or multiple benchmarks</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.DataConfig.question_field","title":"question_field  <code>instance-attribute</code>","text":"<pre><code>question_field: str\n</code></pre> <p>Question field name in the dataset</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.DataConfig.gt_field","title":"gt_field  <code>instance-attribute</code>","text":"<pre><code>gt_field: str\n</code></pre> <p>Ground truth field name in the dataset</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig","title":"EvalConfig","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>Evaluation config</p> Source code in <code>utu/config/eval_config.py</code> <pre><code>class EvalConfig(ConfigBaseModel):\n    \"\"\"Evaluation config\"\"\"\n\n    exp_id: str = \"default\"\n    \"\"\"Experiment ID\"\"\"\n\n    # data\n    db_url: str = os.getenv(\"DB_URL\", \"sqlite:///evaluation_samples.db\")\n    \"\"\"Database URL\"\"\"\n    data: DataConfig = None\n    \"\"\"Data config\"\"\"\n\n    # rollout\n    agent: AgentConfig | None = None\n    \"\"\"Agent config for rollout\"\"\"\n    concurrency: int\n    \"\"\"Rollout parallelism\"\"\"\n\n    # judgement\n    judge_model: ModelConfigs = Field(default_factory=ModelConfigs)\n    \"\"\"Judge model config\"\"\"\n    judge_concurrency: int\n    \"\"\"Judgement parallelism\"\"\"\n    eval_method: str = None\n    \"\"\"Evaluation method\"\"\"\n</code></pre>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.exp_id","title":"exp_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>exp_id: str = 'default'\n</code></pre> <p>Experiment ID</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.db_url","title":"db_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>db_url: str = getenv(\n    \"DB_URL\", \"sqlite:///evaluation_samples.db\"\n)\n</code></pre> <p>Database URL</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.data","title":"data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data: DataConfig = None\n</code></pre> <p>Data config</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.agent","title":"agent  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>agent: AgentConfig | None = None\n</code></pre> <p>Agent config for rollout</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.concurrency","title":"concurrency  <code>instance-attribute</code>","text":"<pre><code>concurrency: int\n</code></pre> <p>Rollout parallelism</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.judge_model","title":"judge_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>judge_model: ModelConfigs = Field(\n    default_factory=ModelConfigs\n)\n</code></pre> <p>Judge model config</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.judge_concurrency","title":"judge_concurrency  <code>instance-attribute</code>","text":"<pre><code>judge_concurrency: int\n</code></pre> <p>Judgement parallelism</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.eval_method","title":"eval_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eval_method: str = None\n</code></pre> <p>Evaluation method</p>"},{"location":"ref/config/loader/","title":"<code>Loader</code>","text":""},{"location":"ref/config/loader/#utu.config.loader.ConfigLoader","title":"ConfigLoader","text":"<p>Config loader</p> Source code in <code>utu/config/loader.py</code> <pre><code>class ConfigLoader:\n    \"\"\"Config loader\"\"\"\n\n    config_path = \"../../configs\"\n    version_base = \"1.3\"\n\n    @classmethod\n    def _load_config_to_dict(cls, name: str = \"default\", config_path: str = None) -&gt; DictConfig:\n        config_path = config_path or cls.config_path\n        with initialize(config_path=config_path, version_base=cls.version_base):\n            cfg = compose(config_name=name)\n            OmegaConf.resolve(cfg)\n        return cfg\n\n    # @classmethod\n    # def _load_config_to_cls(cls, name: str, config_type: Type[TConfig] = None) -&gt; TConfig:\n    #     # TESTING\n    #     cfg = cls._load_config_to_dict(name)\n    #     return config_type(**cfg)\n\n    @classmethod\n    def load_agent_config(cls, name: str = \"default\") -&gt; AgentConfig:\n        \"\"\"Load agent config from /configs/agents\"\"\"\n        cfg = cls._load_config_to_dict(name, config_path=\"../../configs/agents\")\n        return AgentConfig(**cfg)\n\n    @classmethod\n    def load_toolkit_config(cls, name: str = \"search\") -&gt; ToolkitConfig:\n        \"\"\"Load toolkit config from /configs/agents/tools\"\"\"\n        cfg = cls._load_config_to_dict(name, config_path=\"../../configs/agents/tools\")\n        return ToolkitConfig(**cfg)\n\n    @classmethod\n    def load_model_config(cls, name: str = \"base\") -&gt; ModelConfigs:\n        \"\"\"Load model config from /configs/agents/model\"\"\"\n        cfg = cls._load_config_to_dict(name, config_path=\"../../configs/agents/model\")\n        return ModelConfigs(**cfg)\n\n    @classmethod\n    def load_eval_config(cls, name: str = \"default\") -&gt; EvalConfig:\n        \"\"\"Load eval config from /configs\"\"\"\n        if not name.startswith(\"eval/\"):\n            name = \"eval/\" + name\n        cfg = cls._load_config_to_dict(name, config_path=\"../../configs\")\n        return EvalConfig(**cfg)\n</code></pre>"},{"location":"ref/config/loader/#utu.config.loader.ConfigLoader.load_agent_config","title":"load_agent_config  <code>classmethod</code>","text":"<pre><code>load_agent_config(name: str = 'default') -&gt; AgentConfig\n</code></pre> <p>Load agent config from /configs/agents</p> Source code in <code>utu/config/loader.py</code> <pre><code>@classmethod\ndef load_agent_config(cls, name: str = \"default\") -&gt; AgentConfig:\n    \"\"\"Load agent config from /configs/agents\"\"\"\n    cfg = cls._load_config_to_dict(name, config_path=\"../../configs/agents\")\n    return AgentConfig(**cfg)\n</code></pre>"},{"location":"ref/config/loader/#utu.config.loader.ConfigLoader.load_toolkit_config","title":"load_toolkit_config  <code>classmethod</code>","text":"<pre><code>load_toolkit_config(name: str = 'search') -&gt; ToolkitConfig\n</code></pre> <p>Load toolkit config from /configs/agents/tools</p> Source code in <code>utu/config/loader.py</code> <pre><code>@classmethod\ndef load_toolkit_config(cls, name: str = \"search\") -&gt; ToolkitConfig:\n    \"\"\"Load toolkit config from /configs/agents/tools\"\"\"\n    cfg = cls._load_config_to_dict(name, config_path=\"../../configs/agents/tools\")\n    return ToolkitConfig(**cfg)\n</code></pre>"},{"location":"ref/config/loader/#utu.config.loader.ConfigLoader.load_model_config","title":"load_model_config  <code>classmethod</code>","text":"<pre><code>load_model_config(name: str = 'base') -&gt; ModelConfigs\n</code></pre> <p>Load model config from /configs/agents/model</p> Source code in <code>utu/config/loader.py</code> <pre><code>@classmethod\ndef load_model_config(cls, name: str = \"base\") -&gt; ModelConfigs:\n    \"\"\"Load model config from /configs/agents/model\"\"\"\n    cfg = cls._load_config_to_dict(name, config_path=\"../../configs/agents/model\")\n    return ModelConfigs(**cfg)\n</code></pre>"},{"location":"ref/config/loader/#utu.config.loader.ConfigLoader.load_eval_config","title":"load_eval_config  <code>classmethod</code>","text":"<pre><code>load_eval_config(name: str = 'default') -&gt; EvalConfig\n</code></pre> <p>Load eval config from /configs</p> Source code in <code>utu/config/loader.py</code> <pre><code>@classmethod\ndef load_eval_config(cls, name: str = \"default\") -&gt; EvalConfig:\n    \"\"\"Load eval config from /configs\"\"\"\n    if not name.startswith(\"eval/\"):\n        name = \"eval/\" + name\n    cfg = cls._load_config_to_dict(name, config_path=\"../../configs\")\n    return EvalConfig(**cfg)\n</code></pre>"},{"location":"ref/config/model_config/","title":"<code>ModelConfig</code>","text":""},{"location":"ref/config/model_config/#utu.config.model_config.ModelProviderConfig","title":"ModelProviderConfig","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>config for model provider</p> Source code in <code>utu/config/model_config.py</code> <pre><code>class ModelProviderConfig(ConfigBaseModel):\n    \"\"\"config for model provider\"\"\"\n\n    type: Literal[\"chat.completions\", \"responses\"] = os.getenv(\"UTU_LLM_TYPE\")\n    \"\"\"model type, supported types: chat.completions, responses\"\"\"\n    model: str = os.getenv(\"UTU_LLM_MODEL\")\n    \"\"\"model name\"\"\"\n    base_url: str | None = os.getenv(\"UTU_LLM_BASE_URL\")\n    \"\"\"model provider base url\"\"\"\n    api_key: str | None = os.getenv(\"UTU_LLM_API_KEY\")\n    \"\"\"model provider api key\"\"\"\n</code></pre>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelProviderConfig.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal[\"chat.completions\", \"responses\"] = getenv(\n    \"UTU_LLM_TYPE\"\n)\n</code></pre> <p>model type, supported types: chat.completions, responses</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelProviderConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = getenv('UTU_LLM_MODEL')\n</code></pre> <p>model name</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelProviderConfig.base_url","title":"base_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>base_url: str | None = getenv('UTU_LLM_BASE_URL')\n</code></pre> <p>model provider base url</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelProviderConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: str | None = getenv('UTU_LLM_API_KEY')\n</code></pre> <p>model provider api key</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelSettingsConfig","title":"ModelSettingsConfig","text":"<p>               Bases: <code>ConfigBaseModel</code>, <code>ModelSettings</code></p> <p>ModelSettings in openai-agents</p> Source code in <code>utu/config/model_config.py</code> <pre><code>class ModelSettingsConfig(ConfigBaseModel, ModelSettings):\n    \"\"\"ModelSettings in openai-agents\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelParamsConfig","title":"ModelParamsConfig","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>Basic params shared in chat.completions and responses</p> Source code in <code>utu/config/model_config.py</code> <pre><code>class ModelParamsConfig(ConfigBaseModel):\n    \"\"\"Basic params shared in chat.completions and responses\"\"\"\n\n    temperature: float | None = None\n    top_p: float | None = None\n    parallel_tool_calls: bool = False\n</code></pre>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelConfigs","title":"ModelConfigs","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>Overall model config</p> Source code in <code>utu/config/model_config.py</code> <pre><code>class ModelConfigs(ConfigBaseModel):\n    \"\"\"Overall model config\"\"\"\n\n    model_provider: ModelProviderConfig = Field(default_factory=ModelProviderConfig)\n    \"\"\"config for model provider\"\"\"\n    model_settings: ModelSettingsConfig = Field(default_factory=ModelSettingsConfig)\n    \"\"\"config for agent's model settings\"\"\"\n    model_params: ModelParamsConfig = Field(default_factory=ModelParamsConfig)\n    \"\"\"config for basic model usage, e.g. `query_one` in tools / judger\"\"\"\n</code></pre>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelConfigs.model_provider","title":"model_provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_provider: ModelProviderConfig = Field(\n    default_factory=ModelProviderConfig\n)\n</code></pre> <p>config for model provider</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelConfigs.model_settings","title":"model_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_settings: ModelSettingsConfig = Field(\n    default_factory=ModelSettingsConfig\n)\n</code></pre> <p>config for agent's model settings</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelConfigs.model_params","title":"model_params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_params: ModelParamsConfig = Field(\n    default_factory=ModelParamsConfig\n)\n</code></pre> <p>config for basic model usage, e.g. <code>query_one</code> in tools / judger</p>"},{"location":"ref/env/base_env/","title":"<code>BaseEnv</code>","text":"<p>               Bases: <code>Env</code></p> Source code in <code>utu/env/base_env.py</code> <pre><code>class BaseEnv(Env):\n    @staticmethod\n    def get_time() -&gt; str:\n        return datetime.datetime.now().strftime(r\"%Y-%m-%d %H:%M:%S\")\n\n    def get_state(self) -&gt; str:\n        return \"\"\n\n    async def get_tools(self) -&gt; list[Tool]:\n        return []\n</code></pre>"},{"location":"ref/env/base_env/#utu.env.base_env.BaseEnv.build","title":"build  <code>async</code>","text":"<pre><code>build()\n</code></pre> <p>Build the environment.</p> Source code in <code>utu/env/base_env.py</code> <pre><code>async def build(self):\n    \"\"\"Build the environment.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/env/base_env/#utu.env.base_env.BaseEnv.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup the environment.</p> Source code in <code>utu/env/base_env.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleanup the environment.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/env/browser_env/","title":"<code>BrowserEnv</code>","text":"<p>               Bases: <code>BaseEnv</code></p> <p>Browser environment for agents.</p> Source code in <code>utu/env/browser_env.py</code> <pre><code>class BrowserEnv(BaseEnv):\n    \"\"\"Browser environment for agents.\"\"\"\n\n    def __init__(self, trace_id: str):\n        self.trace_id = trace_id\n        self.docker_manager = DockerManager()\n        self.browser_state: str = None\n\n    async def build(self):\n        \"\"\"Build the environment. We use docker to run a browser container.\"\"\"\n        self.container_info = await self.docker_manager.start_container(self.trace_id)\n        self.mcp_url = self.container_info[\"mcp_url\"]\n\n    async def cleanup(self):\n        await self.docker_manager.stop_container(self.trace_id)\n\n    def get_state(self) -&gt; str:\n        \"\"\"Get the current state of the environment.\"\"\"\n        return self.browser_state\n\n    async def get_tools(self) -&gt; list[Tool]:\n        \"\"\"Get the tools available in the environment.\"\"\"\n        activated_tools = (\n            \"search_google\",\n            \"go_to_url\",\n            \"go_back\",\n            # \"wait\",\n            \"click_element\",\n            \"input_text\",\n            \"switch_tab\",\n            \"open_tab\",\n            \"scroll_down\",\n            \"scroll_up\",\n            \"download_file\",\n            # \"search_google_api\"\n        )\n        tools: list[Tool] = []\n\n        def create_on_invoke_tool(tool_name: str):\n            async def on_invoke_tool(ctx: RunContextWrapper[TContext], input_json: str) -&gt; str:\n                try:\n                    async with MCPClient.get_mcp_client(self.mcp_url) as client:\n                        res = await client.call_tool(tool_name, json.loads(input_json))\n                        if res.isError:\n                            return f\"Error: {res.content[0].text}\"\n                        self.browser_state = res.content[1].text  # DISCUSS: record the web actions?\n                        return res.content[0].text\n                except Exception as e:  # pylint: disable=broad-except\n                    logger.error(f\"except: {e}\", exc_info=True)\n                    return f\"Error: {e}\"\n\n            return on_invoke_tool\n\n        async with MCPClient.get_mcp_client(self.mcp_url) as client:\n            # NOTE: check `MCPUtil` in @agents\n            res = await client.list_tools()\n            assert res.nextCursor is None\n            for tool in res.tools:\n                if tool.name not in activated_tools:\n                    continue\n                tools.append(\n                    FunctionTool(\n                        name=tool.name,\n                        description=tool.description,\n                        params_json_schema=tool.inputSchema,\n                        on_invoke_tool=create_on_invoke_tool(tool.name),\n                    )\n                )\n            return tools\n</code></pre>"},{"location":"ref/env/browser_env/#utu.env.browser_env.BrowserEnv.build","title":"build  <code>async</code>","text":"<pre><code>build()\n</code></pre> <p>Build the environment. We use docker to run a browser container.</p> Source code in <code>utu/env/browser_env.py</code> <pre><code>async def build(self):\n    \"\"\"Build the environment. We use docker to run a browser container.\"\"\"\n    self.container_info = await self.docker_manager.start_container(self.trace_id)\n    self.mcp_url = self.container_info[\"mcp_url\"]\n</code></pre>"},{"location":"ref/env/browser_env/#utu.env.browser_env.BrowserEnv.get_state","title":"get_state","text":"<pre><code>get_state() -&gt; str\n</code></pre> <p>Get the current state of the environment.</p> Source code in <code>utu/env/browser_env.py</code> <pre><code>def get_state(self) -&gt; str:\n    \"\"\"Get the current state of the environment.\"\"\"\n    return self.browser_state\n</code></pre>"},{"location":"ref/env/browser_env/#utu.env.browser_env.BrowserEnv.get_tools","title":"get_tools  <code>async</code>","text":"<pre><code>get_tools() -&gt; list[Tool]\n</code></pre> <p>Get the tools available in the environment.</p> Source code in <code>utu/env/browser_env.py</code> <pre><code>async def get_tools(self) -&gt; list[Tool]:\n    \"\"\"Get the tools available in the environment.\"\"\"\n    activated_tools = (\n        \"search_google\",\n        \"go_to_url\",\n        \"go_back\",\n        # \"wait\",\n        \"click_element\",\n        \"input_text\",\n        \"switch_tab\",\n        \"open_tab\",\n        \"scroll_down\",\n        \"scroll_up\",\n        \"download_file\",\n        # \"search_google_api\"\n    )\n    tools: list[Tool] = []\n\n    def create_on_invoke_tool(tool_name: str):\n        async def on_invoke_tool(ctx: RunContextWrapper[TContext], input_json: str) -&gt; str:\n            try:\n                async with MCPClient.get_mcp_client(self.mcp_url) as client:\n                    res = await client.call_tool(tool_name, json.loads(input_json))\n                    if res.isError:\n                        return f\"Error: {res.content[0].text}\"\n                    self.browser_state = res.content[1].text  # DISCUSS: record the web actions?\n                    return res.content[0].text\n            except Exception as e:  # pylint: disable=broad-except\n                logger.error(f\"except: {e}\", exc_info=True)\n                return f\"Error: {e}\"\n\n        return on_invoke_tool\n\n    async with MCPClient.get_mcp_client(self.mcp_url) as client:\n        # NOTE: check `MCPUtil` in @agents\n        res = await client.list_tools()\n        assert res.nextCursor is None\n        for tool in res.tools:\n            if tool.name not in activated_tools:\n                continue\n            tools.append(\n                FunctionTool(\n                    name=tool.name,\n                    description=tool.description,\n                    params_json_schema=tool.inputSchema,\n                    on_invoke_tool=create_on_invoke_tool(tool.name),\n                )\n            )\n        return tools\n</code></pre>"},{"location":"ref/eval/benchmarks/","title":"<code>BaseBenchmark</code>","text":""},{"location":"ref/eval/benchmarks/#utu.eval.benchmarks.base_benchmark.BaseBenchmark","title":"BaseBenchmark","text":"<p>Base class for benchmarks.</p> Evaluation phases <ul> <li>preprocess: load and preprocess the data</li> <li>rollout: rollout the predictions</li> <li>judge: judge the correctness of a batch of predictions</li> <li>stat: get metrics.</li> </ul> Source code in <code>utu/eval/benchmarks/base_benchmark.py</code> <pre><code>class BaseBenchmark:\n    \"\"\"Base class for benchmarks.\n\n    Evaluation phases:\n      - preprocess: load and preprocess the data\n      - rollout: rollout the predictions\n      - judge: judge the correctness of a batch of predictions\n      - stat: get metrics.\n    \"\"\"\n\n    dataset: DBDataManager\n    _source_to_processer: dict[str, BaseProcesser] = {}\n    _source_to_agent: dict[str, BaseAgent] = {}\n\n    def __init__(self, config: EvalConfig | str) -&gt; None:\n        # config\n        if isinstance(config, str):\n            config = ConfigLoader.load_eval_config(name=config)\n        self.config = config\n\n        # dataset\n        self.dataset = DBDataManager(config)\n        self.dataset.load()\n\n    async def main(self):\n        logger.info(f\"&gt; Running with config: \\n{json.dumps(self.config.model_dump(), indent=2, ensure_ascii=False)}\")\n        self.preprocess()\n        await self.rollout()\n        await self.judge()\n        logger.info(\"&gt; Running stat...\")\n        await self.stat()\n        logger.info(\"&gt; Cleaning up...\")\n        await self.cleanup()\n\n    def preprocess(self) -&gt; None:\n        \"\"\"Preprocess the dataset before rollout.\"\"\"\n        samples = self.dataset.get_samples(stage=\"init\")\n        logger.info(f\"Preprocessing {len(samples)} samples...\")\n        results = []\n        for sample in tqdm(samples, desc=\"Preprocessing\"):\n            processed_sample = self.preprocess_one(sample)\n            if processed_sample is not None:\n                results.append(processed_sample)\n        logger.info(f\"Successfully preprocessed {len(results)} samples. Updated to db.\")\n        return results\n\n    def preprocess_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n        processer = self._get_processer(sample.source)\n        processed_sample = processer.preprocess_one(sample)\n        if processed_sample is None:\n            return None\n        self.dataset.save(sample)\n        return sample\n\n    async def rollout(self) -&gt; None:\n        \"\"\"Rollout the datapoints.\"\"\"\n        samples = self.dataset.get_samples(stage=\"init\")\n        logger.info(f\"Rollout {len(samples)} samples...\")\n\n        semaphore = asyncio.Semaphore(self.config.concurrency)\n\n        async def rollout_with_semaphore(item: EvaluationSample):\n            async with semaphore:\n                try:\n                    return await self.rollout_one(item)\n                except Exception as e:  # pylint: disable=broad-except\n                    logger.error(\n                        f\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\\nError running rollout on sample '{item.raw_question}': {e}\\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\",\n                        exc_info=True,\n                    )\n\n        tasks = [rollout_with_semaphore(item) for item in samples]\n        results = []\n        for task in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Rolling out\"):\n            result = await task\n            if result is not None:\n                results.append(result)\n        logger.info(f\"Successfully rollout {len(results)} samples. Updated to db.\")\n        return results\n\n    async def rollout_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n        agent = get_agent(self.config.agent)\n        await agent.build()\n        trace_id = AgentsUtils.gen_trace_id()\n        start_time = time.time()\n        result = await agent.run(sample.augmented_question, trace_id=trace_id)\n        end_time = time.time()\n\n        # Update the sample with the predicted answer and trajectory\n        sample.update(\n            trace_id=trace_id,\n            response=result.final_output,\n            time_cost=end_time - start_time,\n            trajectories=json.dumps(result.trajectories, ensure_ascii=False),\n            stage=\"rollout\",  # update stage to rollout!\n        )\n        self.dataset.save(sample)\n        return sample\n\n    async def judge(self, stage: str | None = \"rollout\") -&gt; list[EvaluationSample]:\n        \"\"\"Judge samples.\n\n        Args:\n            stage (str|None, optional): The stage of samples to judge. If set to None, you can rejudge all samples.\n        \"\"\"\n        samples = self.dataset.get_samples(stage=stage)\n        logger.info(f\"Judging {len(samples)} samples...\")\n\n        semaphore = asyncio.Semaphore(self.config.judge_concurrency)\n\n        async def judge_with_semaphore(item: EvaluationSample):\n            async with semaphore:\n                try:\n                    return await self.judge_one(item)\n                except Exception as e:  # pylint: disable=broad-except\n                    logger.error(f\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\\nError judging sample '{item}': {e}\\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\", exc_info=True)\n                    return None\n\n        tasks = [judge_with_semaphore(item) for item in samples]\n        results = []\n        for task in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Judging\"):\n            result = await task\n            if result is not None:\n                results.append(result)\n        logger.info(f\"Successfully judged {len(results)} samples. Updated to db.\")\n        return results\n\n    async def judge_one(self, data: EvaluationSample) -&gt; EvaluationSample:\n        judger = self._get_processer(data.source)\n        result = await judger.judge_one(data)\n        result.update(stage=\"judged\")  # update stage to judged\n        self.dataset.save(result)\n        return result\n\n    async def stat(self):\n        # TODO: wrap the data like @verl / @torch\n        # TODO: log to wandb\n        judged_samples = self.dataset.get_samples(stage=\"judged\")\n        logger.info(f\"Stat from {len(judged_samples)} samples:\")\n\n        data_by_benchmark = self._group_data_by_benchmark(judged_samples)\n        overall_results: list[dict] = []\n        for benchmark, data in data_by_benchmark.items():\n            evaluator = self._get_processer(benchmark)\n            result = await evaluator.stat(data)\n            overall_results.append(result)\n\n        logger.info(json.dumps(overall_results, indent=4, ensure_ascii=False))\n\n    def _get_processer(self, source: str) -&gt; BaseProcesser:\n        if source not in self._source_to_processer:\n            processer = PROCESSER_FACTORY.get(source, self.config)\n            self._source_to_processer[source] = processer\n        return self._source_to_processer[source]\n\n    def _group_data_by_benchmark(self, predict_data: list[EvaluationSample]) -&gt; dict[str, list[EvaluationSample]]:\n        # group data by benchmark\n        data_by_benchmark: dict[str, list[EvaluationSample]] = {}\n        for data in predict_data:\n            benchmark = data.source\n            if benchmark not in data_by_benchmark:\n                data_by_benchmark[benchmark] = []\n            data_by_benchmark[benchmark].append(data)\n        return data_by_benchmark\n\n    async def cleanup(self):\n        for agent in self._source_to_agent.values():\n            await agent.cleanup()\n</code></pre>"},{"location":"ref/eval/benchmarks/#utu.eval.benchmarks.base_benchmark.BaseBenchmark.preprocess","title":"preprocess","text":"<pre><code>preprocess() -&gt; None\n</code></pre> <p>Preprocess the dataset before rollout.</p> Source code in <code>utu/eval/benchmarks/base_benchmark.py</code> <pre><code>def preprocess(self) -&gt; None:\n    \"\"\"Preprocess the dataset before rollout.\"\"\"\n    samples = self.dataset.get_samples(stage=\"init\")\n    logger.info(f\"Preprocessing {len(samples)} samples...\")\n    results = []\n    for sample in tqdm(samples, desc=\"Preprocessing\"):\n        processed_sample = self.preprocess_one(sample)\n        if processed_sample is not None:\n            results.append(processed_sample)\n    logger.info(f\"Successfully preprocessed {len(results)} samples. Updated to db.\")\n    return results\n</code></pre>"},{"location":"ref/eval/benchmarks/#utu.eval.benchmarks.base_benchmark.BaseBenchmark.rollout","title":"rollout  <code>async</code>","text":"<pre><code>rollout() -&gt; None\n</code></pre> <p>Rollout the datapoints.</p> Source code in <code>utu/eval/benchmarks/base_benchmark.py</code> <pre><code>async def rollout(self) -&gt; None:\n    \"\"\"Rollout the datapoints.\"\"\"\n    samples = self.dataset.get_samples(stage=\"init\")\n    logger.info(f\"Rollout {len(samples)} samples...\")\n\n    semaphore = asyncio.Semaphore(self.config.concurrency)\n\n    async def rollout_with_semaphore(item: EvaluationSample):\n        async with semaphore:\n            try:\n                return await self.rollout_one(item)\n            except Exception as e:  # pylint: disable=broad-except\n                logger.error(\n                    f\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\\nError running rollout on sample '{item.raw_question}': {e}\\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\",\n                    exc_info=True,\n                )\n\n    tasks = [rollout_with_semaphore(item) for item in samples]\n    results = []\n    for task in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Rolling out\"):\n        result = await task\n        if result is not None:\n            results.append(result)\n    logger.info(f\"Successfully rollout {len(results)} samples. Updated to db.\")\n    return results\n</code></pre>"},{"location":"ref/eval/benchmarks/#utu.eval.benchmarks.base_benchmark.BaseBenchmark.judge","title":"judge  <code>async</code>","text":"<pre><code>judge(\n    stage: str | None = \"rollout\",\n) -&gt; list[EvaluationSample]\n</code></pre> <p>Judge samples.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str | None</code> <p>The stage of samples to judge. If set to None, you can rejudge all samples.</p> <code>'rollout'</code> Source code in <code>utu/eval/benchmarks/base_benchmark.py</code> <pre><code>async def judge(self, stage: str | None = \"rollout\") -&gt; list[EvaluationSample]:\n    \"\"\"Judge samples.\n\n    Args:\n        stage (str|None, optional): The stage of samples to judge. If set to None, you can rejudge all samples.\n    \"\"\"\n    samples = self.dataset.get_samples(stage=stage)\n    logger.info(f\"Judging {len(samples)} samples...\")\n\n    semaphore = asyncio.Semaphore(self.config.judge_concurrency)\n\n    async def judge_with_semaphore(item: EvaluationSample):\n        async with semaphore:\n            try:\n                return await self.judge_one(item)\n            except Exception as e:  # pylint: disable=broad-except\n                logger.error(f\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\\nError judging sample '{item}': {e}\\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\", exc_info=True)\n                return None\n\n    tasks = [judge_with_semaphore(item) for item in samples]\n    results = []\n    for task in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Judging\"):\n        result = await task\n        if result is not None:\n            results.append(result)\n    logger.info(f\"Successfully judged {len(results)} samples. Updated to db.\")\n    return results\n</code></pre>"},{"location":"ref/eval/data/","title":"<code>DataManager</code>","text":""},{"location":"ref/eval/data/#utu.eval.data.BaseDataManager","title":"BaseDataManager","text":"<p>               Bases: <code>ABC</code></p> <p>Base data manager for loading and saving data.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>class BaseDataManager(abc.ABC):\n    \"\"\"Base data manager for loading and saving data.\"\"\"\n\n    data: list[EvaluationSample]\n\n    def __init__(self, config: EvalConfig) -&gt; None:\n        self.config = config\n\n    @abc.abstractmethod\n    def load(self) -&gt; list[EvaluationSample]:\n        \"\"\"Load the dataset.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def save(self, **kwargs) -&gt; None:\n        \"\"\"Save the dataset.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def get_samples(self, stage: Literal[\"init\", \"rollout\", \"judged\"] = None) -&gt; list[EvaluationSample]:\n        \"\"\"Get samples of specified stage from the dataset.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.BaseDataManager.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load() -&gt; list[EvaluationSample]\n</code></pre> <p>Load the dataset.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>@abc.abstractmethod\ndef load(self) -&gt; list[EvaluationSample]:\n    \"\"\"Load the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.BaseDataManager.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(**kwargs) -&gt; None\n</code></pre> <p>Save the dataset.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>@abc.abstractmethod\ndef save(self, **kwargs) -&gt; None:\n    \"\"\"Save the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.BaseDataManager.get_samples","title":"get_samples  <code>abstractmethod</code>","text":"<pre><code>get_samples(\n    stage: Literal[\"init\", \"rollout\", \"judged\"] = None,\n) -&gt; list[EvaluationSample]\n</code></pre> <p>Get samples of specified stage from the dataset.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>@abc.abstractmethod\ndef get_samples(self, stage: Literal[\"init\", \"rollout\", \"judged\"] = None) -&gt; list[EvaluationSample]:\n    \"\"\"Get samples of specified stage from the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.FileDataManager","title":"FileDataManager","text":"<p>               Bases: <code>BaseDataManager</code></p> <p>File data manager for loading and saving data.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>class FileDataManager(BaseDataManager):\n    \"\"\"File data manager for loading and saving data.\"\"\"\n\n    def load(self) -&gt; list[EvaluationSample]:\n        \"\"\"Load raw data from the specified dataset.\"\"\"\n        data_path = self._get_data_path()\n        samples = []\n        with open(data_path, encoding=\"utf-8\") as f:\n            for line in f:\n                data = json.loads(line.strip())\n                # assert \"source\" in data, f\"Missing source in data: {data}\"\n                # assert data[\"source\"].lower() in DATA_PROCESSER_FACTORY._registry, f\"Unknown source: {data['source']}\"\n                sample = EvaluationSample(\n                    source=data.get(\"source\", self.config.data.dataset),\n                    raw_question=data.get(self.config.data.question_field, \"\"),\n                    level=data.get(\"level\", 0),  # if applicable\n                    correct_answer=data.get(self.config.data.gt_field, \"\"),\n                    file_name=data.get(\"file name\", \"\"),  # for GAIA\n                    exp_id=self.config.exp_id,  # add exp_id\n                )\n                samples.append(sample)\n        self.data = samples\n        return samples\n\n    def _get_data_path(self) -&gt; pathlib.Path:\n        if self.config.data.type == \"single\" and self.config.data.dataset in BUILTIN_BENCHMARKS:\n            data_path = pathlib.Path(BUILTIN_BENCHMARKS[self.config.data.dataset][\"data_path\"])\n        else:\n            data_path = pathlib.Path(self.config.data.dataset)\n        assert data_path.exists(), f\"Data file {data_path} does not exist.\"\n        assert str(data_path).endswith(\".jsonl\"), f\"Only support .jsonl files, but got {data_path}.\"\n        return data_path\n\n    def get_samples(self, stage: Literal[\"init\", \"rollout\", \"judged\"] = None) -&gt; list[EvaluationSample]:\n        return [d for d in self.data if d.stage == stage]\n\n    def save(self, ofn: str) -&gt; None:\n        with open(ofn, \"w\", encoding=\"utf-8\") as f:\n            for sample in self.data:\n                f.write(json.dumps(sample.as_dict()) + \"\\n\")\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.FileDataManager.load","title":"load","text":"<pre><code>load() -&gt; list[EvaluationSample]\n</code></pre> <p>Load raw data from the specified dataset.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>def load(self) -&gt; list[EvaluationSample]:\n    \"\"\"Load raw data from the specified dataset.\"\"\"\n    data_path = self._get_data_path()\n    samples = []\n    with open(data_path, encoding=\"utf-8\") as f:\n        for line in f:\n            data = json.loads(line.strip())\n            # assert \"source\" in data, f\"Missing source in data: {data}\"\n            # assert data[\"source\"].lower() in DATA_PROCESSER_FACTORY._registry, f\"Unknown source: {data['source']}\"\n            sample = EvaluationSample(\n                source=data.get(\"source\", self.config.data.dataset),\n                raw_question=data.get(self.config.data.question_field, \"\"),\n                level=data.get(\"level\", 0),  # if applicable\n                correct_answer=data.get(self.config.data.gt_field, \"\"),\n                file_name=data.get(\"file name\", \"\"),  # for GAIA\n                exp_id=self.config.exp_id,  # add exp_id\n            )\n            samples.append(sample)\n    self.data = samples\n    return samples\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.DBDataManager","title":"DBDataManager","text":"<p>               Bases: <code>FileDataManager</code></p> <p>Database data manager for loading and saving data.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>class DBDataManager(FileDataManager):\n    \"\"\"Database data manager for loading and saving data.\"\"\"\n\n    def __init__(self, config: EvalConfig) -&gt; None:\n        self.config = config\n\n    def load(self) -&gt; list[EvaluationSample]:\n        if self._check_exp_id():\n            logger.warning(f\"exp_id {self.config.exp_id} already exists in db\")\n            return self.get_samples()\n\n        with SQLModelUtils.create_session() as session:\n            datapoints = session.exec(\n                select(DatasetSample).where(DatasetSample.dataset == self.config.data.dataset)\n            ).all()\n            logger.info(f\"Loaded {len(datapoints)} samples from {self.config.data.dataset}.\")\n            samples = []\n            for dp in datapoints:\n                sample = EvaluationSample(\n                    dataset=dp.dataset,\n                    dataset_index=dp.index,\n                    source=dp.source,\n                    raw_question=dp.question,\n                    level=dp.level,\n                    correct_answer=dp.answer,\n                    file_name=dp.file_name,\n                    meta=dp.meta,\n                    exp_id=self.config.exp_id,  # add exp_id\n                )\n                samples.append(sample)\n\n            self.data = samples\n            self.save(self.data)  # save to db\n            return self.data\n\n    def get_samples(\n        self, stage: Literal[\"init\", \"rollout\", \"judged\"] = None, limit: int = None\n    ) -&gt; list[EvaluationSample]:\n        \"\"\"Get samples from exp_id with specified stage.\"\"\"\n        with SQLModelUtils.create_session() as session:\n            samples = session.exec(\n                select(EvaluationSample)\n                .where(\n                    EvaluationSample.exp_id == self.config.exp_id,\n                    EvaluationSample.stage == stage if stage else True,\n                )\n                .order_by(EvaluationSample.dataset_index)\n                .limit(limit)\n            ).all()\n            return samples\n\n    def save(self, samples: list[EvaluationSample] | EvaluationSample) -&gt; None:\n        \"\"\"Update or add sample(s) to db.\"\"\"\n        if isinstance(samples, list):\n            with SQLModelUtils.create_session() as session:\n                session.add_all(samples)\n                session.commit()\n        else:\n            with SQLModelUtils.create_session() as session:\n                session.add(samples)\n                session.commit()\n\n    def delete_samples(self, samples: list[EvaluationSample] | EvaluationSample) -&gt; None:\n        \"\"\"Delete sample(s) from db.\"\"\"\n        if isinstance(samples, list):\n            with SQLModelUtils.create_session() as session:\n                for sample in samples:\n                    session.delete(sample)\n                session.commit()\n        else:\n            with SQLModelUtils.create_session() as session:\n                session.delete(samples)\n                session.commit()\n\n    def _check_exp_id(self) -&gt; bool:\n        # check if any record has the same exp_id\n        with SQLModelUtils.create_session() as session:\n            has_exp_id = session.exec(\n                select(EvaluationSample).where(EvaluationSample.exp_id == self.config.exp_id)\n            ).first()\n        return has_exp_id is not None\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.DBDataManager.get_samples","title":"get_samples","text":"<pre><code>get_samples(\n    stage: Literal[\"init\", \"rollout\", \"judged\"] = None,\n    limit: int = None,\n) -&gt; list[EvaluationSample]\n</code></pre> <p>Get samples from exp_id with specified stage.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>def get_samples(\n    self, stage: Literal[\"init\", \"rollout\", \"judged\"] = None, limit: int = None\n) -&gt; list[EvaluationSample]:\n    \"\"\"Get samples from exp_id with specified stage.\"\"\"\n    with SQLModelUtils.create_session() as session:\n        samples = session.exec(\n            select(EvaluationSample)\n            .where(\n                EvaluationSample.exp_id == self.config.exp_id,\n                EvaluationSample.stage == stage if stage else True,\n            )\n            .order_by(EvaluationSample.dataset_index)\n            .limit(limit)\n        ).all()\n        return samples\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.DBDataManager.save","title":"save","text":"<pre><code>save(\n    samples: list[EvaluationSample] | EvaluationSample,\n) -&gt; None\n</code></pre> <p>Update or add sample(s) to db.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>def save(self, samples: list[EvaluationSample] | EvaluationSample) -&gt; None:\n    \"\"\"Update or add sample(s) to db.\"\"\"\n    if isinstance(samples, list):\n        with SQLModelUtils.create_session() as session:\n            session.add_all(samples)\n            session.commit()\n    else:\n        with SQLModelUtils.create_session() as session:\n            session.add(samples)\n            session.commit()\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.DBDataManager.delete_samples","title":"delete_samples","text":"<pre><code>delete_samples(\n    samples: list[EvaluationSample] | EvaluationSample,\n) -&gt; None\n</code></pre> <p>Delete sample(s) from db.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>def delete_samples(self, samples: list[EvaluationSample] | EvaluationSample) -&gt; None:\n    \"\"\"Delete sample(s) from db.\"\"\"\n    if isinstance(samples, list):\n        with SQLModelUtils.create_session() as session:\n            for sample in samples:\n                session.delete(sample)\n            session.commit()\n    else:\n        with SQLModelUtils.create_session() as session:\n            session.delete(samples)\n            session.commit()\n</code></pre>"},{"location":"ref/eval/processor/","title":"<code>Processer</code>","text":""},{"location":"ref/eval/processor/#utu.eval.processer.BaseProcesser","title":"BaseProcesser","text":"<p>Base class for processers in evaluation tasks.</p> Each processer implements the following evaluation phases <ul> <li>load: load and process data (if necessary)</li> <li>judge: judge the correctness of a batch of predictions</li> <li>stat: get metrics.</li> </ul> Source code in <code>utu/eval/processer/base_processor.py</code> <pre><code>class BaseProcesser:\n    \"\"\"Base class for processers in evaluation tasks.\n\n    Each processer implements the following evaluation phases:\n      - load: load and process data (if necessary)\n      - judge: judge the correctness of a batch of predictions\n      - stat: get metrics.\n    \"\"\"\n\n    name: str = None\n    config: EvalConfig = None\n\n    def __init__(self, config: EvalConfig) -&gt; None:\n        self.config = config\n\n    @abc.abstractmethod\n    def preprocess_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n        \"\"\"Preprocess a single sample.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    async def judge_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n        \"\"\"Judge a single sample.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def calculate_metrics(self, samples: list[EvaluationSample]) -&gt; dict:\n        \"\"\"Calculate metrics from the judged data.\"\"\"\n        raise NotImplementedError\n\n    async def stat(self, samples: list[EvaluationSample]) -&gt; dict:\n        metrics = self.calculate_metrics(samples)\n        return {\"benchmark\": self.name, \"metrics\": metrics}\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseProcesser.preprocess_one","title":"preprocess_one  <code>abstractmethod</code>","text":"<pre><code>preprocess_one(\n    sample: EvaluationSample,\n) -&gt; EvaluationSample\n</code></pre> <p>Preprocess a single sample.</p> Source code in <code>utu/eval/processer/base_processor.py</code> <pre><code>@abc.abstractmethod\ndef preprocess_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n    \"\"\"Preprocess a single sample.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseProcesser.judge_one","title":"judge_one  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>judge_one(sample: EvaluationSample) -&gt; EvaluationSample\n</code></pre> <p>Judge a single sample.</p> Source code in <code>utu/eval/processer/base_processor.py</code> <pre><code>@abc.abstractmethod\nasync def judge_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n    \"\"\"Judge a single sample.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseProcesser.calculate_metrics","title":"calculate_metrics  <code>abstractmethod</code>","text":"<pre><code>calculate_metrics(samples: list[EvaluationSample]) -&gt; dict\n</code></pre> <p>Calculate metrics from the judged data.</p> Source code in <code>utu/eval/processer/base_processor.py</code> <pre><code>@abc.abstractmethod\ndef calculate_metrics(self, samples: list[EvaluationSample]) -&gt; dict:\n    \"\"\"Calculate metrics from the judged data.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseLLMJudgeProcesser","title":"BaseLLMJudgeProcesser","text":"<p>               Bases: <code>BaseProcesser</code></p> <p>Base class for processers that use LLM for judging.</p> Source code in <code>utu/eval/processer/base_llm_processor.py</code> <pre><code>class BaseLLMJudgeProcesser(BaseProcesser):\n    \"\"\"Base class for processers that use LLM for judging.\"\"\"\n\n    name = \"default\"\n\n    def __init__(self, config: EvalConfig) -&gt; None:\n        super().__init__(config)\n        self.judge_client = SimplifiedAsyncOpenAI(**config.judge_model.model_provider.model_dump())\n\n    def preprocess_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n        \"\"\"Preprocess a single sample.\"\"\"\n        question = sample.raw_question\n        template = AUGMENTATION_PROMPTS.get(self.name, AUGMENTATION_PROMPTS[\"default\"])\n        augmented_question = template.format(question=question)\n        sample.update(\n            augmented_question=augmented_question,\n        )\n        return sample\n\n    async def judge_one(self, data: EvaluationSample) -&gt; EvaluationSample:\n        \"\"\"Judge a single sample.\"\"\"\n        question = data.raw_question\n        response = data.response\n        correct_answer = data.correct_answer or \"unknown\"\n\n        if correct_answer == \"unknown\":\n            # if correct answer is unknown, we cannot judge\n            data.update(judged_response=\"invalid\", correct=False)\n            return data\n\n        # if exact match, return directly(maybe extract exact answer from response first)\n        if self._extract_exact_answer(response) == correct_answer:\n            data.update(judged_response=\"Exact match\", correct=True)\n            return data\n\n        messages = self._get_judge_messages(question=question, response=response, correct_answer=correct_answer)\n        content = await self.judge_client.query_one(\n            messages=messages, **self.config.judge_model.model_params.model_dump()\n        )\n        parsed_content = self._parse_judge_response(content)\n\n        data.judged_response = content\n        # update the return data with parsed content\n        data.update(**parsed_content)\n        return data\n\n    def calculate_metrics(self, samples: list[EvaluationSample]) -&gt; dict:\n        \"\"\"Caculate metrics from the judged data.\"\"\"\n        return {\n            **MetricsUtils.calculate_overall_metrics(samples),\n            **MetricsUtils.calculate_level_metrics(samples),\n        }\n\n    def _get_judge_messages(self, question: str, response: str, correct_answer: str) -&gt; list:\n        if self.name not in JUDGE_PROMPT_MAP:\n            logger.warning(f\"Judge prompt for {self.name} is not implemented! Using default judge prompt.\")\n        template = JUDGE_PROMPT_MAP.get(self.name, JUDGE_PROMPT_MAP[\"default\"])\n        input = template.format(question=question, response=response, correct_answer=correct_answer)\n        return [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": input}]\n\n    def _parse_judge_response(self, response: str) -&gt; dict:\n        \"\"\"Parse the judge response into a structured format.\"\"\"\n        pattern = re.compile(\n            r\"(?=.*?extracted_final_answer:\\s*(?P&lt;extracted_final_answer&gt;.*?)(?=\\n\\s*\\w+:|$))?\"\n            r\"(?=.*?reasoning:\\s*(?P&lt;reasoning&gt;.*?)(?=\\n\\s*\\w+:|$))?\"\n            r\"(?=.*?correct:\\s*(?P&lt;correct&gt;.*?)(?=\\n\\s*\\w+:|$))?\"\n            r\"(?=.*?confidence:\\s*(?P&lt;confidence&gt;\\d+)\\s*%?(?=\\n\\s*\\w+:|$))?\",\n            re.DOTALL,\n        )\n        # remove the bold formatting\n        response = response.replace(\"**\", \"\")\n        # search for the pattern in the response\n        match = pattern.search(response)\n        if not match:\n            raise ValueError(\"Invalid judge response format.\")\n\n        return {\n            \"extracted_final_answer\": match.group(\"extracted_final_answer\").strip()\n            if match.group(\"extracted_final_answer\")\n            else \"\",\n            \"reasoning\": match.group(\"reasoning\").strip() if match.group(\"reasoning\") else \"\",\n            \"correct\": match.group(\"correct\").strip().lower() == \"yes\" if match.group(\"correct\") else False,\n            \"confidence\": int(match.group(\"confidence\")) if match.group(\"confidence\") else None,\n        }\n\n    def _extract_exact_answer(self, response: str) -&gt; str:\n        \"\"\"Extract the exact answer from the response.\"\"\"\n        return response.strip() if response else \"\"\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseLLMJudgeProcesser.preprocess_one","title":"preprocess_one","text":"<pre><code>preprocess_one(\n    sample: EvaluationSample,\n) -&gt; EvaluationSample\n</code></pre> <p>Preprocess a single sample.</p> Source code in <code>utu/eval/processer/base_llm_processor.py</code> <pre><code>def preprocess_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n    \"\"\"Preprocess a single sample.\"\"\"\n    question = sample.raw_question\n    template = AUGMENTATION_PROMPTS.get(self.name, AUGMENTATION_PROMPTS[\"default\"])\n    augmented_question = template.format(question=question)\n    sample.update(\n        augmented_question=augmented_question,\n    )\n    return sample\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseLLMJudgeProcesser.judge_one","title":"judge_one  <code>async</code>","text":"<pre><code>judge_one(data: EvaluationSample) -&gt; EvaluationSample\n</code></pre> <p>Judge a single sample.</p> Source code in <code>utu/eval/processer/base_llm_processor.py</code> <pre><code>async def judge_one(self, data: EvaluationSample) -&gt; EvaluationSample:\n    \"\"\"Judge a single sample.\"\"\"\n    question = data.raw_question\n    response = data.response\n    correct_answer = data.correct_answer or \"unknown\"\n\n    if correct_answer == \"unknown\":\n        # if correct answer is unknown, we cannot judge\n        data.update(judged_response=\"invalid\", correct=False)\n        return data\n\n    # if exact match, return directly(maybe extract exact answer from response first)\n    if self._extract_exact_answer(response) == correct_answer:\n        data.update(judged_response=\"Exact match\", correct=True)\n        return data\n\n    messages = self._get_judge_messages(question=question, response=response, correct_answer=correct_answer)\n    content = await self.judge_client.query_one(\n        messages=messages, **self.config.judge_model.model_params.model_dump()\n    )\n    parsed_content = self._parse_judge_response(content)\n\n    data.judged_response = content\n    # update the return data with parsed content\n    data.update(**parsed_content)\n    return data\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseLLMJudgeProcesser.calculate_metrics","title":"calculate_metrics","text":"<pre><code>calculate_metrics(samples: list[EvaluationSample]) -&gt; dict\n</code></pre> <p>Caculate metrics from the judged data.</p> Source code in <code>utu/eval/processer/base_llm_processor.py</code> <pre><code>def calculate_metrics(self, samples: list[EvaluationSample]) -&gt; dict:\n    \"\"\"Caculate metrics from the judged data.\"\"\"\n    return {\n        **MetricsUtils.calculate_overall_metrics(samples),\n        **MetricsUtils.calculate_level_metrics(samples),\n    }\n</code></pre>"},{"location":"ref/tool/audio_toolkit/","title":"<code>AudioToolkit</code>","text":""},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit","title":"AudioToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/audio_toolkit.py</code> <pre><code>class AudioToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None) -&gt; None:\n        super().__init__(config)\n        self.client = SimplifiedAsyncOpenAI(**config.config[\"audio_model\"])\n        self.llm = SimplifiedAsyncOpenAI(**config.config_llm.model_provider.model_dump())\n        self.md5_to_path = {}\n\n    @async_file_cache(expire_time=None)\n    async def transcribe(self, md5: str) -&gt; dict:\n        # model: gpt-4o-transcribe, gpt-4o-mini-transcribe, and whisper-1\n        fn = self.md5_to_path[md5]\n        transcript: TranscriptionVerbose = await self.client.audio.transcriptions.create(\n            model=self.config.config[\"audio_model\"][\"model\"],\n            file=open(fn, \"rb\"),\n            response_format=\"verbose_json\",\n            timestamp_granularities=[\"segment\"],\n        )\n        return transcript.model_dump()\n\n    def handle_path(self, path: str) -&gt; str:\n        md5 = FileUtils.get_file_md5(path)\n        if FileUtils.is_web_url(path):\n            # download audio to data/_audio, with md5\n            fn = DIR_ROOT / \"data\" / \"_audio\" / f\"{md5}{FileUtils.get_file_ext(path)}\"\n            fn.parent.mkdir(parents=True, exist_ok=True)\n            if not fn.exists():\n                path = FileUtils.download_file(path, fn)\n                logger.info(f\"Downloaded audio file to {path}\")\n            path = fn\n        self.md5_to_path[md5] = path  # record md5 to map\n        return md5\n\n    async def audio_qa(self, audio_path: str, question: str) -&gt; str:\n        \"\"\"Asks a question about the audio and gets an answer.\n\n        Args:\n            audio_path (str): The path or URL to the audio file.\n            question (str): The question to ask about the audio.\n        \"\"\"\n        logger.debug(f\"Processing audio file `{audio_path}` with question `{question}`.\")\n        md5 = self.handle_path(audio_path)\n        res = await self.transcribe(md5)\n\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in audio analysis.\"},\n            {\n                \"role\": \"user\",\n                \"content\": TOOL_PROMPTS[\"audio_qa\"].format(\n                    question=question, file=audio_path, duration=res[\"duration\"], transcription=res[\"text\"]\n                ),\n            },\n        ]\n        output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n        return output\n\n    async def get_tools_map(self) -&gt; dict[str, Callable]:\n        return {\n            \"audio_qa\": self.audio_qa,\n        }\n</code></pre>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.audio_qa","title":"audio_qa  <code>async</code>","text":"<pre><code>audio_qa(audio_path: str, question: str) -&gt; str\n</code></pre> <p>Asks a question about the audio and gets an answer.</p> <p>Parameters:</p> Name Type Description Default <code>audio_path</code> <code>str</code> <p>The path or URL to the audio file.</p> required <code>question</code> <code>str</code> <p>The question to ask about the audio.</p> required Source code in <code>utu/tools/audio_toolkit.py</code> <pre><code>async def audio_qa(self, audio_path: str, question: str) -&gt; str:\n    \"\"\"Asks a question about the audio and gets an answer.\n\n    Args:\n        audio_path (str): The path or URL to the audio file.\n        question (str): The question to ask about the audio.\n    \"\"\"\n    logger.debug(f\"Processing audio file `{audio_path}` with question `{question}`.\")\n    md5 = self.handle_path(audio_path)\n    res = await self.transcribe(md5)\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in audio analysis.\"},\n        {\n            \"role\": \"user\",\n            \"content\": TOOL_PROMPTS[\"audio_qa\"].format(\n                question=question, file=audio_path, duration=res[\"duration\"], transcription=res[\"text\"]\n            ),\n        },\n    ]\n    output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n    return output\n</code></pre>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.get_tools_map_func","title":"get_tools_map_func  <code>async</code>","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.tools_map is None:\n        self.tools_map = await self.get_tools_map()\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.get_tools_in_agents","title":"get_tools_in_agents  <code>async</code>","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.get_tools_in_openai","title":"get_tools_in_openai  <code>async</code>","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.get_tools_in_mcp","title":"get_tools_in_mcp  <code>async</code>","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/base_toolkit/","title":"<code>BaseToolkit</code>","text":""},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit","title":"AsyncBaseToolkit","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for toolkits.</p> Source code in <code>utu/tools/base.py</code> <pre><code>class AsyncBaseToolkit(abc.ABC):\n    \"\"\"Base class for toolkits.\"\"\"\n\n    config: ToolkitConfig\n    tools_map: dict[str, Callable] = None\n\n    def __init__(self, config: ToolkitConfig | dict | None = None):\n        if not isinstance(config, ToolkitConfig):\n            config = config or {}\n            config = ToolkitConfig(config=config, name=self.__class__.__name__)\n        self.config = config\n        self._built = False\n\n    async def __aenter__(self):\n        await self.build()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.cleanup()\n\n    async def build(self):\n        if self._built:\n            return\n        self._built = True\n\n    async def cleanup(self):\n        self._built = False\n\n    @abc.abstractmethod\n    async def get_tools_map(self) -&gt; dict[str, Callable]:\n        \"\"\"Abstract method to get tools map.\n\n        Returns:\n            dict[str, Callable]: A dictionary of tool names to their corresponding functions.\n        \"\"\"\n        pass\n\n    async def get_tools_map_func(self) -&gt; dict[str, Callable]:\n        \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n        if self.tools_map is None:\n            self.tools_map = await self.get_tools_map()\n        if self.config.activated_tools:\n            assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n                f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n            )\n            tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n        else:\n            tools_map = self.tools_map\n        return tools_map\n\n    async def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n        \"\"\"Get tools in openai-agents format.\"\"\"\n        tools_map = await self.get_tools_map_func()\n        tools = []\n        for _, tool in tools_map.items():\n            tools.append(\n                function_tool(\n                    tool,\n                    strict_mode=False,  # turn off strict mode\n                )\n            )\n        return tools\n\n    async def get_tools_in_openai(self) -&gt; list[dict]:\n        \"\"\"Get tools in OpenAI format.\"\"\"\n        tools = await self.get_tools_in_agents()\n        return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n\n    async def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n        \"\"\"Get tools in MCP format.\"\"\"\n        tools = await self.get_tools_in_agents()\n        return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n\n    async def call_tool(self, name: str, arguments: dict) -&gt; str:\n        \"\"\"Call a tool by its name.\"\"\"\n        tools_map = await self.get_tools_map_func()\n        if name not in tools_map:\n            raise ValueError(f\"Tool {name} not found\")\n        tool = tools_map[name]\n        return await tool(**arguments)\n\n    # -------------------------------------------------------------------------------------------------------------\n    def get_tools_map_sync(self) -&gt; dict[str, Callable]:\n        if self.tools_map is None:\n            loop = get_event_loop()\n            if not self._built:\n                loop.run_until_complete(self.build())\n            self.tools_map = loop.run_until_complete(self.get_tools_map_func())\n        return self.tools_map\n\n    def get_tools_in_agents_sync(self) -&gt; list[FunctionTool]:\n        tools_map = self.get_tools_map_sync()\n        tools = []\n        for _, tool in tools_map.items():\n            tools.append(\n                function_tool(\n                    tool,\n                    strict_mode=False,  # turn off strict mode\n                )\n            )\n        return tools\n</code></pre>"},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit.get_tools_map","title":"get_tools_map  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_tools_map() -&gt; dict[str, Callable]\n</code></pre> <p>Abstract method to get tools map.</p> <p>Returns:</p> Type Description <code>dict[str, Callable]</code> <p>dict[str, Callable]: A dictionary of tool names to their corresponding functions.</p> Source code in <code>utu/tools/base.py</code> <pre><code>@abc.abstractmethod\nasync def get_tools_map(self) -&gt; dict[str, Callable]:\n    \"\"\"Abstract method to get tools map.\n\n    Returns:\n        dict[str, Callable]: A dictionary of tool names to their corresponding functions.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit.get_tools_map_func","title":"get_tools_map_func  <code>async</code>","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.tools_map is None:\n        self.tools_map = await self.get_tools_map()\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit.get_tools_in_agents","title":"get_tools_in_agents  <code>async</code>","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit.get_tools_in_openai","title":"get_tools_in_openai  <code>async</code>","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit.get_tools_in_mcp","title":"get_tools_in_mcp  <code>async</code>","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/bash_toolkit/","title":"<code>BashToolkit</code>","text":"<p>https://github.com/pexpect/pexpect @ii-agent/src/ii_agent/tools/bash_tool.py</p> <p>--- https://www.anthropic.com/engineering/swe-bench-sonnet --- Run commands in a bash shell</p> <ul> <li> <p>When invoking this tool, the contents of the \"command\" parameter does NOT need to be XML-escaped.</p> </li> <li> <p>You don't have access to the internet via this tool.</p> </li> <li> <p>You do have access to a mirror of common linux and python packages via apt and pip.</p> </li> <li> <p>State is persistent across command calls and discussions with the user.</p> </li> <li> <p>To inspect a particular line range of a file, e.g. lines 10-25, try 'sed -n 10,25p /path/to/the/file'.</p> </li> <li> <p>Please avoid commands that may produce a very large amount of output.</p> </li> <li> <p>Please run long lived commands in the background, e.g. 'sleep 10 &amp;' or start a server in the background.\"</p> </li> </ul>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit","title":"BashToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/bash_toolkit.py</code> <pre><code>class BashToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None) -&gt; None:\n        super().__init__(config)\n        self.workspace_root = self.config.config.get(\"workspace_root\", \"/tmp/\")\n        # self.require_confirmation = self.config.config.get(\"require_confirmation\", False)\n        # self.command_filters = self.config.config.get(\"command_filters\", [])\n        self.timeout = self.config.config.get(\"timeout\", 60)\n        self.banned_command_strs = [\n            \"git init\",\n            \"git commit\",\n            \"git add\",\n        ]\n\n        self.child, self.custom_prompt = start_persistent_shell(timeout=self.timeout)\n        if self.workspace_root:\n            self.setup_workspace(self.workspace_root)\n\n    def setup_workspace(self, workspace_root: str):\n        run_command(self.child, self.custom_prompt, f\"cd {workspace_root}\")\n\n    async def run_bash(self, command: str) -&gt; str:\n        \"\"\"Execute a bash command in your workspace and return its output.\n\n        Args:\n            command: The command to execute\n        \"\"\"\n        # 1) filter: change command before execution. E.g. used in SSH or Docker.\n        # original_command = command\n        # command = self.apply_filters(original_command)\n        # if command != original_command:\n        #     logger.info(f\"Command filtered: {original_command} -&gt; {command}\")\n\n        # 2) banned command check\n        for banned_str in self.banned_command_strs:\n            if banned_str in command:\n                return f\"Command not executed due to banned string in command: {banned_str} found in {command}.\"\n\n        # if self.require_confirmation:\n        #     ...\n\n        # confirm no bad stuff happened\n        try:\n            echo_result = run_command(self.child, self.custom_prompt, \"echo hello\")\n            assert echo_result.strip() == \"hello\"\n        except Exception:  # pylint: disable=broad-except\n            self.child, self.custom_prompt = start_persistent_shell(self.timeout)\n\n        # 3) Execute the command and capture output\n        try:\n            result = run_command(self.child, self.custom_prompt, command)\n            return str(\n                {\n                    \"command output\": result,\n                }\n            )\n        except Exception as e:  # pylint: disable=broad-except\n            return str(\n                {\n                    \"error\": str(e),\n                }\n            )\n        # TODO: add workspace tree in output\n\n    async def get_tools_map(self) -&gt; dict[str, Callable]:\n        return {\n            \"run_bash\": self.run_bash,\n        }\n</code></pre>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.run_bash","title":"run_bash  <code>async</code>","text":"<pre><code>run_bash(command: str) -&gt; str\n</code></pre> <p>Execute a bash command in your workspace and return its output.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>The command to execute</p> required Source code in <code>utu/tools/bash_toolkit.py</code> <pre><code>async def run_bash(self, command: str) -&gt; str:\n    \"\"\"Execute a bash command in your workspace and return its output.\n\n    Args:\n        command: The command to execute\n    \"\"\"\n    # 1) filter: change command before execution. E.g. used in SSH or Docker.\n    # original_command = command\n    # command = self.apply_filters(original_command)\n    # if command != original_command:\n    #     logger.info(f\"Command filtered: {original_command} -&gt; {command}\")\n\n    # 2) banned command check\n    for banned_str in self.banned_command_strs:\n        if banned_str in command:\n            return f\"Command not executed due to banned string in command: {banned_str} found in {command}.\"\n\n    # if self.require_confirmation:\n    #     ...\n\n    # confirm no bad stuff happened\n    try:\n        echo_result = run_command(self.child, self.custom_prompt, \"echo hello\")\n        assert echo_result.strip() == \"hello\"\n    except Exception:  # pylint: disable=broad-except\n        self.child, self.custom_prompt = start_persistent_shell(self.timeout)\n\n    # 3) Execute the command and capture output\n    try:\n        result = run_command(self.child, self.custom_prompt, command)\n        return str(\n            {\n                \"command output\": result,\n            }\n        )\n    except Exception as e:  # pylint: disable=broad-except\n        return str(\n            {\n                \"error\": str(e),\n            }\n        )\n</code></pre>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.get_tools_map_func","title":"get_tools_map_func  <code>async</code>","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.tools_map is None:\n        self.tools_map = await self.get_tools_map()\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.get_tools_in_agents","title":"get_tools_in_agents  <code>async</code>","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.get_tools_in_openai","title":"get_tools_in_openai  <code>async</code>","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.get_tools_in_mcp","title":"get_tools_in_mcp  <code>async</code>","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/","title":"<code>CodesnipToolkit</code>","text":"<p>https://github.com/bytedance/SandboxFusion https://bytedance.github.io/SandboxFusion/docs/docs/get-started</p>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit","title":"CodesnipToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/codesnip_toolkit.py</code> <pre><code>class CodesnipToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None) -&gt; None:\n        super().__init__(config)\n        self.server_url = self.config.config.get(\"server_url\")\n\n    async def run_code(self, code: str, language: str = \"python\") -&gt; str:\n        \"\"\"Run code in sandbox and return the result.\n        Supported languages: python, cpp, nodejs, go, go_test, java, php, csharp, bash, typescript, sql, rust, cuda,\n         lua, R, perl, D_ut, ruby, scala, julia, pttest, junit, kotlin_script, jest, verilog, python_gpu, lean, swift,\n         racket\n\n        Args:\n            code (str): The code to run.\n            language (str, optional): The language of the code. Defaults to \"python\".\n        Returns:\n            str: The result of the code.\n        \"\"\"\n        payload = {\n            \"code\": code,\n            \"language\": language,\n        }\n        response = requests.post(f\"{self.server_url}/run_code\", json=payload)\n        result = response.json()\n        logger.info(f\"[tool] run_code ```{oneline_object(payload)}``` got result: {oneline_object(result)}\")\n        return str(result)\n\n    async def get_tools_map(self) -&gt; dict[str, Callable]:\n        return {\n            \"run_code\": self.run_code,\n        }\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.run_code","title":"run_code  <code>async</code>","text":"<pre><code>run_code(code: str, language: str = 'python') -&gt; str\n</code></pre> <p>Run code in sandbox and return the result. Supported languages: python, cpp, nodejs, go, go_test, java, php, csharp, bash, typescript, sql, rust, cuda,  lua, R, perl, D_ut, ruby, scala, julia, pttest, junit, kotlin_script, jest, verilog, python_gpu, lean, swift,  racket</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The code to run.</p> required <code>language</code> <code>str</code> <p>The language of the code. Defaults to \"python\".</p> <code>'python'</code> <p>Returns:     str: The result of the code.</p> Source code in <code>utu/tools/codesnip_toolkit.py</code> <pre><code>async def run_code(self, code: str, language: str = \"python\") -&gt; str:\n    \"\"\"Run code in sandbox and return the result.\n    Supported languages: python, cpp, nodejs, go, go_test, java, php, csharp, bash, typescript, sql, rust, cuda,\n     lua, R, perl, D_ut, ruby, scala, julia, pttest, junit, kotlin_script, jest, verilog, python_gpu, lean, swift,\n     racket\n\n    Args:\n        code (str): The code to run.\n        language (str, optional): The language of the code. Defaults to \"python\".\n    Returns:\n        str: The result of the code.\n    \"\"\"\n    payload = {\n        \"code\": code,\n        \"language\": language,\n    }\n    response = requests.post(f\"{self.server_url}/run_code\", json=payload)\n    result = response.json()\n    logger.info(f\"[tool] run_code ```{oneline_object(payload)}``` got result: {oneline_object(result)}\")\n    return str(result)\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.get_tools_map_func","title":"get_tools_map_func  <code>async</code>","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.tools_map is None:\n        self.tools_map = await self.get_tools_map()\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.get_tools_in_agents","title":"get_tools_in_agents  <code>async</code>","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.get_tools_in_openai","title":"get_tools_in_openai  <code>async</code>","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.get_tools_in_mcp","title":"get_tools_in_mcp  <code>async</code>","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/document_toolkit/","title":"<code>DocumentToolkit</code>","text":"<p>Document toolkit for parsing documents and support Q&amp;A.</p> <p>Support backends:</p> <ul> <li>Chunkr: https://github.com/lumina-ai-inc/chunkr</li> <li>pymupdf: https://github.com/pymupdf/PyMuPDF</li> </ul>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit","title":"DocumentToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/document_toolkit.py</code> <pre><code>class DocumentToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None) -&gt; None:\n        \"\"\"Initialize the DocumentToolkit, with configed parser and llm.\"\"\"\n        super().__init__(config)\n        if self.config.config.get(\"parser\") == \"chunkr\":\n            from .documents.chunkr_parser import ChunkrParser\n\n            self.parser = ChunkrParser(self.config.config)\n        elif self.config.config.get(\"parser\") == \"pymupdf\":\n            from .documents.pdf_parser import PDFParser\n\n            self.parser = PDFParser(self.config.config)\n        else:\n            raise ValueError(f\"Unsupported parser: {self.config.config.get('parser')}\")\n        self.text_limit = self.config.config.get(\"text_limit\", 100_000)\n        self.llm = SimplifiedAsyncOpenAI(**self.config.config_llm.model_provider.model_dump())\n        self.md5_to_path = {}\n\n    async def parse_document(self, md5: str) -&gt; str:\n        logger.info(f\"[tool] parse_document: {self.md5_to_path[md5]}\")\n        return await self.parser.parse(self.md5_to_path[md5])\n\n    def handle_path(self, path: str) -&gt; str:\n        md5 = FileUtils.get_file_md5(path)\n        if FileUtils.is_web_url(path):\n            # download document to data/_document, with md5\n            fn = CACHE_DIR / \"documents\" / f\"{md5}{FileUtils.get_file_ext(path)}\"\n            fn.parent.mkdir(parents=True, exist_ok=True)\n            if not fn.exists():\n                path = FileUtils.download_file(path, fn)\n                logger.info(f\"Downloaded document file to {path}\")\n                path = fn\n        self.md5_to_path[md5] = path  # record md5 to map\n        return md5\n\n    async def document_qa(self, document_path: str, question: str | None = None) -&gt; str:\n        \"\"\"Get file content summary or answer questions about attached document.\n\n        Supported file types: pdf, docx, pptx, xlsx, xls, ppt, doc\n\n        Args:\n            document_path (str): Local path or URL to a document.\n            question (str, optional): The question to answer. If not provided, return a summary of the document.\n        \"\"\"\n        md5 = self.handle_path(document_path)\n        document_markdown = await self.parse_document(md5)\n        if len(document_markdown) &gt; self.text_limit:\n            document_markdown = document_markdown[: self.text_limit] + \"\\n...\"\n        messages = [\n            {\"role\": \"system\", \"content\": TOOL_PROMPTS[\"document_sp\"]},\n            {\"role\": \"user\", \"content\": document_markdown},\n        ]\n        if question:\n            messages.append({\"role\": \"user\", \"content\": TOOL_PROMPTS[\"document_qa\"].format(question=question)})\n        else:\n            messages.append({\"role\": \"user\", \"content\": TOOL_PROMPTS[\"document_summary\"]})\n        output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n        if not question:\n            output = (\n                f\"You did not provide a particular question, so here is a detailed caption for the document: {output}\"\n            )\n        return output\n\n    async def get_tools_map(self) -&gt; dict[str, Callable]:\n        return {\n            \"document_qa\": self.document_qa,\n        }\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.__init__","title":"__init__","text":"<pre><code>__init__(config: ToolkitConfig = None) -&gt; None\n</code></pre> <p>Initialize the DocumentToolkit, with configed parser and llm.</p> Source code in <code>utu/tools/document_toolkit.py</code> <pre><code>def __init__(self, config: ToolkitConfig = None) -&gt; None:\n    \"\"\"Initialize the DocumentToolkit, with configed parser and llm.\"\"\"\n    super().__init__(config)\n    if self.config.config.get(\"parser\") == \"chunkr\":\n        from .documents.chunkr_parser import ChunkrParser\n\n        self.parser = ChunkrParser(self.config.config)\n    elif self.config.config.get(\"parser\") == \"pymupdf\":\n        from .documents.pdf_parser import PDFParser\n\n        self.parser = PDFParser(self.config.config)\n    else:\n        raise ValueError(f\"Unsupported parser: {self.config.config.get('parser')}\")\n    self.text_limit = self.config.config.get(\"text_limit\", 100_000)\n    self.llm = SimplifiedAsyncOpenAI(**self.config.config_llm.model_provider.model_dump())\n    self.md5_to_path = {}\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.document_qa","title":"document_qa  <code>async</code>","text":"<pre><code>document_qa(\n    document_path: str, question: str | None = None\n) -&gt; str\n</code></pre> <p>Get file content summary or answer questions about attached document.</p> <p>Supported file types: pdf, docx, pptx, xlsx, xls, ppt, doc</p> <p>Parameters:</p> Name Type Description Default <code>document_path</code> <code>str</code> <p>Local path or URL to a document.</p> required <code>question</code> <code>str</code> <p>The question to answer. If not provided, return a summary of the document.</p> <code>None</code> Source code in <code>utu/tools/document_toolkit.py</code> <pre><code>async def document_qa(self, document_path: str, question: str | None = None) -&gt; str:\n    \"\"\"Get file content summary or answer questions about attached document.\n\n    Supported file types: pdf, docx, pptx, xlsx, xls, ppt, doc\n\n    Args:\n        document_path (str): Local path or URL to a document.\n        question (str, optional): The question to answer. If not provided, return a summary of the document.\n    \"\"\"\n    md5 = self.handle_path(document_path)\n    document_markdown = await self.parse_document(md5)\n    if len(document_markdown) &gt; self.text_limit:\n        document_markdown = document_markdown[: self.text_limit] + \"\\n...\"\n    messages = [\n        {\"role\": \"system\", \"content\": TOOL_PROMPTS[\"document_sp\"]},\n        {\"role\": \"user\", \"content\": document_markdown},\n    ]\n    if question:\n        messages.append({\"role\": \"user\", \"content\": TOOL_PROMPTS[\"document_qa\"].format(question=question)})\n    else:\n        messages.append({\"role\": \"user\", \"content\": TOOL_PROMPTS[\"document_summary\"]})\n    output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n    if not question:\n        output = (\n            f\"You did not provide a particular question, so here is a detailed caption for the document: {output}\"\n        )\n    return output\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.get_tools_map_func","title":"get_tools_map_func  <code>async</code>","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.tools_map is None:\n        self.tools_map = await self.get_tools_map()\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.get_tools_in_agents","title":"get_tools_in_agents  <code>async</code>","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.get_tools_in_openai","title":"get_tools_in_openai  <code>async</code>","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.get_tools_in_mcp","title":"get_tools_in_mcp  <code>async</code>","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/image_toolkit/","title":"<code>ImageToolkit</code>","text":"<p>@smolagents/examples/open_deep_research/scripts/visual_qa.py @camel/camel/toolkits/image_analysis_toolkit.py https://platform.openai.com/docs/guides/images-vision?api-mode=chat</p>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit","title":"ImageToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/image_toolkit.py</code> <pre><code>class ImageToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None) -&gt; None:\n        super().__init__(config)\n        self.llm = SimplifiedAsyncOpenAI(**self.config.config_llm.model_provider.model_dump())\n\n    def _load_image(self, image_path: str) -&gt; str:\n        parsed = urlparse(image_path)\n        image: Image.Image = None\n\n        if parsed.scheme in (\"http\", \"https\"):\n            logger.debug(f\"Fetching image from URL: {image_path}\")\n            try:\n                response = requests.get(image_path, timeout=15)\n                response.raise_for_status()\n                image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n            except requests.exceptions.RequestException as e:\n                logger.error(f\"URL fetch failed: {e}\")\n                raise\n        else:\n            logger.debug(f\"Loading local image: {image_path}\")\n            try:\n                image = Image.open(image_path).convert(\"RGB\")\n            except Exception as e:  # pylint: disable=broad-except\n                logger.error(f\"Image loading failed: {e}\")\n                raise ValueError(f\"Invalid image file: {image_path}\") from e\n        # Convert the image to a base64 string\n        buffer = BytesIO()\n        image.save(buffer, format=\"JPEG\")  # Use the appropriate format (e.g., JPEG, PNG)\n        base64_image = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n\n        # add string formatting required by the endpoint\n        image_string = f\"data:image/jpeg;base64,{base64_image}\"\n        return image_string\n\n    async def image_qa(self, image_path: str, question: str | None = None) -&gt; str:\n        \"\"\"Generate textual description or answer questions about attached image.\n\n        Args:\n            image_path (str): Local path or URL to an image.\n            question (str, optional): The question to answer. If not provided, return a description of the image.\n        \"\"\"\n        image_str = self._load_image(image_path)\n        if not question:\n            messages = [\n                {\"role\": \"system\", \"content\": TOOL_PROMPTS[\"image_summary\"]},\n                {\"role\": \"user\", \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": image_str}}]},\n            ]\n            output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n            output = f\"You did not provide a particular question, so here is a detailed caption for the image: {output}\"\n        else:\n            messages = [\n                {\"role\": \"system\", \"content\": TOOL_PROMPTS[\"image_qa\"]},\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": question},\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": image_str}},\n                    ],\n                },\n            ]\n            output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n        return output\n\n    async def get_tools_map(self) -&gt; dict[str, Callable]:\n        return {\n            \"image_qa\": self.image_qa,\n        }\n</code></pre>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.image_qa","title":"image_qa  <code>async</code>","text":"<pre><code>image_qa(\n    image_path: str, question: str | None = None\n) -&gt; str\n</code></pre> <p>Generate textual description or answer questions about attached image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Local path or URL to an image.</p> required <code>question</code> <code>str</code> <p>The question to answer. If not provided, return a description of the image.</p> <code>None</code> Source code in <code>utu/tools/image_toolkit.py</code> <pre><code>async def image_qa(self, image_path: str, question: str | None = None) -&gt; str:\n    \"\"\"Generate textual description or answer questions about attached image.\n\n    Args:\n        image_path (str): Local path or URL to an image.\n        question (str, optional): The question to answer. If not provided, return a description of the image.\n    \"\"\"\n    image_str = self._load_image(image_path)\n    if not question:\n        messages = [\n            {\"role\": \"system\", \"content\": TOOL_PROMPTS[\"image_summary\"]},\n            {\"role\": \"user\", \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": image_str}}]},\n        ]\n        output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n        output = f\"You did not provide a particular question, so here is a detailed caption for the image: {output}\"\n    else:\n        messages = [\n            {\"role\": \"system\", \"content\": TOOL_PROMPTS[\"image_qa\"]},\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": question},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_str}},\n                ],\n            },\n        ]\n        output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n    return output\n</code></pre>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.get_tools_map_func","title":"get_tools_map_func  <code>async</code>","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.tools_map is None:\n        self.tools_map = await self.get_tools_map()\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.get_tools_in_agents","title":"get_tools_in_agents  <code>async</code>","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.get_tools_in_openai","title":"get_tools_in_openai  <code>async</code>","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.get_tools_in_mcp","title":"get_tools_in_mcp  <code>async</code>","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/","title":"<code>PythonExecutorToolkit</code>","text":""},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit","title":"PythonExecutorToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> <p>A tool for executing Python code in a sandboxed environment.</p> Source code in <code>utu/tools/python_executor_toolkit.py</code> <pre><code>class PythonExecutorToolkit(AsyncBaseToolkit):\n    \"\"\"\n    A tool for executing Python code in a sandboxed environment.\n    \"\"\"\n\n    def __init__(self, config: ToolkitConfig | dict | None = None):\n        super().__init__(config)\n\n    async def get_tools_map(self) -&gt; dict[str, callable]:\n        return {\n            \"execute_python_code\": self.execute_python_code,\n        }\n\n    async def execute_python_code(self, code: str, workdir: str = \"./run_workdir\", timeout: int = 30) -&gt; dict:\n        \"\"\"\n        Executes Python code and returns the output.\n\n        Args:\n            code (str): The Python code to execute.\n            workdir (str): The working directory for the execution. Defaults to \"./run_workdir\".\n            timeout (int): The execution timeout in seconds. Defaults to 30.\n\n        Returns:\n            dict: A dictionary containing the execution results.\n        \"\"\"\n        loop = asyncio.get_running_loop()\n        try:\n            return await asyncio.wait_for(\n                loop.run_in_executor(\n                    None,  # Use the default thread pool executor\n                    _execute_python_code_sync,\n                    code,\n                    workdir,\n                ),\n                timeout=timeout,\n            )\n        except TimeoutError:\n            return {\n                \"success\": False,\n                \"message\": f\"Code execution timed out ({timeout} seconds)\",\n                \"stdout\": \"\",\n                \"stderr\": \"\",\n                \"status\": False,\n                \"output\": \"\",\n                \"files\": [],\n                \"error\": f\"Code execution timed out ({timeout} seconds)\",\n            }\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.execute_python_code","title":"execute_python_code  <code>async</code>","text":"<pre><code>execute_python_code(\n    code: str,\n    workdir: str = \"./run_workdir\",\n    timeout: int = 30,\n) -&gt; dict\n</code></pre> <p>Executes Python code and returns the output.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The Python code to execute.</p> required <code>workdir</code> <code>str</code> <p>The working directory for the execution. Defaults to \"./run_workdir\".</p> <code>'./run_workdir'</code> <code>timeout</code> <code>int</code> <p>The execution timeout in seconds. Defaults to 30.</p> <code>30</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the execution results.</p> Source code in <code>utu/tools/python_executor_toolkit.py</code> <pre><code>async def execute_python_code(self, code: str, workdir: str = \"./run_workdir\", timeout: int = 30) -&gt; dict:\n    \"\"\"\n    Executes Python code and returns the output.\n\n    Args:\n        code (str): The Python code to execute.\n        workdir (str): The working directory for the execution. Defaults to \"./run_workdir\".\n        timeout (int): The execution timeout in seconds. Defaults to 30.\n\n    Returns:\n        dict: A dictionary containing the execution results.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    try:\n        return await asyncio.wait_for(\n            loop.run_in_executor(\n                None,  # Use the default thread pool executor\n                _execute_python_code_sync,\n                code,\n                workdir,\n            ),\n            timeout=timeout,\n        )\n    except TimeoutError:\n        return {\n            \"success\": False,\n            \"message\": f\"Code execution timed out ({timeout} seconds)\",\n            \"stdout\": \"\",\n            \"stderr\": \"\",\n            \"status\": False,\n            \"output\": \"\",\n            \"files\": [],\n            \"error\": f\"Code execution timed out ({timeout} seconds)\",\n        }\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.get_tools_map_func","title":"get_tools_map_func  <code>async</code>","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.tools_map is None:\n        self.tools_map = await self.get_tools_map()\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.get_tools_in_agents","title":"get_tools_in_agents  <code>async</code>","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.get_tools_in_openai","title":"get_tools_in_openai  <code>async</code>","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.get_tools_in_mcp","title":"get_tools_in_mcp  <code>async</code>","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/search_toolkit/","title":"<code>SearchToolkit</code>","text":""},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit","title":"SearchToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/search_toolkit.py</code> <pre><code>class SearchToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None):\n        \"\"\"Initialize the SearchToolkit.\n\n        - Required env variables: `JINA_API_KEY`, `SERPER_API_KEY`\"\"\"\n        super().__init__(config)\n        self.jina_url_template = r\"https://r.jina.ai/{url}\"\n        self.jina_header = {\"Authorization\": f\"Bearer {self.config.config.get('JINA_API_KEY')}\"}\n        self.serper_url = r\"https://google.serper.dev/search\"\n        self.serper_header = {\"X-API-KEY\": self.config.config.get(\"SERPER_API_KEY\"), \"Content-Type\": \"application/json\"}\n        # config\n        self.llm = SimplifiedAsyncOpenAI(\n            **self.config.config_llm.model_provider.model_dump() if self.config.config_llm else {}\n        )\n        self.summary_token_limit = self.config.config.get(\"summary_token_limit\", 1_000)\n\n    @async_file_cache(expire_time=None)\n    async def search_google(self, query: str):\n        params = {\"q\": query, \"gl\": \"cn\", \"hl\": \"zh-cn\", \"num\": 100}\n        async with aiohttp.ClientSession() as session:\n            async with session.post(self.serper_url, headers=self.serper_header, json=params) as response:\n                response.raise_for_status()  # avoid cache error!\n                results = await response.json()\n                return results\n\n    async def search_google_api(self, query: str, num_results: int = 5) -&gt; dict:\n        \"\"\"web search to gather information from the web.\n\n        Tips:\n        1. search query should be concrete and not vague or super long\n        2. try to add Google search operators in query if necessary,\n        - \" \" for exact match;\n        - -xxx for exclude;\n        - * wildcard matching;\n        - filetype:xxx for file types;\n        - site:xxx for site search;\n        - before:YYYY-MM-DD, after:YYYY-MM-DD for time range.\n\n        Args:\n            query (str): The query to search for.\n            num_results (int, optional): The number of results to return. Defaults to 5.\n        \"\"\"\n        # https://serper.dev/playground\n        logger.info(f\"[tool] search_google_api: {oneline_object(query)}\")\n        res = await self.search_google(query)\n        # filter the search results\n        results = self._filter_results(res[\"organic\"], num_results)\n        formatted_results = []\n        for i, r in enumerate(results, 1):\n            formatted_results.append(f\"{i}. {r['title']} ({r['link']})\")\n            if \"snippet\" in r:\n                formatted_results[-1] += f\"\\nsnippet: {r['snippet']}\"\n            if \"sitelinks\" in r:\n                formatted_results[-1] += f\"\\nsitelinks: {r['sitelinks']}\"\n        msg = \"\\n\".join(formatted_results)\n        logger.info(oneline_object(msg))\n        return msg\n\n    def _filter_results(self, results: list[dict], limit: int) -&gt; list[dict]:\n        # can also use search operator `-site:huggingface.co`\n        # ret: {title, link, snippet, position, | sitelinks}\n        res = []\n        for result in results:\n            if not RE_MATCHED_SITES.match(result[\"link\"]):\n                res.append(result)\n            if len(res) &gt;= limit:\n                break\n        return res\n\n    @async_file_cache(expire_time=None)\n    async def get_content(self, url: str) -&gt; str:\n        # Get the content of the url\n        logger.info(f\"[tool] get_content: {oneline_object(url)}\")\n        async with aiohttp.ClientSession() as session:\n            async with session.get(self.jina_url_template.format(url=url), headers=self.jina_header) as response:\n                text = await response.text()\n                logger.info(f\"[tool] get_content: {oneline_object(text)}...\")\n                return text\n\n    async def web_qa(self, url: str, query: str) -&gt; str:\n        \"\"\"Ask question to a webpage, you will get the answer and related links from the specified url.\n\n        Tips:\n        - Use cases: gather information from a webpage, ask detailed questions.\n\n        Args:\n            url (str): The url to ask question to.\n            query (str): The question to ask. Should be clear, concise, and specific.\n        \"\"\"\n        logger.info(f\"[tool] web_qa: {oneline_object({url, query})}\")\n        content = await self.get_content(url)\n        query = (\n            query or \"Summarize the content of this webpage, in the same language as the webpage.\"\n        )  # use the same language\n        res_summary, res_links = await asyncio.gather(\n            self._qa(content, query), self._extract_links(url, content, query)\n        )\n        result = f\"Summary: {res_summary}\\n\\nRelated Links: {res_links}\"\n        return result\n\n    async def _qa(self, content: str, query: str) -&gt; str:\n        template = TOOL_PROMPTS[\"search_qa\"].format(content=content, query=query)\n        return await self.llm.query_one(\n            messages=[{\"role\": \"user\", \"content\": template}], **self.config.config_llm.model_params.model_dump()\n        )\n\n    async def _extract_links(self, url: str, content: str, query: str) -&gt; str:\n        template = TOOL_PROMPTS[\"search_related\"].format(url=url, content=content, query=query)\n        return await self.llm.query_one(\n            messages=[{\"role\": \"user\", \"content\": template}], **self.config.config_llm.model_params.model_dump()\n        )\n\n    async def get_tools_map(self) -&gt; dict[str, Callable]:\n        return {\n            \"search_google_api\": self.search_google_api,\n            # \"get_content\": self.get_content,\n            \"web_qa\": self.web_qa,\n        }\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.__init__","title":"__init__","text":"<pre><code>__init__(config: ToolkitConfig = None)\n</code></pre> <p>Initialize the SearchToolkit.</p> <ul> <li>Required env variables: <code>JINA_API_KEY</code>, <code>SERPER_API_KEY</code></li> </ul> Source code in <code>utu/tools/search_toolkit.py</code> <pre><code>def __init__(self, config: ToolkitConfig = None):\n    \"\"\"Initialize the SearchToolkit.\n\n    - Required env variables: `JINA_API_KEY`, `SERPER_API_KEY`\"\"\"\n    super().__init__(config)\n    self.jina_url_template = r\"https://r.jina.ai/{url}\"\n    self.jina_header = {\"Authorization\": f\"Bearer {self.config.config.get('JINA_API_KEY')}\"}\n    self.serper_url = r\"https://google.serper.dev/search\"\n    self.serper_header = {\"X-API-KEY\": self.config.config.get(\"SERPER_API_KEY\"), \"Content-Type\": \"application/json\"}\n    # config\n    self.llm = SimplifiedAsyncOpenAI(\n        **self.config.config_llm.model_provider.model_dump() if self.config.config_llm else {}\n    )\n    self.summary_token_limit = self.config.config.get(\"summary_token_limit\", 1_000)\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.search_google_api","title":"search_google_api  <code>async</code>","text":"<pre><code>search_google_api(query: str, num_results: int = 5) -&gt; dict\n</code></pre> <p>web search to gather information from the web.</p> <p>Tips: 1. search query should be concrete and not vague or super long 2. try to add Google search operators in query if necessary, - \" \" for exact match; - -xxx for exclude; - * wildcard matching; - filetype:xxx for file types; - site:xxx for site search; - before:YYYY-MM-DD, after:YYYY-MM-DD for time range.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to search for.</p> required <code>num_results</code> <code>int</code> <p>The number of results to return. Defaults to 5.</p> <code>5</code> Source code in <code>utu/tools/search_toolkit.py</code> <pre><code>async def search_google_api(self, query: str, num_results: int = 5) -&gt; dict:\n    \"\"\"web search to gather information from the web.\n\n    Tips:\n    1. search query should be concrete and not vague or super long\n    2. try to add Google search operators in query if necessary,\n    - \" \" for exact match;\n    - -xxx for exclude;\n    - * wildcard matching;\n    - filetype:xxx for file types;\n    - site:xxx for site search;\n    - before:YYYY-MM-DD, after:YYYY-MM-DD for time range.\n\n    Args:\n        query (str): The query to search for.\n        num_results (int, optional): The number of results to return. Defaults to 5.\n    \"\"\"\n    # https://serper.dev/playground\n    logger.info(f\"[tool] search_google_api: {oneline_object(query)}\")\n    res = await self.search_google(query)\n    # filter the search results\n    results = self._filter_results(res[\"organic\"], num_results)\n    formatted_results = []\n    for i, r in enumerate(results, 1):\n        formatted_results.append(f\"{i}. {r['title']} ({r['link']})\")\n        if \"snippet\" in r:\n            formatted_results[-1] += f\"\\nsnippet: {r['snippet']}\"\n        if \"sitelinks\" in r:\n            formatted_results[-1] += f\"\\nsitelinks: {r['sitelinks']}\"\n    msg = \"\\n\".join(formatted_results)\n    logger.info(oneline_object(msg))\n    return msg\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.web_qa","title":"web_qa  <code>async</code>","text":"<pre><code>web_qa(url: str, query: str) -&gt; str\n</code></pre> <p>Ask question to a webpage, you will get the answer and related links from the specified url.</p> <p>Tips: - Use cases: gather information from a webpage, ask detailed questions.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url to ask question to.</p> required <code>query</code> <code>str</code> <p>The question to ask. Should be clear, concise, and specific.</p> required Source code in <code>utu/tools/search_toolkit.py</code> <pre><code>async def web_qa(self, url: str, query: str) -&gt; str:\n    \"\"\"Ask question to a webpage, you will get the answer and related links from the specified url.\n\n    Tips:\n    - Use cases: gather information from a webpage, ask detailed questions.\n\n    Args:\n        url (str): The url to ask question to.\n        query (str): The question to ask. Should be clear, concise, and specific.\n    \"\"\"\n    logger.info(f\"[tool] web_qa: {oneline_object({url, query})}\")\n    content = await self.get_content(url)\n    query = (\n        query or \"Summarize the content of this webpage, in the same language as the webpage.\"\n    )  # use the same language\n    res_summary, res_links = await asyncio.gather(\n        self._qa(content, query), self._extract_links(url, content, query)\n    )\n    result = f\"Summary: {res_summary}\\n\\nRelated Links: {res_links}\"\n    return result\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.get_tools_map_func","title":"get_tools_map_func  <code>async</code>","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.tools_map is None:\n        self.tools_map = await self.get_tools_map()\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.get_tools_in_agents","title":"get_tools_in_agents  <code>async</code>","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.get_tools_in_openai","title":"get_tools_in_openai  <code>async</code>","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.get_tools_in_mcp","title":"get_tools_in_mcp  <code>async</code>","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/","title":"<code>TabularDataToolkit</code>","text":""},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit","title":"TabularDataToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/tabular_data_toolkit.py</code> <pre><code>class TabularDataToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None):\n        super().__init__(config)\n        self.llm = SimplifiedAsyncOpenAI(\n            **self.config.config_llm.model_provider.model_dump() if self.config.config_llm else {}\n        )\n\n    def get_tabular_columns(self, file_path: str, return_feat: list[str] = None) -&gt; str:\n        \"\"\"Extract raw column metadata from tabular data files.\n\n        Directly reads file and returns basic column information:\n        column names, data types, and sample values.\n\n        Args:\n            file_path (str): Path to the tabular data file.\n\n        Returns:\n            str: Formatted string with raw column information.\n        \"\"\"\n        logger.info(f\"[tool] get_tabular_columns: {file_path}\")\n        if not os.path.exists(file_path):\n            return self._stringify_column_info([{\"error\": f\"File '{file_path}' does not exist.\"}])\n\n        try:\n            # 1. Load the tabular data using the helper function\n            df = self._load_tabular_data(file_path)\n            # 2. Build column information\n            column_info = []\n            for col in df.columns:\n                try:\n                    # Get data type\n                    dtype = str(df[col].dtype)\n\n                    # Get a non-null sample value\n                    sample_value = None\n                    non_null_values = df[col].dropna()\n                    if len(non_null_values) &gt; 0:\n                        # Get the first non-null value as sample\n                        sample_value = non_null_values.iloc[0]\n                        # Convert to string, handling different data types\n                        if pd.isna(sample_value):\n                            sample_str = \"NaN\"\n                        elif isinstance(sample_value, float):\n                            if math.isnan(sample_value):\n                                sample_str = \"NaN\"\n                            else:\n                                sample_str = str(sample_value)\n                        else:\n                            sample_str = str(sample_value)\n                    else:\n                        sample_str = \"No data\"\n\n                    column_info.append({\"column_name\": str(col), \"type\": dtype, \"sample\": sample_str})\n\n                except Exception as e:  # pylint: disable=broad-except\n                    logger.warning(f\"Error processing column '{col}': {e}\")\n                    column_info.append({\"column_name\": str(col), \"type\": \"unknown\", \"sample\": \"Error reading sample\"})\n\n            return self._stringify_column_info(column_info, return_feat=return_feat)\n\n        except Exception as e:  # pylint: disable=broad-except\n            error_msg = f\"Error reading file '{file_path}': {str(e)}\"\n            logger.error(error_msg)\n            return self._stringify_column_info([{\"error\": error_msg}], return_feat=return_feat)\n\n    @async_file_cache(mode=\"file\", expire_time=None)\n    async def get_column_info(self, file_path: str) -&gt; str:\n        \"\"\"Intelligently analyze and interpret column information.\n\n        Builds on get_tabular_columns() to provide simple file structure analysis\n        and column meaning interpretation.\n\n        Args:\n            file_path (str): Path to the tabular data file.\n\n        Returns:\n            str: Analysis with file structure and column explanations.\n        \"\"\"\n        column_info_str = self.get_tabular_columns(file_path)\n        prompt = TEMPLATE_COLUMN_QA.format(column_info=column_info_str)\n        logger.info(f\"[tool] get_column_info: {file_path}\")\n\n        try:\n            response = await self.llm.query_one(\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                # **self.config.config_llm.model_params.model_dump()\n            )\n            return response\n        except Exception as e:  # pylint: disable=broad-except\n            error_msg = f\"Error during LLM processing: {str(e)}\"\n            logger.error(error_msg)\n            return error_msg\n\n    def _load_tabular_data(self, file_path: str) -&gt; pd.DataFrame:\n        \"\"\"Load tabular data from a file and return as a DataFrame.\n\n        Returns:\n            pd.DataFrame: DataFrame containing the tabular data.\n        Raises:\n            Exception: If the file cannot be loaded as tabular data.\n        \"\"\"\n        # Get file extension to determine how to read the file\n        file_ext = pathlib.Path(file_path).suffix.lower()\n\n        # Read the file based on its extension\n        if file_ext == \".csv\":\n            # Try different encodings for CSV files\n            encodings = [\"utf-8\", \"latin1\", \"cp1252\", \"iso-8859-1\"]\n            df = None\n            for encoding in encodings:\n                try:\n                    df = pd.read_csv(file_path, encoding=encoding)\n                    break\n                except UnicodeDecodeError:\n                    continue\n            if df is None:\n                raise Exception(\"Could not read CSV file with any supported encoding\")\n        elif file_ext in [\".xlsx\", \".xls\"]:\n            df = pd.read_excel(file_path)\n        elif file_ext == \".json\":\n            # Try to read JSON as tabular data\n            df = pd.read_json(file_path)\n        elif file_ext == \".parquet\":\n            df = pd.read_parquet(file_path)\n        elif file_ext == \".tsv\":\n            # Tab-separated values\n            encodings = [\"utf-8\", \"latin1\", \"cp1252\", \"iso-8859-1\"]\n            df = None\n            for encoding in encodings:\n                try:\n                    df = pd.read_csv(file_path, sep=\"\\t\", encoding=encoding)\n                    break\n                except UnicodeDecodeError:\n                    continue\n            if df is None:\n                raise Exception(\"Could not read TSV file with any supported encoding\")\n        else:\n            # Try to read as CSV by default\n            try:\n                df = pd.read_csv(file_path)\n            except Exception as e:  # pylint: disable=broad-except\n                raise Exception(f\"Unsupported file format: {file_ext}\") from e\n\n        return df\n\n    def _stringify_column_info(self, column_info: list[dict], return_feat: list[str] = None) -&gt; str:\n        \"\"\"Convert column information to a formatted string.\"\"\"\n        if \"error\" in column_info[0]:\n            return column_info[0][\"error\"]\n\n        lines = []\n        return_keys = [\"column_name\", \"type\", \"sample\"]\n        if return_feat:\n            return_keys = [key for key in return_keys if key in return_feat]\n        for i, col in enumerate(column_info):\n            lines.append(\n                f\"- Column {i + 1}: {json.dumps({k: col[k] for k in return_keys if k in col}, ensure_ascii=False)}\"\n            )\n        return \"\\n\".join(lines)\n\n    async def get_tools_map(self) -&gt; dict[str, Callable]:\n        \"\"\"Return a mapping of tool names to their corresponding methods.\"\"\"\n        return {\n            # \"get_tabular_columns\": self.get_tabular_columns,\n            # \"get_column_info\": self.get_column_info,\n        }\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.get_tabular_columns","title":"get_tabular_columns","text":"<pre><code>get_tabular_columns(\n    file_path: str, return_feat: list[str] = None\n) -&gt; str\n</code></pre> <p>Extract raw column metadata from tabular data files.</p> <p>Directly reads file and returns basic column information: column names, data types, and sample values.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the tabular data file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted string with raw column information.</p> Source code in <code>utu/tools/tabular_data_toolkit.py</code> <pre><code>def get_tabular_columns(self, file_path: str, return_feat: list[str] = None) -&gt; str:\n    \"\"\"Extract raw column metadata from tabular data files.\n\n    Directly reads file and returns basic column information:\n    column names, data types, and sample values.\n\n    Args:\n        file_path (str): Path to the tabular data file.\n\n    Returns:\n        str: Formatted string with raw column information.\n    \"\"\"\n    logger.info(f\"[tool] get_tabular_columns: {file_path}\")\n    if not os.path.exists(file_path):\n        return self._stringify_column_info([{\"error\": f\"File '{file_path}' does not exist.\"}])\n\n    try:\n        # 1. Load the tabular data using the helper function\n        df = self._load_tabular_data(file_path)\n        # 2. Build column information\n        column_info = []\n        for col in df.columns:\n            try:\n                # Get data type\n                dtype = str(df[col].dtype)\n\n                # Get a non-null sample value\n                sample_value = None\n                non_null_values = df[col].dropna()\n                if len(non_null_values) &gt; 0:\n                    # Get the first non-null value as sample\n                    sample_value = non_null_values.iloc[0]\n                    # Convert to string, handling different data types\n                    if pd.isna(sample_value):\n                        sample_str = \"NaN\"\n                    elif isinstance(sample_value, float):\n                        if math.isnan(sample_value):\n                            sample_str = \"NaN\"\n                        else:\n                            sample_str = str(sample_value)\n                    else:\n                        sample_str = str(sample_value)\n                else:\n                    sample_str = \"No data\"\n\n                column_info.append({\"column_name\": str(col), \"type\": dtype, \"sample\": sample_str})\n\n            except Exception as e:  # pylint: disable=broad-except\n                logger.warning(f\"Error processing column '{col}': {e}\")\n                column_info.append({\"column_name\": str(col), \"type\": \"unknown\", \"sample\": \"Error reading sample\"})\n\n        return self._stringify_column_info(column_info, return_feat=return_feat)\n\n    except Exception as e:  # pylint: disable=broad-except\n        error_msg = f\"Error reading file '{file_path}': {str(e)}\"\n        logger.error(error_msg)\n        return self._stringify_column_info([{\"error\": error_msg}], return_feat=return_feat)\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.get_column_info","title":"get_column_info  <code>async</code>","text":"<pre><code>get_column_info(file_path: str) -&gt; str\n</code></pre> <p>Intelligently analyze and interpret column information.</p> <p>Builds on get_tabular_columns() to provide simple file structure analysis and column meaning interpretation.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the tabular data file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Analysis with file structure and column explanations.</p> Source code in <code>utu/tools/tabular_data_toolkit.py</code> <pre><code>@async_file_cache(mode=\"file\", expire_time=None)\nasync def get_column_info(self, file_path: str) -&gt; str:\n    \"\"\"Intelligently analyze and interpret column information.\n\n    Builds on get_tabular_columns() to provide simple file structure analysis\n    and column meaning interpretation.\n\n    Args:\n        file_path (str): Path to the tabular data file.\n\n    Returns:\n        str: Analysis with file structure and column explanations.\n    \"\"\"\n    column_info_str = self.get_tabular_columns(file_path)\n    prompt = TEMPLATE_COLUMN_QA.format(column_info=column_info_str)\n    logger.info(f\"[tool] get_column_info: {file_path}\")\n\n    try:\n        response = await self.llm.query_one(\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            # **self.config.config_llm.model_params.model_dump()\n        )\n        return response\n    except Exception as e:  # pylint: disable=broad-except\n        error_msg = f\"Error during LLM processing: {str(e)}\"\n        logger.error(error_msg)\n        return error_msg\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.get_tools_map","title":"get_tools_map  <code>async</code>","text":"<pre><code>get_tools_map() -&gt; dict[str, Callable]\n</code></pre> <p>Return a mapping of tool names to their corresponding methods.</p> Source code in <code>utu/tools/tabular_data_toolkit.py</code> <pre><code>async def get_tools_map(self) -&gt; dict[str, Callable]:\n    \"\"\"Return a mapping of tool names to their corresponding methods.\"\"\"\n    return {\n        # \"get_tabular_columns\": self.get_tabular_columns,\n        # \"get_column_info\": self.get_column_info,\n    }\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.get_tools_map_func","title":"get_tools_map_func  <code>async</code>","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.tools_map is None:\n        self.tools_map = await self.get_tools_map()\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.get_tools_in_agents","title":"get_tools_in_agents  <code>async</code>","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.get_tools_in_openai","title":"get_tools_in_openai  <code>async</code>","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.get_tools_in_mcp","title":"get_tools_in_mcp  <code>async</code>","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/video_toolkit/","title":"<code>VideoToolkit</code>","text":"<p>https://github.com/googleapis/python-genai https://ai.google.dev/gemini-api/docs/api-key</p>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit","title":"VideoToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/video_toolkit.py</code> <pre><code>class VideoToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None) -&gt; None:\n        super().__init__(config)\n        self.client = genai.Client(\n            api_key=self.config.config.get(\"google_api_key\"), http_options=HttpOptions(api_version=\"v1alpha\")\n        )\n        self.model = self.config.config.get(\"google_model\")\n\n    async def video_qa(self, video_url: str, question: str) -&gt; str:\n        r\"\"\"Asks a question about the video.\n\n        Args:\n            video_path (str): The path or URL to the video file.\n            question (str): The question to ask about the video.\n        \"\"\"\n        if not video_url.startswith(\"http\"):\n            video_part = Part.from_uri(file_uri=video_url)\n        else:\n            # e.g. Youtube URL\n            video_part = Part.from_uri(\n                file_uri=video_url,\n                mime_type=\"video/mp4\",\n            )\n        response = self.client.models.generate_content(\n            model=self.model,\n            contents=[\n                question,\n                video_part,\n            ],\n        )\n\n        logger.debug(f\"Video analysis response from gemini: {response.text}\")\n        return response.text\n\n    async def get_tools_map(self) -&gt; dict[str, Callable]:\n        return {\n            \"video_qa\": self.video_qa,\n        }\n</code></pre>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.video_qa","title":"video_qa  <code>async</code>","text":"<pre><code>video_qa(video_url: str, question: str) -&gt; str\n</code></pre> <p>Asks a question about the video.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The path or URL to the video file.</p> required <code>question</code> <code>str</code> <p>The question to ask about the video.</p> required Source code in <code>utu/tools/video_toolkit.py</code> <pre><code>async def video_qa(self, video_url: str, question: str) -&gt; str:\n    r\"\"\"Asks a question about the video.\n\n    Args:\n        video_path (str): The path or URL to the video file.\n        question (str): The question to ask about the video.\n    \"\"\"\n    if not video_url.startswith(\"http\"):\n        video_part = Part.from_uri(file_uri=video_url)\n    else:\n        # e.g. Youtube URL\n        video_part = Part.from_uri(\n            file_uri=video_url,\n            mime_type=\"video/mp4\",\n        )\n    response = self.client.models.generate_content(\n        model=self.model,\n        contents=[\n            question,\n            video_part,\n        ],\n    )\n\n    logger.debug(f\"Video analysis response from gemini: {response.text}\")\n    return response.text\n</code></pre>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.get_tools_map_func","title":"get_tools_map_func  <code>async</code>","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.tools_map is None:\n        self.tools_map = await self.get_tools_map()\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.get_tools_in_agents","title":"get_tools_in_agents  <code>async</code>","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.get_tools_in_openai","title":"get_tools_in_openai  <code>async</code>","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.get_tools_in_mcp","title":"get_tools_in_mcp  <code>async</code>","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = await self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = await self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tracing/db_tracer/","title":"<code>DBTracingProcessor</code>","text":"<p>               Bases: <code>TracingProcessor</code></p> <p>Basic tracing processor that stores events into database.</p> <p>Required environment variables: <code>DB_URL</code></p> Source code in <code>utu/tracing/db_tracer.py</code> <pre><code>class DBTracingProcessor(TracingProcessor):\n    \"\"\"Basic tracing processor that stores events into database.\n\n    Required environment variables: `DB_URL`\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        if not SQLModelUtils.check_db_available():\n            logger.warning(\"DB_URL not set or database connection failed! Tracing will not be stored into database!\")\n            self.enabled = False\n        else:\n            self.enabled = True\n\n    def on_trace_start(self, trace: Trace) -&gt; None:\n        pass\n\n    def on_trace_end(self, trace: Trace) -&gt; None:\n        pass\n\n    def on_span_start(self, span: Span[Any]) -&gt; None:\n        pass\n\n    def on_span_end(self, span: Span[Any]) -&gt; None:\n        if not self.enabled:\n            return\n\n        data = span.span_data\n        if isinstance(data, GenerationSpanData):\n            with SQLModelUtils.create_session() as session:\n                session.add(\n                    GenerationTracingModel(\n                        trace_id=get_current_trace().trace_id,\n                        span_id=span.span_id,\n                        input=data.input,\n                        output=data.output,\n                        model=data.model,\n                        model_configs=data.model_config,\n                        usage=data.usage,\n                    )\n                )\n                session.commit()\n        elif isinstance(data, ResponseSpanData):\n            # print(f\"&gt; response_id={data.response.id}: {data.response.model_dump()}\")\n            with SQLModelUtils.create_session() as session:\n                session.add(\n                    GenerationTracingModel(\n                        trace_id=get_current_trace().trace_id,\n                        span_id=span.span_id,\n                        input=data.input,\n                        output=OpenAIUtils.get_response_output(data.response),\n                        model=OpenAIUtils.maybe_basemodel_to_dict(data.response.model),\n                        model_configs=OpenAIUtils.get_response_configs(data.response),\n                        usage=OpenAIUtils.maybe_basemodel_to_dict(data.response.usage),\n                        type=\"responses\",\n                        response_id=data.response.id,\n                    )\n                )\n                session.commit()\n        elif isinstance(data, FunctionSpanData):\n            with SQLModelUtils.create_session() as session:\n                session.add(\n                    ToolTracingModel(\n                        name=data.name,\n                        input=data.input,\n                        output=data.output,\n                        mcp_data=data.mcp_data,\n                        trace_id=get_current_trace().trace_id,\n                        span_id=span.span_id,\n                    )\n                )\n                session.commit()\n\n    def force_flush(self) -&gt; None:\n        pass\n\n    def shutdown(self) -&gt; None:\n        pass\n</code></pre>"},{"location":"ref/utils/agents_utils/","title":"<code>AgentsUtils</code>","text":""},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.ChatCompletionConverter","title":"ChatCompletionConverter","text":"<p>               Bases: <code>Converter</code></p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>class ChatCompletionConverter(Converter):\n    @classmethod\n    def items_to_messages(cls, items: str | Iterable[TResponseInputItem]) -&gt; list[ChatCompletionMessageParam]:\n        # skip reasoning, see chatcmpl_converter.Converter.items_to_messages()\n        # agents.exceptions.UserError: Unhandled item type or structure:\n        # {'id': '__fake_id__', 'summary': [{'text': '...', 'type': 'summary_text'}], 'type': 'reasoning'}\n        if not isinstance(items, str):  # TODO: check it!\n            items = cls.filter_items(items)\n        return Converter.items_to_messages(items)\n\n    @classmethod\n    def filter_items(cls, items: str | Iterable[TResponseInputItem]) -&gt; str | list[TResponseInputItem]:\n        if isinstance(items, str):\n            return items\n        filtered_items = []\n        for item in items:\n            if item.get(\"type\", None) == \"reasoning\":\n                continue\n            filtered_items.append(item)\n        return filtered_items\n\n    @classmethod\n    def items_to_dict(cls, items: str | Iterable[TResponseInputItem]) -&gt; list[dict]:\n        \"\"\"convert items to a list of dict which have {\"role\", \"content\"}\n        WIP!\n        \"\"\"\n        if isinstance(items, str):\n            return [{\"role\": \"user\", \"content\": items}]\n        result = []\n        for item in items:\n            if msg := Converter.maybe_easy_input_message(item):\n                result.append(msg)\n            elif msg := Converter.maybe_input_message(item):\n                result.append(msg)\n            elif msg := Converter.maybe_response_output_message(item):\n                result.append(msg)\n            elif msg := Converter.maybe_file_search_call(item):\n                msg.update({\"role\": \"tool\", \"content\": msg[\"results\"]})\n                result.append(msg)\n            elif msg := Converter.maybe_function_tool_call(item):\n                msg.update({\"role\": \"assistant\", \"content\": f\"{msg['name']}({msg['arguments']})\"})\n                result.append(msg)\n            elif msg := Converter.maybe_function_tool_call_output(item):\n                msg.update({\"role\": \"tool\", \"content\": msg[\"output\"], \"tool_call_id\": msg[\"call_id\"]})\n                result.append(msg)\n            elif msg := Converter.maybe_reasoning_message(item):\n                msg.update({\"role\": \"assistant\", \"content\": msg[\"summary\"]})\n                result.append(msg)\n            else:\n                logger.warning(f\"Unknown message type: {item}\")\n                result.append({\"role\": \"assistant\", \"content\": f\"Unknown message type: {item}\"})\n        return result\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.ChatCompletionConverter.items_to_dict","title":"items_to_dict  <code>classmethod</code>","text":"<pre><code>items_to_dict(\n    items: str | Iterable[TResponseInputItem],\n) -&gt; list[dict]\n</code></pre> <p>convert items to a list of dict which have {\"role\", \"content\"} WIP!</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>@classmethod\ndef items_to_dict(cls, items: str | Iterable[TResponseInputItem]) -&gt; list[dict]:\n    \"\"\"convert items to a list of dict which have {\"role\", \"content\"}\n    WIP!\n    \"\"\"\n    if isinstance(items, str):\n        return [{\"role\": \"user\", \"content\": items}]\n    result = []\n    for item in items:\n        if msg := Converter.maybe_easy_input_message(item):\n            result.append(msg)\n        elif msg := Converter.maybe_input_message(item):\n            result.append(msg)\n        elif msg := Converter.maybe_response_output_message(item):\n            result.append(msg)\n        elif msg := Converter.maybe_file_search_call(item):\n            msg.update({\"role\": \"tool\", \"content\": msg[\"results\"]})\n            result.append(msg)\n        elif msg := Converter.maybe_function_tool_call(item):\n            msg.update({\"role\": \"assistant\", \"content\": f\"{msg['name']}({msg['arguments']})\"})\n            result.append(msg)\n        elif msg := Converter.maybe_function_tool_call_output(item):\n            msg.update({\"role\": \"tool\", \"content\": msg[\"output\"], \"tool_call_id\": msg[\"call_id\"]})\n            result.append(msg)\n        elif msg := Converter.maybe_reasoning_message(item):\n            msg.update({\"role\": \"assistant\", \"content\": msg[\"summary\"]})\n            result.append(msg)\n        else:\n            logger.warning(f\"Unknown message type: {item}\")\n            result.append({\"role\": \"assistant\", \"content\": f\"Unknown message type: {item}\"})\n    return result\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.AgentsUtils","title":"AgentsUtils","text":"<p>Utils for openai-agents SDK</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>class AgentsUtils:\n    \"\"\"Utils for openai-agents SDK\"\"\"\n\n    @staticmethod\n    def generate_group_id() -&gt; str:\n        \"\"\"Generate a unique group ID. (Used in OpenAI tracing)\n        Ref: https://openai.github.io/openai-agents-python/tracing/\n        \"\"\"\n        return uuid.uuid4().hex[:16]\n\n    @staticmethod\n    def gen_trace_id() -&gt; str:\n        return gen_trace_id()\n\n    @staticmethod\n    def get_current_trace() -&gt; Trace:\n        return get_current_trace()\n\n    @staticmethod\n    def get_agents_model(\n        type: Literal[\"responses\", \"chat.completions\"] = None,\n        model: str = None,\n        base_url: str = None,\n        api_key: str = None,\n    ) -&gt; OpenAIChatCompletionsModel | OpenAIResponsesModel:\n        type = type or os.getenv(\"UTU_LLM_TYPE\", \"chat.completions\")\n        model = model or os.getenv(\"UTU_LLM_MODEL\")\n        base_url = base_url or os.getenv(\"UTU_LLM_BASE_URL\")\n        api_key = api_key or os.getenv(\"UTU_LLM_API_KEY\")\n        if not api_key or not base_url:\n            raise ValueError(\"UTU_LLM_API_KEY and UTU_LLM_BASE_URL must be set\")\n        openai_client = AsyncOpenAI(\n            api_key=api_key,\n            base_url=base_url,\n            timeout=100,\n        )\n        if type == \"chat.completions\":\n            return OpenAIChatCompletionsModel(model=model, openai_client=openai_client)\n        elif type == \"responses\":\n            return OpenAIResponsesModel(model=model, openai_client=openai_client)\n        else:\n            raise ValueError(\"Invalid type: \" + type)\n\n    @staticmethod\n    def get_trajectory_from_agent_result(agent_result: RunResult) -&gt; dict:\n        return {\n            \"agent\": agent_result.last_agent.name,\n            \"trajectory\": ChatCompletionConverter.items_to_messages(agent_result.to_input_list()),\n        }\n\n    @staticmethod\n    def print_new_items(new_items: list[RunItem]) -&gt; None:\n        \"\"\"Print new items generated by Runner.run()\"\"\"\n        for new_item in new_items:\n            agent_name = new_item.agent.name\n            if isinstance(new_item, MessageOutputItem):\n                PrintUtils.print_bot(f\"{agent_name}: {ItemHelpers.text_message_output(new_item)}\")\n            elif isinstance(new_item, HandoffOutputItem):\n                PrintUtils.print_info(f\"Handed off from {new_item.source_agent.name} to {new_item.target_agent.name}\")\n            elif isinstance(new_item, ToolCallItem):\n                assert isinstance(new_item.raw_item, ResponseFunctionToolCall)  # DONOT use openai's built-in tools\n                PrintUtils.print_info(\n                    f\"{agent_name}: Calling a tool: {new_item.raw_item.name}({json.loads(new_item.raw_item.arguments)})\"\n                )\n            elif isinstance(new_item, ToolCallOutputItem):\n                PrintUtils.print_tool(f\"Tool call output: {new_item.output}\")\n            elif isinstance(new_item, ReasoningItem):\n                PrintUtils.print_info(f\"{agent_name}: Reasoning: {new_item.raw_item}\")\n            else:\n                PrintUtils.print_info(f\"{agent_name}: Skipping item: {new_item.__class__.__name__}\")\n\n    @staticmethod\n    async def print_stream_events(result: AsyncIterator[StreamEvent]) -&gt; None:\n        \"\"\"Print stream events generated by Runner.run_streamed()\"\"\"\n        async for event in result:\n            # print(f\"&gt; [DEBUG] event: {event}\")\n            if isinstance(event, RawResponsesStreamEvent):\n                # event.data -- ResponseStreamEvent\n                if event.data.type == \"response.output_text.delta\":\n                    PrintUtils.print_info(f\"{event.data.delta}\", end=\"\")\n                elif event.data.type == \"response.reasoning_text.delta\":\n                    PrintUtils.print_info(f\"{event.data.delta}\", end=\"\")\n                elif event.data.type == \"response.reasoning_text.done\":\n                    PrintUtils.print_info(\"&lt;/reasoning_text&gt;\", end=\"\")\n                elif event.data.type in (\"response.output_text.done\",):\n                    PrintUtils.print_info(\"\")\n                elif event.data.type in (\n                    \"response.created\",\n                    \"response.completed\",\n                    \"response.in_progress\",\n                    \"response.content_part.added\",\n                    \"response.content_part.done\",\n                    \"response.output_item.added\",\n                    \"response.output_item.done\",\n                    \"response.function_call_arguments.delta\",\n                    \"response.function_call_arguments.done\",\n                ):\n                    pass\n                else:\n                    PrintUtils.print_info(f\"Unknown event type: {event.data.type}! {event}\")\n                    # raise ValueError(f\"Unknown event type: {event.data.type}\")\n            elif isinstance(event, RunItemStreamEvent):\n                item: RunItem = event.item\n                if item.type == \"message_output_item\":\n                    PrintUtils.print_bot(f\"&lt;{item.agent.name}&gt; {ItemHelpers.text_message_output(item).strip()}\")\n                elif item.type == \"handoff_call_item\":  # same as `ToolCallItem`\n                    PrintUtils.print_bot(f\"[handoff_call] {item.raw_item.name}({item.raw_item.arguments})\")\n                elif item.type == \"handoff_output_item\":\n                    PrintUtils.print_info(f\"&gt;&gt; Handoff from {item.source_agent.name} to {item.target_agent.name}\")\n                elif item.type == \"tool_call_item\":\n                    PrintUtils.print_bot(\n                        f\"&lt;{item.agent.name}&gt; [tool_call] {item.raw_item.name}({item.raw_item.arguments})\"\n                    )\n                elif item.type == \"tool_call_output_item\":\n                    PrintUtils.print_tool(f\"&lt;{item.agent.name}&gt; [tool_output] {item.output}\")  # item.raw_item\n                elif item.type == \"reasoning_item\":\n                    pass\n                elif event.type in (\n                    \"mcp_list_tools_item\",\n                    \"mcp_approval_request_item\",\n                    \"mcp_approval_response_item\",\n                ):\n                    PrintUtils.print_info(f\"  &gt;&gt;&gt; Skipping item: {event}\")\n                else:\n                    PrintUtils.print_info(f\"  &gt;&gt;&gt; Skipping item: {item.__class__.__name__}\")\n            elif isinstance(event, AgentUpdatedStreamEvent):\n                PrintUtils.print_info(f\"&gt;&gt; new agent: {event.new_agent.name}\")\n            else:\n                raise ValueError(f\"Unknown event type: {event.type}\")\n        print()  # Newline after stream?\n\n    @staticmethod\n    def convert_model_settings(params: OpenAIChatCompletionParams) -&gt; ModelSettings:\n        # \"tools\", \"messages\", \"model\"\n        # FIXME: move to extra_args\n        for p in (\"max_completion_tokens\", \"top_logprobs\", \"logprobs\", \"seed\", \"stop\"):\n            if p in params:\n                logger.warning(f\"Parameter `{p}` is not supported in ModelSettings\")\n        return ModelSettings(\n            max_tokens=params.get(\"max_tokens\", None),\n            temperature=params.get(\"temperature\", None),\n            top_p=params.get(\"top_p\", None),\n            frequency_penalty=params.get(\"frequency_penalty\", None),\n            presence_penalty=params.get(\"presence_penalty\", None),\n            tool_choice=params.get(\"tool_choice\", None),\n            parallel_tool_calls=params.get(\"parallel_tool_calls\", None),\n            extra_query=params.get(\"extra_query\", None),\n            extra_body=params.get(\"extra_body\", None),\n            extra_headers=params.get(\"extra_headers\", None),\n        )\n\n    @staticmethod\n    def convert_sp_input(\n        messages: list[ChatCompletionMessageParam],\n    ) -&gt; tuple[str | None, str | list[TResponseInputItem]]:\n        if isinstance(messages, str):\n            return None, messages\n        if messages[0].get(\"role\", None) == \"system\":\n            return messages[0][\"content\"], messages[1:]\n        return None, messages\n\n    @staticmethod\n    def convert_tool(tool: ChatCompletionToolParam) -&gt; FunctionTool:\n        assert tool[\"type\"] == \"function\"\n        return FunctionTool(\n            name=tool[\"function\"][\"name\"],\n            description=tool[\"function\"].get(\"description\", \"\"),\n            params_json_schema=tool[\"function\"].get(\"parameters\", None),\n            on_invoke_tool=None,\n        )\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.AgentsUtils.generate_group_id","title":"generate_group_id  <code>staticmethod</code>","text":"<pre><code>generate_group_id() -&gt; str\n</code></pre> <p>Generate a unique group ID. (Used in OpenAI tracing) Ref: https://openai.github.io/openai-agents-python/tracing/</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>@staticmethod\ndef generate_group_id() -&gt; str:\n    \"\"\"Generate a unique group ID. (Used in OpenAI tracing)\n    Ref: https://openai.github.io/openai-agents-python/tracing/\n    \"\"\"\n    return uuid.uuid4().hex[:16]\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.AgentsUtils.print_new_items","title":"print_new_items  <code>staticmethod</code>","text":"<pre><code>print_new_items(new_items: list[RunItem]) -&gt; None\n</code></pre> <p>Print new items generated by Runner.run()</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>@staticmethod\ndef print_new_items(new_items: list[RunItem]) -&gt; None:\n    \"\"\"Print new items generated by Runner.run()\"\"\"\n    for new_item in new_items:\n        agent_name = new_item.agent.name\n        if isinstance(new_item, MessageOutputItem):\n            PrintUtils.print_bot(f\"{agent_name}: {ItemHelpers.text_message_output(new_item)}\")\n        elif isinstance(new_item, HandoffOutputItem):\n            PrintUtils.print_info(f\"Handed off from {new_item.source_agent.name} to {new_item.target_agent.name}\")\n        elif isinstance(new_item, ToolCallItem):\n            assert isinstance(new_item.raw_item, ResponseFunctionToolCall)  # DONOT use openai's built-in tools\n            PrintUtils.print_info(\n                f\"{agent_name}: Calling a tool: {new_item.raw_item.name}({json.loads(new_item.raw_item.arguments)})\"\n            )\n        elif isinstance(new_item, ToolCallOutputItem):\n            PrintUtils.print_tool(f\"Tool call output: {new_item.output}\")\n        elif isinstance(new_item, ReasoningItem):\n            PrintUtils.print_info(f\"{agent_name}: Reasoning: {new_item.raw_item}\")\n        else:\n            PrintUtils.print_info(f\"{agent_name}: Skipping item: {new_item.__class__.__name__}\")\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.AgentsUtils.print_stream_events","title":"print_stream_events  <code>async</code> <code>staticmethod</code>","text":"<pre><code>print_stream_events(\n    result: AsyncIterator[StreamEvent],\n) -&gt; None\n</code></pre> <p>Print stream events generated by Runner.run_streamed()</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>@staticmethod\nasync def print_stream_events(result: AsyncIterator[StreamEvent]) -&gt; None:\n    \"\"\"Print stream events generated by Runner.run_streamed()\"\"\"\n    async for event in result:\n        # print(f\"&gt; [DEBUG] event: {event}\")\n        if isinstance(event, RawResponsesStreamEvent):\n            # event.data -- ResponseStreamEvent\n            if event.data.type == \"response.output_text.delta\":\n                PrintUtils.print_info(f\"{event.data.delta}\", end=\"\")\n            elif event.data.type == \"response.reasoning_text.delta\":\n                PrintUtils.print_info(f\"{event.data.delta}\", end=\"\")\n            elif event.data.type == \"response.reasoning_text.done\":\n                PrintUtils.print_info(\"&lt;/reasoning_text&gt;\", end=\"\")\n            elif event.data.type in (\"response.output_text.done\",):\n                PrintUtils.print_info(\"\")\n            elif event.data.type in (\n                \"response.created\",\n                \"response.completed\",\n                \"response.in_progress\",\n                \"response.content_part.added\",\n                \"response.content_part.done\",\n                \"response.output_item.added\",\n                \"response.output_item.done\",\n                \"response.function_call_arguments.delta\",\n                \"response.function_call_arguments.done\",\n            ):\n                pass\n            else:\n                PrintUtils.print_info(f\"Unknown event type: {event.data.type}! {event}\")\n                # raise ValueError(f\"Unknown event type: {event.data.type}\")\n        elif isinstance(event, RunItemStreamEvent):\n            item: RunItem = event.item\n            if item.type == \"message_output_item\":\n                PrintUtils.print_bot(f\"&lt;{item.agent.name}&gt; {ItemHelpers.text_message_output(item).strip()}\")\n            elif item.type == \"handoff_call_item\":  # same as `ToolCallItem`\n                PrintUtils.print_bot(f\"[handoff_call] {item.raw_item.name}({item.raw_item.arguments})\")\n            elif item.type == \"handoff_output_item\":\n                PrintUtils.print_info(f\"&gt;&gt; Handoff from {item.source_agent.name} to {item.target_agent.name}\")\n            elif item.type == \"tool_call_item\":\n                PrintUtils.print_bot(\n                    f\"&lt;{item.agent.name}&gt; [tool_call] {item.raw_item.name}({item.raw_item.arguments})\"\n                )\n            elif item.type == \"tool_call_output_item\":\n                PrintUtils.print_tool(f\"&lt;{item.agent.name}&gt; [tool_output] {item.output}\")  # item.raw_item\n            elif item.type == \"reasoning_item\":\n                pass\n            elif event.type in (\n                \"mcp_list_tools_item\",\n                \"mcp_approval_request_item\",\n                \"mcp_approval_response_item\",\n            ):\n                PrintUtils.print_info(f\"  &gt;&gt;&gt; Skipping item: {event}\")\n            else:\n                PrintUtils.print_info(f\"  &gt;&gt;&gt; Skipping item: {item.__class__.__name__}\")\n        elif isinstance(event, AgentUpdatedStreamEvent):\n            PrintUtils.print_info(f\"&gt;&gt; new agent: {event.new_agent.name}\")\n        else:\n            raise ValueError(f\"Unknown event type: {event.type}\")\n    print()  # Newline after stream?\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.SimplifiedOpenAIChatCompletionsModel","title":"SimplifiedOpenAIChatCompletionsModel","text":"<p>               Bases: <code>OpenAIChatCompletionsModel</code></p> <p>extend OpenAIChatCompletionsModel to support basic api - enable tracing based on SimplifiedAsyncOpenAI</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>class SimplifiedOpenAIChatCompletionsModel(OpenAIChatCompletionsModel):\n    \"\"\"extend OpenAIChatCompletionsModel to support basic api\n    - enable tracing based on SimplifiedAsyncOpenAI\n    \"\"\"\n\n    async def query_one(self, **kwargs) -&gt; str:\n        system_instructions, input = AgentsUtils.convert_sp_input(kwargs[\"messages\"])\n        model_settings = AgentsUtils.convert_model_settings(kwargs)\n        tools = [AgentsUtils.convert_tool(tool) for tool in kwargs.get(\"tools\", [])]\n        response = await self.get_response(\n            system_instructions=system_instructions,\n            input=input,\n            model_settings=model_settings,\n            tools=tools,\n            output_schema=None,\n            handoffs=[],\n            tracing=ModelTracing.ENABLED,\n            previous_response_id=None,\n            prompt=None,\n        )\n        return ChatCompletionConverter.items_to_messages(response.to_input_items())\n</code></pre>"},{"location":"ref/utils/openai_utils/","title":"<code>OpenAIUtils</code>","text":""},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.OpenAIUtils","title":"OpenAIUtils","text":"Source code in <code>utu/utils/openai_utils/openai_utils.py</code> <pre><code>class OpenAIUtils:\n    # --------------------------------------------------------\n    # chat completions\n    # --------------------------------------------------------\n    @staticmethod\n    def print_message(message: ChatCompletionMessage) -&gt; None:\n        if hasattr(message, \"reasoning_content\") and message.reasoning_content:\n            PrintUtils.print_info(f\"{message.reasoning_content}\")\n        if message.content:\n            PrintUtils.print_bot(f\"{message.content}\", add_prefix=True)\n        if message.tool_calls:\n            for tool_call in message.tool_calls:\n                PrintUtils.print_bot(f\"&lt;{tool_call.function.name}&gt;{tool_call.function.arguments}\", add_prefix=True)\n\n    @staticmethod\n    async def print_stream(stream: AsyncStream[ChatCompletionChunk]) -&gt; ChatCompletionMessage:\n        final_tool_calls: dict[int, ChatCompletionMessageToolCall] = {}\n        content = \"\"\n        async for chunk in stream:\n            delta = chunk.choices[0].delta\n            if hasattr(delta, \"reasoning_content\") and delta.reasoning_content:\n                PrintUtils.print_info(f\"{delta.reasoning_content}\", end=\"\", color=\"green\")\n            if delta.content:\n                content += delta.content\n                PrintUtils.print_info(f\"{delta.content}\", end=\"\", color=\"gray\")\n            if delta.tool_calls:\n                for tool_call in delta.tool_calls:\n                    index = tool_call.index\n                    if index not in final_tool_calls:\n                        final_tool_calls[index] = tool_call\n                        PrintUtils.print_info(\n                            f\"&lt;{tool_call.function.name}&gt;{tool_call.function.arguments}\", end=\"\", color=\"blue\"\n                        )\n                    else:\n                        if final_tool_calls[index].function.arguments:\n                            final_tool_calls[index].function.arguments += tool_call.function.arguments\n                        else:\n                            final_tool_calls[index].function.arguments = tool_call.function.arguments\n                        PrintUtils.print_info(f\"{tool_call.function.arguments}\", end=\"\", color=\"blue\")\n        PrintUtils.print_info(\"\")  # print a newline\n        tool_calls = [\n            ChatCompletionMessageFunctionToolCall(\n                id=tool_call.id,\n                function=tool_call.function.model_dump(),\n                type=tool_call.type,  # type is always \"function\"\n            )\n            for tool_call in final_tool_calls.values()\n        ]\n        message = ChatCompletionMessage(role=\"assistant\", content=content, tool_calls=tool_calls)\n        OpenAIUtils.print_message(message)\n        return message\n\n    # --------------------------------------------------------\n    # responses\n    # --------------------------------------------------------\n    @staticmethod\n    def print_response(response: Response) -&gt; None:\n        for item in response.output:\n            # print(f\"&gt; responses item: {item}\")\n            match item.type:\n                case \"reasoning\":\n                    content = getattr(item, \"content\", item.summary)\n                    PrintUtils.print_bot(f\"&lt;reasoning&gt;{content}&lt;/reasoning&gt;\", add_prefix=True, color=\"gray\")\n                case \"message\":\n                    PrintUtils.print_bot(f\"{item.content}\", add_prefix=True)\n                case \"function_call\":\n                    PrintUtils.print_info(f\"&lt;{item.name}&gt;({item.arguments})\")\n                case \"file_search_call\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt;({item.queries})\")\n                case \"web_search_call\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt;({item.action})\")\n                case \"computer_call\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt;({item.action})\")\n                case \"image_generation_call\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt; -&gt; {item.result[:4]}\")\n                case \"code_interpreter_call\":\n                    PrintUtils.print_info(\n                        f\"&lt;{item.type}&gt;(container_id={item.container_id}, code={item.code}) -&gt; {item.outputs}\"\n                    )\n                case \"local_shell_call\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt;(action={item.action})\")\n                case \"mcp_list_tools\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt;(server={item.server_label}) -&gt; {item.tools}\")\n                case \"mcp_call\":\n                    PrintUtils.print_info(\n                        f\"&lt;{item.type}&gt;(server={item.server_label}) {item.name}({item.arguments}) -&gt; {item.output}\"\n                    )\n                case \"mcp_approval_request\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt;(server={item.server_label}) {item.name}({item.arguments})\")\n                case _:\n                    PrintUtils.print_error(f\"Unknown item type: {item.type}\\n{item}\")\n\n    @staticmethod\n    def print_response_stream(stream: AsyncStream[ResponseStreamEvent]) -&gt; Response:\n        raise NotImplementedError\n\n    @staticmethod\n    def get_response_configs(response: Response, include_output: bool = False) -&gt; dict:\n        \"\"\"Get response configs from response\"\"\"\n        data = response.model_dump()\n        if not include_output:\n            del data[\"output\"]\n        return data\n\n    @staticmethod\n    def get_response_output(response: Response) -&gt; list[dict]:\n        \"\"\"Get response output from response\"\"\"\n        return response.model_dump()[\"output\"]\n\n    @classmethod\n    def tool_chatcompletion_to_responses(cls, tool: ChatCompletionToolParam) -&gt; FunctionToolParam:\n        assert tool[\"type\"] == \"function\"\n        return FunctionToolParam(\n            name=tool[\"function\"][\"name\"],\n            description=tool[\"function\"].get(\"description\", \"\"),\n            parameters=tool[\"function\"].get(\"parameters\", None),\n            type=\"function\",\n        )\n\n    @staticmethod\n    def maybe_basemodel_to_dict(obj: Any) -&gt; dict | None:\n        if isinstance(obj, BaseModel):\n            return obj.model_dump()\n        return obj\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.OpenAIUtils.get_response_configs","title":"get_response_configs  <code>staticmethod</code>","text":"<pre><code>get_response_configs(\n    response: Response, include_output: bool = False\n) -&gt; dict\n</code></pre> <p>Get response configs from response</p> Source code in <code>utu/utils/openai_utils/openai_utils.py</code> <pre><code>@staticmethod\ndef get_response_configs(response: Response, include_output: bool = False) -&gt; dict:\n    \"\"\"Get response configs from response\"\"\"\n    data = response.model_dump()\n    if not include_output:\n        del data[\"output\"]\n    return data\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.OpenAIUtils.get_response_output","title":"get_response_output  <code>staticmethod</code>","text":"<pre><code>get_response_output(response: Response) -&gt; list[dict]\n</code></pre> <p>Get response output from response</p> Source code in <code>utu/utils/openai_utils/openai_utils.py</code> <pre><code>@staticmethod\ndef get_response_output(response: Response) -&gt; list[dict]:\n    \"\"\"Get response output from response\"\"\"\n    return response.model_dump()[\"output\"]\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.SimplifiedAsyncOpenAI","title":"SimplifiedAsyncOpenAI","text":"<p>               Bases: <code>AsyncOpenAI</code></p> <p>Simplified OpenAI client for chat.completions and responses API, with default config</p> Source code in <code>utu/utils/openai_utils/simplified_client.py</code> <pre><code>class SimplifiedAsyncOpenAI(AsyncOpenAI):\n    \"\"\"Simplified OpenAI client for chat.completions and responses API, with default config\"\"\"\n\n    def __init__(\n        self,\n        *,\n        type: Literal[\"chat.completions\", \"responses\"] = None,\n        # openai client kwargs\n        api_key: str | None = None,\n        base_url: str | None = None,\n        # default configs\n        **kwargs: dict,\n    ) -&gt; None:\n        logger.info(f\"&gt; type: {type}, base_url: {base_url}, kwargs: {kwargs}\")\n        super().__init__(\n            api_key=api_key or os.getenv(\"UTU_LLM_API_KEY\") or \"xxx\", base_url=base_url or os.getenv(\"UTU_LLM_BASE_URL\")\n        )\n        self.type = type or os.getenv(\"UTU_LLM_TYPE\", \"chat.completions\")\n        self.type_create_params = (\n            OpenAIChatCompletionParamsKeys if self.type == \"chat.completions\" else OpenAIResponsesParamsKeys\n        )\n        self.default_config = self._process_kwargs(kwargs)\n\n    def _process_kwargs(self, kwargs: dict) -&gt; dict:\n        # parse kwargs for ChatCompletionParams\n        default_config = {}\n        for k, v in kwargs.items():\n            if k in self.type_create_params:\n                default_config[k] = v\n        default_config[\"model\"] = default_config.get(\"model\", os.getenv(\"UTU_LLM_MODEL\"))\n        return default_config\n\n    async def query_one(self, **kwargs) -&gt; str:\n        \"\"\"Simplified chat.complete / responses API\n        WARNING: Only for basic text i/o usage! You should not use the method with querying with customized configs!\n        \"\"\"\n        if \"stream\" in kwargs:\n            assert kwargs[\"stream\"] is False, \"stream is not supported in `query_one`\"\n\n        if self.type == \"chat.completions\":\n            chat_completion: ChatCompletion = await self.chat_completions_create(**kwargs)\n            return chat_completion.choices[0].message.content\n        elif self.type == \"responses\":\n            response: Response = await self.responses_create(**kwargs)\n            return response.output_text  # NOTE: will not return toolcall or reasoning\n        else:\n            raise ValueError(f\"Unknown type: {self.type}\")\n\n    async def chat_completions_create(self, **kwargs) -&gt; ChatCompletion | AsyncStream[ChatCompletionChunk]:\n        assert self.type == \"chat.completions\", \"`chat_completions_create` is not supported for responses API\"\n        unknown_params = self.check_known_keys(kwargs, self.type_create_params)\n        if unknown_params:\n            logger.warning(f\"Unknown parameters: {unknown_params} for {self.type} API!\")\n        kwargs = self.process_chat_completion_params(kwargs, self.default_config)\n        return await self.chat.completions.create(**kwargs)\n\n    async def responses_create(self, **kwargs) -&gt; Response | AsyncStream[ResponseStreamEvent]:\n        unknown_params = self.check_known_keys(kwargs, self.type_create_params)\n        if unknown_params - {\"messages\"}:  # ignore\n            logger.warning(f\"Unknown parameters: {unknown_params} for {self.type} API!\")\n        assert self.type == \"responses\", \"`responses_create` is not supported for chat.completions API\"\n        kwargs = self.process_responses_params(kwargs, self.default_config)\n        return await self.responses.create(**kwargs)\n\n    def process_chat_completion_params(\n        self, kwargs: OpenAIChatCompletionParams, default_config: OpenAIChatCompletionParams\n    ) -&gt; OpenAIChatCompletionParams:\n        \"\"\"Process chat completion params, convert str to list of messages, merge default config\"\"\"\n        assert \"messages\" in kwargs\n        if isinstance(kwargs[\"messages\"], str):\n            kwargs[\"messages\"] = [{\"role\": \"user\", \"content\": kwargs[\"messages\"]}]\n        return self._merge_default_config(kwargs, default_config)\n\n    def process_responses_params(\n        self, kwargs: OpenAIResponsesParams, default_config: OpenAIResponsesParams\n    ) -&gt; OpenAIResponsesParams:\n        \"\"\"Process responses params, convert str to list of messages, merge default config\"\"\"\n        if \"input\" not in kwargs:\n            # try parse query for chat.completions\n            assert \"messages\" in kwargs\n            input = kwargs.pop(\"messages\")\n            if isinstance(input, str):\n                kwargs[\"input\"] = [{\"role\": \"user\", \"content\": input}]\n            else:\n                kwargs[\"input\"] = input\n        else:\n            if isinstance(kwargs[\"input\"], str):\n                kwargs[\"input\"] = [{\"role\": \"user\", \"content\": kwargs[\"input\"]}]\n        return self._merge_default_config(kwargs, default_config)\n\n    def _merge_default_config(self, kwargs: dict, default_config: dict) -&gt; dict:\n        \"\"\"Merge default config\"\"\"\n        for k, v in default_config.items():\n            if k not in kwargs:\n                kwargs[k] = v\n        return kwargs\n\n    def check_known_keys(self, kwargs: dict, known_keys: set[str]) -&gt; set:\n        \"\"\"Check if all keys in kwargs are in known_keys\"\"\"\n        unknown_keys = set(kwargs.keys()) - known_keys\n        return unknown_keys\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.SimplifiedAsyncOpenAI.query_one","title":"query_one  <code>async</code>","text":"<pre><code>query_one(**kwargs) -&gt; str\n</code></pre> <p>Simplified chat.complete / responses API WARNING: Only for basic text i/o usage! You should not use the method with querying with customized configs!</p> Source code in <code>utu/utils/openai_utils/simplified_client.py</code> <pre><code>async def query_one(self, **kwargs) -&gt; str:\n    \"\"\"Simplified chat.complete / responses API\n    WARNING: Only for basic text i/o usage! You should not use the method with querying with customized configs!\n    \"\"\"\n    if \"stream\" in kwargs:\n        assert kwargs[\"stream\"] is False, \"stream is not supported in `query_one`\"\n\n    if self.type == \"chat.completions\":\n        chat_completion: ChatCompletion = await self.chat_completions_create(**kwargs)\n        return chat_completion.choices[0].message.content\n    elif self.type == \"responses\":\n        response: Response = await self.responses_create(**kwargs)\n        return response.output_text  # NOTE: will not return toolcall or reasoning\n    else:\n        raise ValueError(f\"Unknown type: {self.type}\")\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.SimplifiedAsyncOpenAI.process_chat_completion_params","title":"process_chat_completion_params","text":"<pre><code>process_chat_completion_params(\n    kwargs: OpenAIChatCompletionParams,\n    default_config: OpenAIChatCompletionParams,\n) -&gt; OpenAIChatCompletionParams\n</code></pre> <p>Process chat completion params, convert str to list of messages, merge default config</p> Source code in <code>utu/utils/openai_utils/simplified_client.py</code> <pre><code>def process_chat_completion_params(\n    self, kwargs: OpenAIChatCompletionParams, default_config: OpenAIChatCompletionParams\n) -&gt; OpenAIChatCompletionParams:\n    \"\"\"Process chat completion params, convert str to list of messages, merge default config\"\"\"\n    assert \"messages\" in kwargs\n    if isinstance(kwargs[\"messages\"], str):\n        kwargs[\"messages\"] = [{\"role\": \"user\", \"content\": kwargs[\"messages\"]}]\n    return self._merge_default_config(kwargs, default_config)\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.SimplifiedAsyncOpenAI.process_responses_params","title":"process_responses_params","text":"<pre><code>process_responses_params(\n    kwargs: OpenAIResponsesParams,\n    default_config: OpenAIResponsesParams,\n) -&gt; OpenAIResponsesParams\n</code></pre> <p>Process responses params, convert str to list of messages, merge default config</p> Source code in <code>utu/utils/openai_utils/simplified_client.py</code> <pre><code>def process_responses_params(\n    self, kwargs: OpenAIResponsesParams, default_config: OpenAIResponsesParams\n) -&gt; OpenAIResponsesParams:\n    \"\"\"Process responses params, convert str to list of messages, merge default config\"\"\"\n    if \"input\" not in kwargs:\n        # try parse query for chat.completions\n        assert \"messages\" in kwargs\n        input = kwargs.pop(\"messages\")\n        if isinstance(input, str):\n            kwargs[\"input\"] = [{\"role\": \"user\", \"content\": input}]\n        else:\n            kwargs[\"input\"] = input\n    else:\n        if isinstance(kwargs[\"input\"], str):\n            kwargs[\"input\"] = [{\"role\": \"user\", \"content\": kwargs[\"input\"]}]\n    return self._merge_default_config(kwargs, default_config)\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.SimplifiedAsyncOpenAI.check_known_keys","title":"check_known_keys","text":"<pre><code>check_known_keys(kwargs: dict, known_keys: set[str]) -&gt; set\n</code></pre> <p>Check if all keys in kwargs are in known_keys</p> Source code in <code>utu/utils/openai_utils/simplified_client.py</code> <pre><code>def check_known_keys(self, kwargs: dict, known_keys: set[str]) -&gt; set:\n    \"\"\"Check if all keys in kwargs are in known_keys\"\"\"\n    unknown_keys = set(kwargs.keys()) - known_keys\n    return unknown_keys\n</code></pre>"}]}